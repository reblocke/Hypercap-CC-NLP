{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c3399e6",
   "metadata": {},
   "source": [
    "# Code to calculate agreement between Monique and Reyan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf93ab7",
   "metadata": {},
   "source": [
    "**Rationale:** Quantify inter-rater reliability for multi-label RVC assignments to establish label quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0c13bc",
   "metadata": {},
   "source": [
    "Refactor note: This notebook is now structured for deterministic, fail-fast execution while preserving the original agreement estimands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d11f21",
   "metadata": {},
   "source": [
    "# 1  Annotation Framework: Multi-Label, Order-Invariant Agreement\n",
    "\n",
    "## 1.1  Context and purpose\n",
    "Each emergency-department visit receives up to five Reason-for-Visit (RFV1–RFV5) labels drawn from the **17 top-level NHAMCS RVC groups** defined in the *MIMIC IV Chief Complaint → NHAMCS Reason-for-Visit Reviewer Manual v0.3*.\n",
    "These represent patient-stated reasons, not clinician diagnoses.\n",
    "\n",
    "Our goal is to quantify **inter-rater agreement** among multiple human annotators who each output an **unordered subset** of these groups for every visit.\n",
    "For analysis we ignore RFV ordering—treating the set\n",
    "\\( S_{jr} = \\{\\text{categories assigned by rater r to visit j}\\} \\)\n",
    "as a simple set of 0–5 elements.\n",
    "\n",
    "## 1.2  Visit-level agreement classes\n",
    "For any two raters A and B:\n",
    "\n",
    "| Class | Definition | Intuition |\n",
    "|-------|-------------|-----------|\n",
    "| **Full** | \\(S_A = S_B\\) | Complete category match |\n",
    "| **Partial** | \\(S_A \\cap S_B \\neq \\varnothing\\) and \\(S_A \\ne S_B\\) | Share at least one category |\n",
    "| **None** | \\(S_A \\cap S_B = \\varnothing\\) | Disjoint label sets |\n",
    "\n",
    "For three raters,\n",
    "- **Full** = all identical;\n",
    "- **Partial** = any overlap between any pair;\n",
    "- **None** = pairwise disjoint.\n",
    "\n",
    "## 1.3  Set-similarity metrics\n",
    "Because order is meaningless, comparisons use **set-based overlaps**:\n",
    "\n",
    "| Metric | Formula | Notes |\n",
    "|---------|----------|-------|\n",
    "| **Jaccard** | \\(|S_A∩S_B| / |S_A∪S_B|\\) | Measures intersection over union |\n",
    "| **Overlap Coefficient** | \\(|S_A∩S_B| / \\min(|S_A|, |S_B|)\\) | Robust when one rater lists fewer items |\n",
    "| **F₁ (Set)** | \\(2|S_A∩S_B| / (|S_A|+|S_B|)\\) | Equivalent to Sørensen–Dice |\n",
    "| **Micro-F₁ (Set)** | \\(2∑|S_A∩S_B| / ∑(|S_A|+|S_B|)\\) | Corpus-level summary |\n",
    "\n",
    "These capture graded similarity beyond binary full/none counts.\n",
    "\n",
    "## 1.4  Three-rater adjudication logic\n",
    "When a third reviewer adjudicates:\n",
    "- Compute the distribution of *full / partial / none* across all three.\n",
    "- For disagreements between raters 1 and 2, evaluate whether rater 3:\n",
    "  - matches either rater,\n",
    "  - matches their union or intersection,\n",
    "  - introduces any new category, or\n",
    "  - provides a subset of the union.\n",
    "\n",
    "This quantifies how the final “reference standard” was reached.\n",
    "\n",
    "\n",
    "# 2  Agreement Beyond Chance (Chance-Corrected Metrics)\n",
    "\n",
    "## 2.1  Binary expansion per category\n",
    "Convert the multi-label sets into a binary decision matrix over the 17 canonical RVC groups:\n",
    "for each visit × rater × category, record 1 if the category is present.\n",
    "This yields parallel binary classification tasks where standard chance-corrected statistics apply.\n",
    "\n",
    "## 2.2  Why Gwet’s AC1\n",
    "Cohen’s κ is sensitive to marginal prevalence—especially when most visits lack a given category (the *kappa paradox*).\n",
    "Gwet’s AC1 provides a more stable estimate of “agreement beyond chance” in such sparse settings.\n",
    "\n",
    "For each category *g* with *m* raters:\n",
    "\n",
    "\\[\n",
    "A_j =\n",
    "\\frac{\\binom{n^+_j}{2} + \\binom{m-n^+_j}{2}}{\\binom{m}{2}},\n",
    "\\qquad\n",
    "P_o = \\text{mean}(A_j),\n",
    "\\qquad\n",
    "p = \\frac{1}{Nm}\\sum_{j,r} X_{jr}^{(g)},\n",
    "\\]\n",
    "\\[\n",
    "\\text{AC1} = \\frac{P_o - 2p(1-p)}{1 - 2p(1-p)}.\n",
    "\\]\n",
    "\n",
    "Report **per-category AC1** and **macro-average** across categories.\n",
    "Also compute **pairwise κ** (for comparison) and **percent agreement** to provide a full view.\n",
    "\n",
    "## 2.3  Interpretation\n",
    "| Metric | What it captures | Typical reporting |\n",
    "|---------|------------------|-------------------|\n",
    "| **Percent agreement** | Raw consistency, ignores chance | Always include |\n",
    "| **Cohen’s κ** | Chance-corrected for two raters | Supplemental |\n",
    "| **Gwet’s AC1** | Chance-corrected, robust to imbalance | Primary statistic |\n",
    "| **Multi-rater AC1** | Extension of AC1 to m > 2 | Summary across all raters |\n",
    "\n",
    "High percent agreement with high AC1 (≈ 0.9–1.0) implies strong, reproducible labeling of the RVC groups defined in the Reviewer Manual v0.3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e13398c",
   "metadata": {},
   "source": [
    "**Rationale:** Define set-based agreement metrics and chance-corrected statistics appropriate for multi-label labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b468eec1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T05:01:37.670906Z",
     "iopub.status.busy": "2026-02-10T05:01:37.670384Z",
     "iopub.status.idle": "2026-02-10T05:01:38.149193Z",
     "shell.execute_reply": "2026-02-10T05:01:38.148946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORK_DIR: /Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP\n",
      "Annotation workbook: /Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/Annotation/Final 2025-10-14 Annotation Sample.xlsx\n",
      "NLP workbook (canonical default: MIMICIV all with CC_with_NLP.xlsx): /Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/MIMIC tabular data/MIMICIV all with CC_with_NLP.xlsx\n",
      "Raters output dir: /Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/Annotation/Full Annotations/Agreement Metrics\n",
      "NLP output dir: /Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/annotation_agreement_outputs_nlp\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Configuration cell\n",
    "#\n",
    "# Purpose:\n",
    "# 1) Keep all file paths and run settings in one obvious place.\n",
    "# 2) Make it easy for collaborators to re-run this notebook without editing\n",
    "#    downstream analysis logic.\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load optional .env values (for example WORK_DIR) if present.\n",
    "load_dotenv()\n",
    "\n",
    "# Suppress benign openpyxl extension warning emitted by validated annotation sheets.\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"Data Validation extension is not supported and will be removed\",\n",
    "    category=UserWarning,\n",
    ")\n",
    "\n",
    "# WORK_DIR anchors all relative paths. If not provided, default to the\n",
    "# current working directory so the notebook still runs in a local clone.\n",
    "WORK_DIR = Path(os.getenv(\"WORK_DIR\", Path.cwd())).expanduser().resolve()\n",
    "SRC_DIR = WORK_DIR / \"src\"\n",
    "if SRC_DIR.exists() and str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "from hypercap_cc_nlp.workflow_contracts import (\n",
    "    CANONICAL_NLP_FILENAME,\n",
    "    resolve_rater_nlp_input_path,\n",
    ")\n",
    "\n",
    "RATER_ANNOTATION_PATH = os.getenv(\"RATER_ANNOTATION_PATH\")\n",
    "RATER_NLP_INPUT_FILENAME = os.getenv(\"RATER_NLP_INPUT_FILENAME\")\n",
    "\n",
    "annotation_default_path = WORK_DIR / \"Annotation/Final 2025-10-14 Annotation Sample.xlsx\"\n",
    "annotation_path = (\n",
    "    Path(RATER_ANNOTATION_PATH).expanduser().resolve()\n",
    "    if RATER_ANNOTATION_PATH\n",
    "    else annotation_default_path\n",
    ")\n",
    "nlp_path = resolve_rater_nlp_input_path(\n",
    "    WORK_DIR,\n",
    "    RATER_NLP_INPUT_FILENAME if RATER_NLP_INPUT_FILENAME else None,\n",
    ")\n",
    "\n",
    "# Centralized run configuration. Edit these values when sharing the\n",
    "# notebook across environments or workbook versions.\n",
    "CONFIG = {\n",
    "    \"annotation_path\": annotation_path,\n",
    "    \"annotation_sheet\": \"cohort_cc_sample\",\n",
    "    \"category_sheet\": \"Data\",\n",
    "    \"category_column\": \"RVC Categories\",\n",
    "    \"nlp_path\": nlp_path,\n",
    "    \"nlp_sheet\": 0,\n",
    "    \"nlp_min_sim\": None,\n",
    "    \"raters_output_dir\": WORK_DIR / \"Annotation/Full Annotations/Agreement Metrics\",\n",
    "    \"nlp_output_dir\": WORK_DIR / \"annotation_agreement_outputs_nlp\",\n",
    "}\n",
    "\n",
    "ANNOTATION_PATH = Path(CONFIG[\"annotation_path\"])\n",
    "ANNOTATION_SHEET = CONFIG[\"annotation_sheet\"]\n",
    "CATEGORY_SHEET = CONFIG[\"category_sheet\"]\n",
    "CATEGORY_COLUMN = CONFIG[\"category_column\"]\n",
    "NLP_PATH = Path(CONFIG[\"nlp_path\"])\n",
    "NLP_SHEET = CONFIG[\"nlp_sheet\"]\n",
    "NLP_MIN_SIM = CONFIG[\"nlp_min_sim\"]\n",
    "RATERS_OUTPUT_DIR = Path(CONFIG[\"raters_output_dir\"])\n",
    "NLP_OUTPUT_DIR = Path(CONFIG[\"nlp_output_dir\"])\n",
    "\n",
    "# Fail fast if required input workbooks are missing. This prevents subtle\n",
    "# downstream errors that are harder for new users to debug.\n",
    "for path in (ANNOTATION_PATH, NLP_PATH):\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Required input file not found: {path}\")\n",
    "\n",
    "# Ensure output folders exist before writing artifacts.\n",
    "RATERS_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "NLP_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Echo active configuration for provenance in notebook logs.\n",
    "print(f\"WORK_DIR: {WORK_DIR}\")\n",
    "print(f\"Annotation workbook: {ANNOTATION_PATH}\")\n",
    "print(f\"NLP workbook (canonical default: {CANONICAL_NLP_FILENAME}): {NLP_PATH}\")\n",
    "print(f\"Raters output dir: {RATERS_OUTPUT_DIR}\")\n",
    "print(f\"NLP output dir: {NLP_OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "972fd9f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T05:01:38.150616Z",
     "iopub.status.busy": "2026-02-10T05:01:38.150479Z",
     "iopub.status.idle": "2026-02-10T05:01:38.171308Z",
     "shell.execute_reply": "2026-02-10T05:01:38.171101Z"
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Shared helper functions used by both analyses below.\n",
    "#\n",
    "# These helpers intentionally keep I/O checks, label processing, and metric\n",
    "# calculations separate so trainees can reason about each step in isolation.\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Set, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from hypercap_cc_nlp.rater_core import build_r3_nlp_join_audit\n",
    "\n",
    "\n",
    "# --- Input/sheet validation helpers -------------------------------------------\n",
    "\n",
    "def read_sheet_checked(xlsx_path: Path, sheet_name: str | int) -> pd.DataFrame:\n",
    "    \"\"\"Read an Excel sheet with explicit validation for sheet existence.\"\"\"\n",
    "    if not xlsx_path.exists():\n",
    "        raise FileNotFoundError(f\"Excel file not found: {xlsx_path}\")\n",
    "\n",
    "    xls = pd.ExcelFile(xlsx_path)\n",
    "    if isinstance(sheet_name, str):\n",
    "        if sheet_name not in xls.sheet_names:\n",
    "            raise ValueError(\n",
    "                f\"Sheet '{sheet_name}' not found in {xlsx_path}. Available: {xls.sheet_names}\"\n",
    "            )\n",
    "    elif isinstance(sheet_name, int):\n",
    "        if sheet_name < 0 or sheet_name >= len(xls.sheet_names):\n",
    "            raise ValueError(\n",
    "                f\"Sheet index {sheet_name} is out of range for {xlsx_path}. \"\n",
    "                f\"Valid range: 0..{len(xls.sheet_names) - 1}\"\n",
    "            )\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported sheet type: {type(sheet_name)}\")\n",
    "\n",
    "    return pd.read_excel(xls, sheet_name=sheet_name)\n",
    "\n",
    "\n",
    "def validate_required_columns(df: pd.DataFrame, required: List[str], df_name: str) -> None:\n",
    "    missing = sorted(set(required).difference(df.columns))\n",
    "    if missing:\n",
    "        raise KeyError(f\"{df_name} is missing required columns: {missing}\")\n",
    "\n",
    "\n",
    "def validate_unique_keys(df: pd.DataFrame, keys: List[str], df_name: str) -> None:\n",
    "    validate_required_columns(df, keys, df_name)\n",
    "    duplicates = int(df.duplicated(subset=keys).sum())\n",
    "    if duplicates:\n",
    "        raise ValueError(\n",
    "            f\"{df_name} has {duplicates} duplicate rows for key columns {keys}. \"\n",
    "            \"Expected unique keys before merge.\"\n",
    "        )\n",
    "\n",
    "\n",
    "# --- Label normalization and extraction helpers -------------------------------\n",
    "\n",
    "def normalize_label(value: object) -> str | None:\n",
    "    \"\"\"Normalize punctuation/spacing so category strings match reliably.\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    text = str(value).strip()\n",
    "    if not text:\n",
    "        return None\n",
    "    # Normalize dash variants so semantically identical labels map together\n",
    "    # (e.g., patient-stated vs patient–stated).\n",
    "    text = text.replace(\" - \", \" – \").replace(\"—\", \"–\").replace(\"-\", \"–\")\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "\n",
    "def load_categories_from_sheet(\n",
    "    xlsx_path: Path,\n",
    "    sheet: str = \"Data\",\n",
    "    column: str = \"RVC Categories\",\n",
    ") -> List[str]:\n",
    "    cat_df = read_sheet_checked(xlsx_path, sheet)\n",
    "    validate_required_columns(cat_df, [column], f\"{xlsx_path.name}:{sheet}\")\n",
    "\n",
    "    # Reuse the same normalization as annotation labels so category matching\n",
    "    # is internally consistent across all downstream metrics.\n",
    "    categories = [\n",
    "        normalize_label(c)\n",
    "        for c in cat_df[column].dropna().astype(str).tolist()\n",
    "        if normalize_label(c)\n",
    "    ]\n",
    "\n",
    "    if not categories:\n",
    "        raise ValueError(\n",
    "            f\"No categories found in column '{column}' of sheet '{sheet}' ({xlsx_path}).\"\n",
    "        )\n",
    "\n",
    "    if len(set(categories)) != len(categories):\n",
    "        duplicates = [c for c, n in Counter(categories).items() if n > 1]\n",
    "        raise ValueError(f\"Duplicate canonical categories found: {duplicates}\")\n",
    "\n",
    "    return categories\n",
    "\n",
    "\n",
    "def extract_rater_sets(df: pd.DataFrame, rater: int, max_slots: int = 5) -> List[Set[str]]:\n",
    "    \"\"\"Return one set of assigned categories per row for a numbered rater.\"\"\"\n",
    "    return extract_rater_sets_from_df(df, rater_col_prefix=f\"annot{rater}\", max_slots=max_slots)\n",
    "\n",
    "\n",
    "def extract_rater_sets_from_df(\n",
    "    df: pd.DataFrame,\n",
    "    rater_col_prefix: str,\n",
    "    max_slots: int = 5,\n",
    ") -> List[Set[str]]:\n",
    "    \"\"\"Return one set of categories per row for columns like <prefix>_rvs1_cat..rvsN_cat.\"\"\"\n",
    "    cols = [\n",
    "        f\"{rater_col_prefix}_rvs{i}_cat\"\n",
    "        for i in range(1, max_slots + 1)\n",
    "        if f\"{rater_col_prefix}_rvs{i}_cat\" in df.columns\n",
    "    ]\n",
    "    if not cols:\n",
    "        raise KeyError(\n",
    "            f\"No columns found for prefix '{rater_col_prefix}'. \"\n",
    "            f\"Expected columns like '{rater_col_prefix}_rvs1_cat'.\"\n",
    "        )\n",
    "\n",
    "    sets: List[Set[str]] = []\n",
    "    for _, row in df[cols].iterrows():\n",
    "        labels: Set[str] = set()\n",
    "        for col in cols:\n",
    "            lab = normalize_label(row[col])\n",
    "            if lab:\n",
    "                labels.add(lab)\n",
    "        sets.append(labels)\n",
    "    return sets\n",
    "\n",
    "\n",
    "def extract_nlp_sets_from_df(\n",
    "    df_nlp: pd.DataFrame,\n",
    "    max_slots: int = 5,\n",
    "    min_sim: float | None = None,\n",
    ") -> List[Set[str]]:\n",
    "    \"\"\"Build predicted category sets from RFVn_name columns with optional RFVn_sim threshold.\"\"\"\n",
    "    name_cols = [f\"RFV{i}_name\" for i in range(1, max_slots + 1) if f\"RFV{i}_name\" in df_nlp.columns]\n",
    "    sim_cols = [f\"RFV{i}_sim\" for i in range(1, max_slots + 1) if f\"RFV{i}_sim\" in df_nlp.columns]\n",
    "    # Similarity thresholding is optional. If min_sim is None, keep all\n",
    "    # predicted labels. If set, require RFVn_sim >= min_sim.\n",
    "    use_sim = min_sim is not None and len(sim_cols) == len(name_cols)\n",
    "\n",
    "    sets: List[Set[str]] = []\n",
    "    for _, row in df_nlp.iterrows():\n",
    "        labels: Set[str] = set()\n",
    "        for i in range(1, max_slots + 1):\n",
    "            name_col = f\"RFV{i}_name\"\n",
    "            if name_col not in df_nlp.columns:\n",
    "                continue\n",
    "            name_val = normalize_label(row[name_col])\n",
    "            if not name_val:\n",
    "                continue\n",
    "\n",
    "            if use_sim:\n",
    "                sim_col = f\"RFV{i}_sim\"\n",
    "                sim_val = row.get(sim_col, np.nan)\n",
    "                if pd.isna(sim_val) or float(sim_val) < float(min_sim):\n",
    "                    continue\n",
    "            labels.add(name_val)\n",
    "        sets.append(labels)\n",
    "    return sets\n",
    "\n",
    "\n",
    "# --- Set-level agreement metrics (order-invariant multi-label) ----------------\n",
    "\n",
    "def jaccard(a: Set[str], b: Set[str]) -> float:\n",
    "    union_size = len(a | b)\n",
    "    return 1.0 if union_size == 0 else len(a & b) / union_size\n",
    "\n",
    "\n",
    "def overlap_coeff(a: Set[str], b: Set[str]) -> float:\n",
    "    min_size = min(len(a), len(b))\n",
    "    # Convention: when both sets are empty, treat overlap as perfect (1.0)\n",
    "    # because both raters made the same decision: assign nothing.\n",
    "    return (\n",
    "        1.0\n",
    "        if min_size == 0 and len(a) == 0 and len(b) == 0\n",
    "        else (0.0 if min_size == 0 else len(a & b) / min_size)\n",
    "    )\n",
    "\n",
    "\n",
    "def f1_set(a: Set[str], b: Set[str]) -> float:\n",
    "    denom = len(a) + len(b)\n",
    "    return 1.0 if denom == 0 else 2 * len(a & b) / denom\n",
    "\n",
    "\n",
    "def classify_three(a: Set[str], b: Set[str], c: Set[str]) -> str:\n",
    "    if a == b == c:\n",
    "        return \"full\"\n",
    "    return \"partial\" if (a & b) or (a & c) or (b & c) else \"none\"\n",
    "\n",
    "\n",
    "def micro_f1_raters(a_sets: List[Set[str]], b_sets: List[Set[str]]) -> float:\n",
    "    tp = sum(len(a & b) for a, b in zip(a_sets, b_sets))\n",
    "    denom = sum(len(a) + len(b) for a, b in zip(a_sets, b_sets))\n",
    "    return 1.0 if denom == 0 else 2 * tp / denom\n",
    "\n",
    "\n",
    "def summarize_set_agreement(\n",
    "    a_sets: List[Set[str]],\n",
    "    b_sets: List[Set[str]],\n",
    ") -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    recs = []\n",
    "    for a, b in zip(a_sets, b_sets):\n",
    "        # Visit-level class labels support intuitive interpretation for\n",
    "        # non-statistical readers (exact match, partial overlap, no overlap).\n",
    "        recs.append(\n",
    "            {\n",
    "                \"exact\": int(a == b),\n",
    "                \"partial\": int((a != b) and (len(a & b) > 0)),\n",
    "                \"none\": int(len(a & b) == 0),\n",
    "                \"jaccard\": jaccard(a, b),\n",
    "                \"overlap\": overlap_coeff(a, b),\n",
    "                \"f1_set\": f1_set(a, b),\n",
    "                \"len_a\": len(a),\n",
    "                \"len_b\": len(b),\n",
    "                \"len_inter\": len(a & b),\n",
    "                \"len_union\": len(a | b),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    out = pd.DataFrame(recs)\n",
    "    summary = pd.Series(\n",
    "        {\n",
    "            \"N_items\": len(out),\n",
    "            \"exact_rate\": out[\"exact\"].mean(),\n",
    "            \"partial_rate\": out[\"partial\"].mean(),\n",
    "            \"none_rate\": out[\"none\"].mean(),\n",
    "            \"mean_jaccard\": out[\"jaccard\"].mean(),\n",
    "            \"mean_overlap\": out[\"overlap\"].mean(),\n",
    "            \"mean_f1_set\": out[\"f1_set\"].mean(),\n",
    "            \"micro_f1_set\": micro_f1_raters(a_sets, b_sets),\n",
    "            \"mean_len_a\": out[\"len_a\"].mean(),\n",
    "            \"mean_len_b\": out[\"len_b\"].mean(),\n",
    "        }\n",
    "    )\n",
    "    return out, summary\n",
    "\n",
    "\n",
    "def summarize_three_way(\n",
    "    r1_sets: List[Set[str]],\n",
    "    r2_sets: List[Set[str]],\n",
    "    r3_sets: List[Set[str]],\n",
    ") -> pd.Series:\n",
    "    labels = [\n",
    "        classify_three(a, b, c)\n",
    "        for a, b, c in zip(r1_sets, r2_sets, r3_sets)\n",
    "    ]\n",
    "    n_items = len(labels)\n",
    "    counts = Counter(labels)\n",
    "    return pd.Series(\n",
    "        {\n",
    "            \"N_items\": n_items,\n",
    "            \"full_rate\": counts.get(\"full\", 0) / n_items,\n",
    "            \"partial_rate\": counts.get(\"partial\", 0) / n_items,\n",
    "            \"none_rate\": counts.get(\"none\", 0) / n_items,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def adjudication_resolution(\n",
    "    r1_sets: List[Set[str]],\n",
    "    r2_sets: List[Set[str]],\n",
    "    r3_sets: List[Set[str]],\n",
    ") -> pd.Series:\n",
    "    recs = []\n",
    "    for a, b, c in zip(r1_sets, r2_sets, r3_sets):\n",
    "        if a == b:\n",
    "            continue\n",
    "        union_set = a | b\n",
    "        inter_set = a & b\n",
    "        recs.append(\n",
    "            {\n",
    "                \"r3_equals_r1\": int(c == a),\n",
    "                \"r3_equals_r2\": int(c == b),\n",
    "                \"r3_equals_union\": int(c == union_set),\n",
    "                \"r3_equals_intersection\": int((len(inter_set) > 0) and (c == inter_set)),\n",
    "                \"r3_introduces_new\": int(len(c - union_set) > 0),\n",
    "                \"r3_subset_of_union\": int(c <= union_set),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if not recs:\n",
    "        return pd.Series({\"N_disagreements\": 0})\n",
    "\n",
    "    out = pd.DataFrame(recs)\n",
    "    return pd.Series(\n",
    "        {\n",
    "            \"N_disagreements\": len(out),\n",
    "            \"r3_equals_r1_rate\": out[\"r3_equals_r1\"].mean(),\n",
    "            \"r3_equals_r2_rate\": out[\"r3_equals_r2\"].mean(),\n",
    "            \"r3_equals_union_rate\": out[\"r3_equals_union\"].mean(),\n",
    "            \"r3_equals_intersection_rate\": out[\"r3_equals_intersection\"].mean(),\n",
    "            \"r3_introduces_new_rate\": out[\"r3_introduces_new\"].mean(),\n",
    "            \"r3_subset_of_union_rate\": out[\"r3_subset_of_union\"].mean(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# --- Binary one-vs-rest encoding for chance-corrected metrics -----------------\n",
    "\n",
    "def flatten_binary_decisions(\n",
    "    raters_sets: Dict[str, List[Set[str]]],\n",
    "    categories: List[str],\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    n = len(next(iter(raters_sets.values())))\n",
    "    k = len(categories)\n",
    "    idx = {c: i for i, c in enumerate(categories)}\n",
    "\n",
    "    out: Dict[str, np.ndarray] = {}\n",
    "    for rater, sets in raters_sets.items():\n",
    "        mat = np.zeros((n, k), dtype=int)\n",
    "        for j, labels in enumerate(sets):\n",
    "            for lab in labels:\n",
    "                lab = normalize_label(lab)\n",
    "                if lab in idx:\n",
    "                    mat[j, idx[lab]] = 1\n",
    "        out[rater] = mat\n",
    "    return out\n",
    "\n",
    "\n",
    "def flatten_binary_decisions_single(\n",
    "    sets: List[Set[str]],\n",
    "    categories: List[str],\n",
    ") -> np.ndarray:\n",
    "    idx = {c: i for i, c in enumerate(categories)}\n",
    "    mat = np.zeros((len(sets), len(categories)), dtype=int)\n",
    "    for j, labels in enumerate(sets):\n",
    "        for lab in labels:\n",
    "            lab = normalize_label(lab)\n",
    "            if lab in idx:\n",
    "                mat[j, idx[lab]] = 1\n",
    "    return mat\n",
    "\n",
    "\n",
    "# --- Chance-corrected agreement metrics ---------------------------------------\n",
    "\n",
    "def pairwise_binary_agreement_stats(\n",
    "    m1: np.ndarray,\n",
    "    m2: np.ndarray,\n",
    "    categories: List[str] | None = None,\n",
    "    include_confusion_counts: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute per-category agreement, kappa, and AC1 for two raters.\n",
    "\n",
    "    We convert the multi-label task into one binary task per category\n",
    "    (present vs absent), then compute chance-corrected metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    n, k = m1.shape\n",
    "    rows = []\n",
    "    for c in range(k):\n",
    "        y1 = m1[:, c]\n",
    "        y2 = m2[:, c]\n",
    "        tp = int(((y1 == 1) & (y2 == 1)).sum())\n",
    "        tn = int(((y1 == 0) & (y2 == 0)).sum())\n",
    "        fp = int(((y1 == 0) & (y2 == 1)).sum())\n",
    "        fn = int(((y1 == 1) & (y2 == 0)).sum())\n",
    "        total = tp + tn + fp + fn\n",
    "\n",
    "        if total == 0:\n",
    "            if include_confusion_counts:\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"category_ix\": c,\n",
    "                        \"category\": categories[c] if categories is not None else c,\n",
    "                        \"N\": 0,\n",
    "                        \"tp\": tp,\n",
    "                        \"tn\": tn,\n",
    "                        \"fp\": fp,\n",
    "                        \"fn\": fn,\n",
    "                        \"prevalence_r3\": np.nan,\n",
    "                        \"prevalence_nlp\": np.nan,\n",
    "                        \"percent_agreement\": np.nan,\n",
    "                        \"cohen_kappa\": np.nan,\n",
    "                        \"gwet_ac1\": np.nan,\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"category\": c,\n",
    "                        \"N\": 0,\n",
    "                        \"percent_agreement\": np.nan,\n",
    "                        \"cohen_kappa\": np.nan,\n",
    "                        \"gwet_ac1\": np.nan,\n",
    "                    }\n",
    "                )\n",
    "            continue\n",
    "\n",
    "        # Observed agreement (Po): proportion of matching binary decisions.\n",
    "        po = (tp + tn) / total\n",
    "        p1 = y1.mean()\n",
    "        p2 = y2.mean()\n",
    "        # Cohen kappa expected agreement under independent prevalence.\n",
    "        pe_kappa = p1 * p2 + (1 - p1) * (1 - p2)\n",
    "        kappa = (po - pe_kappa) / (1 - pe_kappa) if (1 - pe_kappa) != 0 else np.nan\n",
    "        # AC1 expected agreement uses pooled prevalence (pbar).\n",
    "        pbar = (p1 + p2) / 2\n",
    "        pe_ac1 = 2 * pbar * (1 - pbar)\n",
    "        ac1 = (po - pe_ac1) / (1 - pe_ac1) if (1 - pe_ac1) != 0 else np.nan\n",
    "\n",
    "        if include_confusion_counts:\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"category_ix\": c,\n",
    "                    \"category\": categories[c] if categories is not None else c,\n",
    "                    \"N\": total,\n",
    "                    \"tp\": tp,\n",
    "                    \"tn\": tn,\n",
    "                    \"fp\": fp,\n",
    "                    \"fn\": fn,\n",
    "                    \"prevalence_r3\": p1,\n",
    "                    \"prevalence_nlp\": p2,\n",
    "                    \"percent_agreement\": po,\n",
    "                    \"cohen_kappa\": kappa,\n",
    "                    \"gwet_ac1\": ac1,\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"category\": c,\n",
    "                    \"N\": total,\n",
    "                    \"percent_agreement\": po,\n",
    "                    \"cohen_kappa\": kappa,\n",
    "                    \"gwet_ac1\": ac1,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def multirater_ac1_per_category(\n",
    "    matrices: List[np.ndarray],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute multi-rater AC1 per category using average pairwise agreement.\"\"\"\n",
    "\n",
    "    m = len(matrices)\n",
    "    n, k = matrices[0].shape\n",
    "    stack = np.stack(matrices, axis=0)\n",
    "    denom_pairs = m * (m - 1) / 2\n",
    "\n",
    "    rows = []\n",
    "    for c in range(k):\n",
    "        m_c = stack[:, :, c]\n",
    "        npos = m_c.sum(axis=0)\n",
    "        nneg = m - npos\n",
    "        # For each visit, Aj is the proportion of agreeing rater pairs\n",
    "        # (both positive or both negative) for this category.\n",
    "        a_j = (npos * (npos - 1) / 2 + nneg * (nneg - 1) / 2) / denom_pairs\n",
    "        po = a_j.mean()\n",
    "        p = npos.sum() / (n * m)\n",
    "        pe = 2 * p * (1 - p)\n",
    "        ac1 = (po - pe) / (1 - pe) if (1 - pe) != 0 else np.nan\n",
    "        rows.append({\"category\": c, \"percent_agreement\": po, \"gwet_ac1\": ac1, \"prevalence\": p})\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Summarize per-category chance-corrected results into macro and micro views.\n",
    "def summarize_kappa_ac1(df_pw: pd.DataFrame) -> pd.Series:\n",
    "    out = {\n",
    "        \"macro_cohen_kappa\": df_pw[\"cohen_kappa\"].mean(),\n",
    "        \"macro_gwet_ac1\": df_pw[\"gwet_ac1\"].mean(),\n",
    "        \"macro_percent_agreement\": df_pw[\"percent_agreement\"].mean(),\n",
    "    }\n",
    "\n",
    "    # Prefer confusion-matrix micro agreement when counts are available.\n",
    "    if {\"tp\", \"tn\", \"N\"}.issubset(df_pw.columns):\n",
    "        out[\"micro_percent_agreement\"] = (df_pw[\"tp\"].sum() + df_pw[\"tn\"].sum()) / df_pw[\"N\"].sum()\n",
    "    else:\n",
    "        out[\"micro_percent_agreement\"] = (df_pw[\"N\"] * df_pw[\"percent_agreement\"]).sum() / df_pw[\"N\"].sum()\n",
    "\n",
    "    return pd.Series(out)\n",
    "\n",
    "\n",
    "# --- Assertion helpers used to fail fast during notebook runs ------------------\n",
    "\n",
    "def assert_rate_bounds(series: pd.Series, label: str) -> None:\n",
    "    non_null = series.dropna()\n",
    "    if non_null.empty:\n",
    "        return\n",
    "    min_val = float(non_null.min())\n",
    "    max_val = float(non_null.max())\n",
    "    if min_val < 0 or max_val > 1:\n",
    "        raise AssertionError(f\"{label} must be in [0, 1], got [{min_val}, {max_val}]\")\n",
    "\n",
    "\n",
    "def assert_expected_columns(df: pd.DataFrame, required: List[str], label: str) -> None:\n",
    "    missing = sorted(set(required).difference(df.columns))\n",
    "    if missing:\n",
    "        raise AssertionError(f\"{label} missing expected columns: {missing}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5818da04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T05:01:38.172519Z",
     "iopub.status.busy": "2026-02-10T05:01:38.172421Z",
     "iopub.status.idle": "2026-02-10T05:01:38.378469Z",
     "shell.execute_reply": "2026-02-10T05:01:38.378241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Pairwise set-level (order-invariant) ===\n",
      "\n",
      "R1 vs R2:\n",
      "N_items         160.000000\n",
      "exact_rate        0.806250\n",
      "partial_rate      0.131250\n",
      "none_rate         0.062500\n",
      "mean_jaccard      0.856250\n",
      "mean_overlap      0.882292\n",
      "mean_f1_set       0.877440\n",
      "micro_f1_set      0.873950\n",
      "mean_len_a        1.493750\n",
      "mean_len_b        1.481250\n",
      "\n",
      "R1 vs R3:\n",
      "N_items         160.000000\n",
      "exact_rate        0.850000\n",
      "partial_rate      0.075000\n",
      "none_rate         0.075000\n",
      "mean_jaccard      0.879167\n",
      "mean_overlap      0.892708\n",
      "mean_f1_set       0.891190\n",
      "micro_f1_set      0.897275\n",
      "mean_len_a        1.493750\n",
      "mean_len_b        1.487500\n",
      "\n",
      "R2 vs R3:\n",
      "N_items         160.000000\n",
      "exact_rate        0.831250\n",
      "partial_rate      0.112500\n",
      "none_rate         0.056250\n",
      "mean_jaccard      0.876042\n",
      "mean_overlap      0.900000\n",
      "mean_f1_set       0.894583\n",
      "micro_f1_set      0.892632\n",
      "mean_len_a        1.481250\n",
      "mean_len_b        1.487500\n",
      "\n",
      "=== Three-rater set-level (full/partial/none) ===\n",
      "N_items         160.00000\n",
      "full_rate         0.75625\n",
      "partial_rate      0.23125\n",
      "none_rate         0.01250\n",
      "\n",
      "=== Chance-corrected (binary per category) ===\n",
      "R1 vs R2:\n",
      "macro_cohen_kappa          0.734781\n",
      "macro_gwet_ac1             0.972273\n",
      "macro_percent_agreement    0.977941\n",
      "micro_percent_agreement    0.977941\n",
      "\n",
      "R1 vs R3:\n",
      "macro_cohen_kappa          0.801703\n",
      "macro_gwet_ac1             0.976768\n",
      "macro_percent_agreement    0.981985\n",
      "micro_percent_agreement    0.981985\n",
      "\n",
      "R2 vs R3:\n",
      "macro_cohen_kappa          0.779458\n",
      "macro_gwet_ac1             0.976298\n",
      "macro_percent_agreement    0.981250\n",
      "micro_percent_agreement    0.981250\n",
      "\n",
      "=== Multi-rater AC1 (3 raters) ===\n",
      "macro_gwet_ac1=0.9751\n",
      "macro_percent_agreement=0.9804\n",
      "\n",
      "=== Adjudication (R3) for R1≠R2 ===\n",
      "N_disagreements                31.000000\n",
      "r3_equals_r1_rate               0.483871\n",
      "r3_equals_r2_rate               0.387097\n",
      "r3_equals_union_rate            0.096774\n",
      "r3_equals_intersection_rate     0.000000\n",
      "r3_introduces_new_rate          0.096774\n",
      "r3_subset_of_union_rate         0.903226\n",
      "\n",
      "\n",
      "Outputs written to: /Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/Annotation/Full Annotations/Agreement Metrics\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Analysis A: Human rater agreement (R1, R2, R3)\n",
    "#\n",
    "# Workflow:\n",
    "# 1) Load validated annotation/category inputs.\n",
    "# 2) Convert each visit to an unordered set of categories per rater.\n",
    "# 3) Compute set-level metrics and chance-corrected binary metrics.\n",
    "# 4) Run assertions, then write reproducible output artifacts.\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Canonical category names define the fixed category universe used for\n",
    "# binary one-vs-rest metrics.\n",
    "CANONICAL_CATS = load_categories_from_sheet(\n",
    "    ANNOTATION_PATH,\n",
    "    sheet=CATEGORY_SHEET,\n",
    "    column=CATEGORY_COLUMN,\n",
    ")\n",
    "\n",
    "df = read_sheet_checked(ANNOTATION_PATH, ANNOTATION_SHEET)\n",
    "\n",
    "# Validate required rater columns up front so missing data fails early.\n",
    "key_cols = [\"hadm_id\", \"subject_id\"]\n",
    "required_rater_cols = key_cols + [\n",
    "    f\"annot{r}_rvs{i}_cat\"\n",
    "    for r in (1, 2, 3)\n",
    "    for i in range(1, 6)\n",
    "]\n",
    "validate_required_columns(df, required_rater_cols, \"Annotation sheet\")\n",
    "validate_unique_keys(df, key_cols, \"Annotation sheet\")\n",
    "\n",
    "r1_sets = extract_rater_sets(df, 1)\n",
    "r2_sets = extract_rater_sets(df, 2)\n",
    "r3_sets = extract_rater_sets(df, 3)\n",
    "\n",
    "assert len(r1_sets) == len(df), \"R1 set count must equal annotation row count\"\n",
    "assert len(r2_sets) == len(df), \"R2 set count must equal annotation row count\"\n",
    "assert len(r3_sets) == len(df), \"R3 set count must equal annotation row count\"\n",
    "\n",
    "# Pairwise set-level agreement captures visit-level overlap/exactness.\n",
    "pair12_df, pair12_summary = summarize_set_agreement(r1_sets, r2_sets)\n",
    "pair13_df, pair13_summary = summarize_set_agreement(r1_sets, r3_sets)\n",
    "pair23_df, pair23_summary = summarize_set_agreement(r2_sets, r3_sets)\n",
    "\n",
    "three_summary = summarize_three_way(r1_sets, r2_sets, r3_sets)\n",
    "\n",
    "# Chance-corrected metrics require binary matrices by category.\n",
    "raters_sets = {\"r1\": r1_sets, \"r2\": r2_sets, \"r3\": r3_sets}\n",
    "bin_mats = flatten_binary_decisions(raters_sets, CANONICAL_CATS)\n",
    "\n",
    "for rater_name, mat in bin_mats.items():\n",
    "    assert mat.shape == (len(df), len(CANONICAL_CATS)), (\n",
    "        f\"{rater_name} binary matrix shape mismatch: expected \"\n",
    "        f\"({len(df)}, {len(CANONICAL_CATS)}), got {mat.shape}\"\n",
    "    )\n",
    "\n",
    "pw12 = pairwise_binary_agreement_stats(bin_mats[\"r1\"], bin_mats[\"r2\"])\n",
    "pw13 = pairwise_binary_agreement_stats(bin_mats[\"r1\"], bin_mats[\"r3\"])\n",
    "pw23 = pairwise_binary_agreement_stats(bin_mats[\"r2\"], bin_mats[\"r3\"])\n",
    "\n",
    "name_map = {i: c for i, c in enumerate(CANONICAL_CATS)}\n",
    "for df_pw in (pw12, pw13, pw23):\n",
    "    df_pw[\"category_name\"] = df_pw[\"category\"].map(name_map)\n",
    "\n",
    "multi_ac1 = multirater_ac1_per_category([bin_mats[\"r1\"], bin_mats[\"r2\"], bin_mats[\"r3\"]])\n",
    "multi_ac1[\"category_name\"] = multi_ac1[\"category\"].map(name_map)\n",
    "\n",
    "pair12_chance = summarize_kappa_ac1(pw12)\n",
    "pair13_chance = summarize_kappa_ac1(pw13)\n",
    "pair23_chance = summarize_kappa_ac1(pw23)\n",
    "\n",
    "multi_macro_ac1 = multi_ac1[\"gwet_ac1\"].mean()\n",
    "multi_macro_agree = multi_ac1[\"percent_agreement\"].mean()\n",
    "\n",
    "# Deterministic validation checks\n",
    "# These assertions guard against silent failures (shape drift, out-of-range\n",
    "# rates, and missing expected columns).\n",
    "for summary_name, summary in (\n",
    "    (\"pair12_summary\", pair12_summary),\n",
    "    (\"pair13_summary\", pair13_summary),\n",
    "    (\"pair23_summary\", pair23_summary),\n",
    "):\n",
    "    assert int(summary[\"N_items\"]) == len(df), f\"{summary_name} N_items mismatch\"\n",
    "    assert_rate_bounds(summary[[\"exact_rate\", \"partial_rate\", \"none_rate\"]], f\"{summary_name} rates\")\n",
    "    assert_rate_bounds(summary[[\"mean_jaccard\", \"mean_overlap\", \"mean_f1_set\", \"micro_f1_set\"]], f\"{summary_name} similarity metrics\")\n",
    "\n",
    "assert_rate_bounds(three_summary[[\"full_rate\", \"partial_rate\", \"none_rate\"]], \"three_summary rates\")\n",
    "assert int(three_summary[\"N_items\"]) == len(df), \"three_summary N_items mismatch\"\n",
    "\n",
    "for label, df_pw in ((\"pw12\", pw12), (\"pw13\", pw13), (\"pw23\", pw23)):\n",
    "    assert_expected_columns(df_pw, [\"category\", \"N\", \"percent_agreement\", \"cohen_kappa\", \"gwet_ac1\", \"category_name\"], label)\n",
    "    assert len(df_pw) == len(CANONICAL_CATS), f\"{label} row count mismatch with categories\"\n",
    "    assert_rate_bounds(df_pw[\"percent_agreement\"], f\"{label} percent_agreement\")\n",
    "    assert_rate_bounds(df_pw[\"gwet_ac1\"], f\"{label} gwet_ac1\")\n",
    "\n",
    "assert_expected_columns(multi_ac1, [\"category\", \"percent_agreement\", \"gwet_ac1\", \"prevalence\", \"category_name\"], \"multi_ac1\")\n",
    "assert len(multi_ac1) == len(CANONICAL_CATS), \"multi_ac1 row count mismatch with categories\"\n",
    "assert_rate_bounds(multi_ac1[\"percent_agreement\"], \"multi_ac1 percent_agreement\")\n",
    "assert_rate_bounds(multi_ac1[\"gwet_ac1\"], \"multi_ac1 gwet_ac1\")\n",
    "\n",
    "# Output filenames are intentionally unchanged to preserve downstream\n",
    "# manuscript/report references.\n",
    "# Outputs\n",
    "pair12_df.to_csv(RATERS_OUTPUT_DIR / \"pair_R1_R2_set_metrics.csv\", index=False)\n",
    "pair13_df.to_csv(RATERS_OUTPUT_DIR / \"pair_R1_R3_set_metrics.csv\", index=False)\n",
    "pair23_df.to_csv(RATERS_OUTPUT_DIR / \"pair_R2_R3_set_metrics.csv\", index=False)\n",
    "\n",
    "pw12.to_csv(RATERS_OUTPUT_DIR / \"pair_R1_R2_binary_stats.csv\", index=False)\n",
    "pw13.to_csv(RATERS_OUTPUT_DIR / \"pair_R1_R3_binary_stats.csv\", index=False)\n",
    "pw23.to_csv(RATERS_OUTPUT_DIR / \"pair_R2_R3_binary_stats.csv\", index=False)\n",
    "\n",
    "multi_ac1.to_csv(RATERS_OUTPUT_DIR / \"all3_multirater_ac1_by_category.csv\", index=False)\n",
    "\n",
    "adj_summary = adjudication_resolution(r1_sets, r2_sets, r3_sets)\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "=== Pairwise set-level (order-invariant) ===\n",
    "\n",
    "R1 vs R2:\n",
    "{pair12_summary.to_string()}\n",
    "\n",
    "R1 vs R3:\n",
    "{pair13_summary.to_string()}\n",
    "\n",
    "R2 vs R3:\n",
    "{pair23_summary.to_string()}\n",
    "\n",
    "=== Three-rater set-level (full/partial/none) ===\n",
    "{three_summary.to_string()}\n",
    "\n",
    "=== Chance-corrected (binary per category) ===\n",
    "R1 vs R2:\n",
    "{pair12_chance.to_string()}\n",
    "\n",
    "R1 vs R3:\n",
    "{pair13_chance.to_string()}\n",
    "\n",
    "R2 vs R3:\n",
    "{pair23_chance.to_string()}\n",
    "\n",
    "=== Multi-rater AC1 (3 raters) ===\n",
    "macro_gwet_ac1={multi_macro_ac1:.4f}\n",
    "macro_percent_agreement={multi_macro_agree:.4f}\n",
    "\n",
    "=== Adjudication (R3) for R1≠R2 ===\n",
    "{adj_summary.to_string()}\n",
    "\"\"\"\n",
    "\n",
    "(RATERS_OUTPUT_DIR / \"summary.txt\").write_text(summary_text)\n",
    "\n",
    "print(summary_text)\n",
    "print(\"\\nOutputs written to:\", RATERS_OUTPUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e438d5",
   "metadata": {},
   "source": [
    "## Agreement with NLP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f909a79d",
   "metadata": {},
   "source": [
    "**Rationale:** Compare adjudicated human labels with NLP outputs to assess model agreement and error modes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57d46381",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T05:01:38.379788Z",
     "iopub.status.busy": "2026-02-10T05:01:38.379676Z",
     "iopub.status.idle": "2026-02-10T05:02:22.571569Z",
     "shell.execute_reply": "2026-02-10T05:02:22.570607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined R3 to NLP on (hadm_id, subject_id): 160 matched (100.0%); 0 adjudicated rows had no NLP match (0.0%); 41162 NLP rows had no adjudicated match (99.6%).\n",
      "INFO: Non-identical overlap is expected for subset benchmarking (adjudicated_fully_covered_subset). Metrics are computed on matched rows only.\n",
      "=== R3 vs NLP join audit ===\n",
      "\n",
      "key_columns                                 [hadm_id, subject_id]\n",
      "r3_rows                                                       160\n",
      "nlp_rows                                                    41322\n",
      "matched_rows                                                  160\n",
      "unmatched_adjudicated_rows                                      0\n",
      "unmatched_nlp_rows                                          41162\n",
      "matched_rate_vs_adjudicated                                   1.0\n",
      "unmatched_rate_vs_adjudicated                                 0.0\n",
      "unmatched_rate_vs_nlp                                    0.996128\n",
      "join_interpretation              adjudicated_fully_covered_subset\n",
      "severity                                                     info\n",
      "\n",
      "=== R3 (adjudicator) vs NLP: set-level (order-invariant) ===\n",
      "\n",
      "N_items         160.000000\n",
      "exact_rate        0.600000\n",
      "partial_rate      0.193750\n",
      "none_rate         0.206250\n",
      "mean_jaccard      0.679063\n",
      "mean_overlap      0.723958\n",
      "mean_f1_set       0.710655\n",
      "micro_f1_set      0.724211\n",
      "mean_len_a        1.487500\n",
      "mean_len_b        1.481250\n",
      "\n",
      "\n",
      "=== R3 vs NLP: chance-corrected (binary per category) ===\n",
      "\n",
      "macro_cohen_kappa          0.472870\n",
      "macro_gwet_ac1             0.949117\n",
      "macro_percent_agreement    0.957353\n",
      "micro_percent_agreement    0.957353\n",
      "\n",
      "Artifacts written to: /Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/annotation_agreement_outputs_nlp\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Analysis B: Adjudicator (R3) vs NLP agreement\n",
    "#\n",
    "# We intentionally mirror Analysis A so outputs are directly comparable:\n",
    "# same set-level metrics and same binary chance-corrected framework.\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import json\n",
    "\n",
    "# Step 1: Load adjudicator labels and validate keys/columns.\n",
    "df = read_sheet_checked(ANNOTATION_PATH, ANNOTATION_SHEET)\n",
    "r3_key_cols = [\"hadm_id\", \"subject_id\"]\n",
    "r3_label_cols = [\n",
    "    c for c in df.columns\n",
    "    if c.startswith(\"annot3_rvs\") and c.endswith(\"_cat\")\n",
    "]\n",
    "r3_cols = r3_key_cols + r3_label_cols\n",
    "validate_required_columns(df, r3_cols, \"Annotation sheet for R3\")\n",
    "validate_unique_keys(df, r3_key_cols, \"Annotation sheet for R3\")\n",
    "\n",
    "df_r3 = df[r3_cols].copy()\n",
    "\n",
    "# Step 2: Load NLP outputs and validate key columns.\n",
    "df_nlp = read_sheet_checked(NLP_PATH, NLP_SHEET)\n",
    "nlp_key_cols = [\"hadm_id\", \"subject_id\"]\n",
    "nlp_name_cols = [f\"RFV{i}_name\" for i in range(1, 6)]\n",
    "present_name_cols = [c for c in nlp_name_cols if c in df_nlp.columns]\n",
    "validate_required_columns(df_nlp, nlp_key_cols, \"NLP sheet\")\n",
    "if not present_name_cols:\n",
    "    raise KeyError(\"NLP file must contain RFV*_name columns.\")\n",
    "validate_unique_keys(df_nlp, nlp_key_cols, \"NLP sheet\")\n",
    "\n",
    "# Step 3: Build key-normalized join plus explicit overlap audit.\n",
    "df_join, unmatched_adjudicated, unmatched_nlp, join_audit = build_r3_nlp_join_audit(\n",
    "    df_r3,\n",
    "    df_nlp,\n",
    "    r3_key_cols,\n",
    ")\n",
    "\n",
    "n_before = len(df_r3)\n",
    "n_after = int(join_audit[\"matched_rows\"])\n",
    "n_dropped = int(join_audit[\"unmatched_adjudicated_rows\"])\n",
    "n_unmatched_nlp = int(join_audit[\"unmatched_nlp_rows\"])\n",
    "\n",
    "assert n_after > 0, \"Join produced zero rows; check key consistency between annotation and NLP workbooks.\"\n",
    "assert n_after <= n_before, \"Inner join should not increase rows when both sides have unique keys.\"\n",
    "\n",
    "def _fmt_rate(value: float | None) -> str:\n",
    "    return \"NA\" if value is None else f\"{value:.1%}\"\n",
    "\n",
    "print(\n",
    "    \"Joined R3 to NLP on (hadm_id, subject_id): \"\n",
    "    f\"{n_after} matched ({_fmt_rate(join_audit['matched_rate_vs_adjudicated'])}); \"\n",
    "    f\"{n_dropped} adjudicated rows had no NLP match \"\n",
    "    f\"({_fmt_rate(join_audit['unmatched_rate_vs_adjudicated'])}); \"\n",
    "    f\"{n_unmatched_nlp} NLP rows had no adjudicated match \"\n",
    "    f\"({_fmt_rate(join_audit['unmatched_rate_vs_nlp'])}).\"\n",
    ")\n",
    "join_severity = str(join_audit.get(\"severity\", \"warning\")).lower()\n",
    "join_interpretation = str(join_audit.get(\"join_interpretation\", \"partial_adjudicated_overlap\"))\n",
    "if n_dropped > 0 or n_unmatched_nlp > 0:\n",
    "    if join_severity == \"info\":\n",
    "        print(\n",
    "            \"INFO: Non-identical overlap is expected for subset benchmarking \"\n",
    "            f\"({join_interpretation}). Metrics are computed on matched rows only.\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            \"WARNING: Non-identical overlap between adjudicated and NLP cohorts \"\n",
    "            f\"({join_interpretation}). Metrics are computed on matched rows only.\"\n",
    "        )\n",
    "\n",
    "# Step 4: Convert joined rows to unordered category sets for both sources.\n",
    "r3_sets_join = extract_rater_sets_from_df(df_join, rater_col_prefix=\"annot3\")\n",
    "nlp_sets = extract_nlp_sets_from_df(df_join, min_sim=NLP_MIN_SIM)\n",
    "\n",
    "assert len(r3_sets_join) == len(df_join), \"R3 joined set count mismatch\"\n",
    "assert len(nlp_sets) == len(df_join), \"NLP set count mismatch\"\n",
    "\n",
    "# Step 5: Set-level agreement gives intuitive visit-level similarity.\n",
    "pair_df, pair_summary = summarize_set_agreement(r3_sets_join, nlp_sets)\n",
    "assert int(pair_summary[\"N_items\"]) == len(df_join), \"R3/NLP summary N_items mismatch\"\n",
    "assert_rate_bounds(pair_summary[[\"exact_rate\", \"partial_rate\", \"none_rate\"]], \"R3/NLP set rates\")\n",
    "assert_rate_bounds(pair_summary[[\"mean_jaccard\", \"mean_overlap\", \"mean_f1_set\", \"micro_f1_set\"]], \"R3/NLP set similarity metrics\")\n",
    "\n",
    "# Step 6: Record category-level differences per visit for error analysis.\n",
    "def list_diff(a: Set[str], b: Set[str]) -> Tuple[List[str], List[str]]:\n",
    "    return sorted(a - b), sorted(b - a)\n",
    "\n",
    "missed = []\n",
    "added = []\n",
    "for a, b in zip(r3_sets_join, nlp_sets):\n",
    "    m, ad = list_diff(a, b)\n",
    "    missed.append(\"; \".join(m))\n",
    "    added.append(\"; \".join(ad))\n",
    "\n",
    "visit_metrics = df_join[r3_key_cols].copy()\n",
    "visit_metrics[\"exact\"] = pair_df[\"exact\"]\n",
    "visit_metrics[\"partial\"] = pair_df[\"partial\"]\n",
    "visit_metrics[\"none\"] = pair_df[\"none\"]\n",
    "visit_metrics[\"jaccard\"] = pair_df[\"jaccard\"]\n",
    "visit_metrics[\"overlap\"] = pair_df[\"overlap\"]\n",
    "visit_metrics[\"f1_set\"] = pair_df[\"f1_set\"]\n",
    "visit_metrics[\"r3_size\"] = pair_df[\"len_a\"]\n",
    "visit_metrics[\"nlp_size\"] = pair_df[\"len_b\"]\n",
    "visit_metrics[\"missed_by_nlp\"] = missed\n",
    "visit_metrics[\"added_by_nlp\"] = added\n",
    "\n",
    "assert_expected_columns(\n",
    "    visit_metrics,\n",
    "    [\n",
    "        \"hadm_id\",\n",
    "        \"subject_id\",\n",
    "        \"exact\",\n",
    "        \"partial\",\n",
    "        \"none\",\n",
    "        \"jaccard\",\n",
    "        \"overlap\",\n",
    "        \"f1_set\",\n",
    "        \"r3_size\",\n",
    "        \"nlp_size\",\n",
    "        \"missed_by_nlp\",\n",
    "        \"added_by_nlp\",\n",
    "    ],\n",
    "    \"visit_metrics\",\n",
    ")\n",
    "assert len(visit_metrics) == len(df_join), \"visit_metrics row count mismatch\"\n",
    "\n",
    "# Step 7: Compute per-category chance-corrected metrics on binary matrices.\n",
    "m_r3 = flatten_binary_decisions_single(r3_sets_join, CANONICAL_CATS)\n",
    "m_nlp = flatten_binary_decisions_single(nlp_sets, CANONICAL_CATS)\n",
    "\n",
    "assert m_r3.shape == (len(df_join), len(CANONICAL_CATS)), \"R3 binary matrix shape mismatch\"\n",
    "assert m_nlp.shape == (len(df_join), len(CANONICAL_CATS)), \"NLP binary matrix shape mismatch\"\n",
    "\n",
    "pw_stats = pairwise_binary_agreement_stats(\n",
    "    m_r3,\n",
    "    m_nlp,\n",
    "    categories=CANONICAL_CATS,\n",
    "    include_confusion_counts=True,\n",
    ")\n",
    "chance_summary = summarize_kappa_ac1(pw_stats)\n",
    "\n",
    "assert_expected_columns(\n",
    "    pw_stats,\n",
    "    [\n",
    "        \"category_ix\",\n",
    "        \"category\",\n",
    "        \"N\",\n",
    "        \"tp\",\n",
    "        \"tn\",\n",
    "        \"fp\",\n",
    "        \"fn\",\n",
    "        \"prevalence_r3\",\n",
    "        \"prevalence_nlp\",\n",
    "        \"percent_agreement\",\n",
    "        \"cohen_kappa\",\n",
    "        \"gwet_ac1\",\n",
    "    ],\n",
    "    \"pw_stats\",\n",
    ")\n",
    "assert len(pw_stats) == len(CANONICAL_CATS), \"pw_stats row count mismatch with categories\"\n",
    "assert_rate_bounds(pw_stats[\"percent_agreement\"], \"pw_stats percent_agreement\")\n",
    "assert_rate_bounds(pw_stats[\"gwet_ac1\"], \"pw_stats gwet_ac1\")\n",
    "\n",
    "# Step 8: Write outputs using stable filenames for downstream workflows.\n",
    "visit_metrics.to_csv(NLP_OUTPUT_DIR / \"R3_vs_NLP_set_metrics_by_visit.csv\", index=False)\n",
    "pw_stats.to_csv(NLP_OUTPUT_DIR / \"R3_vs_NLP_binary_stats_by_category.csv\", index=False)\n",
    "unmatched_adjudicated.to_csv(\n",
    "    NLP_OUTPUT_DIR / \"R3_vs_NLP_unmatched_adjudicated_keys.csv\",\n",
    "    index=False,\n",
    ")\n",
    "unmatched_nlp.to_csv(\n",
    "    NLP_OUTPUT_DIR / \"R3_vs_NLP_unmatched_nlp_keys.csv\",\n",
    "    index=False,\n",
    ")\n",
    "(NLP_OUTPUT_DIR / \"R3_vs_NLP_join_audit.json\").write_text(\n",
    "    json.dumps(join_audit, indent=2)\n",
    ")\n",
    "\n",
    "summary_lines = []\n",
    "summary_lines.append(\"=== R3 vs NLP join audit ===\")\n",
    "summary_lines.append(pd.Series(join_audit).to_string())\n",
    "summary_lines.append(\"=== R3 (adjudicator) vs NLP: set-level (order-invariant) ===\")\n",
    "summary_lines.append(pair_summary.to_string())\n",
    "summary_lines.append(\"\\n=== R3 vs NLP: chance-corrected (binary per category) ===\")\n",
    "summary_lines.append(chance_summary.to_string())\n",
    "summary_text = \"\\n\\n\".join(summary_lines)\n",
    "\n",
    "(NLP_OUTPUT_DIR / \"R3_vs_NLP_summary.txt\").write_text(summary_text)\n",
    "\n",
    "print(summary_text)\n",
    "print(\"\\nArtifacts written to:\", NLP_OUTPUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc6897c",
   "metadata": {},
   "source": [
    "## Draft text: validation metrics (edit as needed)\n",
    "\n",
    "For multi-label RFV assignment, we treat each visit's labels as an unordered set and summarize agreement with set-based metrics (exact/partial/none, Jaccard, overlap, and set-F1). We report both mean set-F1 (average per visit) and micro set-F1 (pooled over all labels). For chance-corrected agreement, we expand the multi-label task into 17 one-vs-rest binary decisions (one per NHAMCS top-level category) and compute per-category Cohen's kappa and Gwet's AC1; macro-averages across categories are reported, and a multi-rater AC1 is computed across all three reviewers. Model-vs-adjudicator agreement uses the same set-level metrics and per-category binary stats, enabling per-category performance reporting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2206fbe6",
   "metadata": {},
   "source": [
    "**Rationale:** Provide manuscript-ready wording for the methods and validation sections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44a53743",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T05:02:22.575874Z",
     "iopub.status.busy": "2026-02-10T05:02:22.575779Z",
     "iopub.status.idle": "2026-02-10T05:02:22.592651Z",
     "shell.execute_reply": "2026-02-10T05:02:22.592424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest AC1 categories (R3 vs NLP):\n",
      "                           category   N  percent_agreement  cohen_kappa  gwet_ac1\n",
      "Diseases (patient–stated diagnosis) 160            0.82500     0.000000  0.791744\n",
      "         Injuries & adverse effects 160            0.92500     0.672131  0.902844\n",
      "                  Symptom – Nervous 160            0.93750     0.783608  0.912127\n",
      "            Symptom – Genitourinary 160            0.92500     0.233227  0.917127\n",
      "               Abnormal test result 160            0.93125     0.143969  0.925438\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Analysis C: Per-category summary tables used for manuscript-style reporting\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# This helper pools pairwise reviewer tables and reports mean metrics by\n",
    "# category, which is easier to read than three separate pair tables.\n",
    "def summarize_per_category_across_pairs(\n",
    "    pw_list: List[pd.DataFrame],\n",
    "    pair_names: List[str],\n",
    ") -> pd.DataFrame:\n",
    "    frames = []\n",
    "    for frame, name in zip(pw_list, pair_names):\n",
    "        tmp = frame.copy()\n",
    "        tmp[\"pair\"] = name\n",
    "        if \"category_name\" not in tmp.columns:\n",
    "            tmp[\"category_name\"] = tmp.get(\"category\")\n",
    "        frames.append(tmp)\n",
    "\n",
    "    all_pw = pd.concat(frames, ignore_index=True)\n",
    "    summary = (\n",
    "        all_pw.groupby(\"category_name\", as_index=False)\n",
    "        .agg(\n",
    "            mean_kappa=(\"cohen_kappa\", \"mean\"),\n",
    "            mean_ac1=(\"gwet_ac1\", \"mean\"),\n",
    "            mean_percent_agreement=(\"percent_agreement\", \"mean\"),\n",
    "            N=(\"N\", \"sum\"),\n",
    "        )\n",
    "        .sort_values(\"mean_ac1\")\n",
    "    )\n",
    "    return summary\n",
    "\n",
    "\n",
    "# Defensive check: this cell should run only after the analysis cells above.\n",
    "for name in (\"pw12\", \"pw13\", \"pw23\", \"pw_stats\"):\n",
    "    if name not in globals():\n",
    "        raise RuntimeError(\n",
    "            f\"Expected variable '{name}' is missing. Run the analysis cells above first.\"\n",
    "        )\n",
    "\n",
    "per_cat_rater_summary = summarize_per_category_across_pairs(\n",
    "    [pw12, pw13, pw23],\n",
    "    [\"R1_R2\", \"R1_R3\", \"R2_R3\"],\n",
    ")\n",
    "assert_expected_columns(\n",
    "    per_cat_rater_summary,\n",
    "    [\"category_name\", \"mean_kappa\", \"mean_ac1\", \"mean_percent_agreement\", \"N\"],\n",
    "    \"per_cat_rater_summary\",\n",
    ")\n",
    "per_cat_rater_summary.to_csv(RATERS_OUTPUT_DIR / \"per_category_rater_agreement_summary.csv\", index=False)\n",
    "\n",
    "# Sorting by AC1 surfaces lowest-agreement categories first for targeted\n",
    "# qualitative review.\n",
    "per_cat_r3_nlp = pw_stats.copy().sort_values(\"gwet_ac1\")\n",
    "assert_expected_columns(\n",
    "    per_cat_r3_nlp,\n",
    "    [\n",
    "        \"category_ix\",\n",
    "        \"category\",\n",
    "        \"N\",\n",
    "        \"tp\",\n",
    "        \"tn\",\n",
    "        \"fp\",\n",
    "        \"fn\",\n",
    "        \"prevalence_r3\",\n",
    "        \"prevalence_nlp\",\n",
    "        \"percent_agreement\",\n",
    "        \"cohen_kappa\",\n",
    "        \"gwet_ac1\",\n",
    "    ],\n",
    "    \"per_cat_r3_nlp\",\n",
    ")\n",
    "assert per_cat_r3_nlp[\"gwet_ac1\"].is_monotonic_increasing, \"R3 vs NLP table should be sorted by gwet_ac1\"\n",
    "per_cat_r3_nlp.to_csv(NLP_OUTPUT_DIR / \"R3_vs_NLP_binary_stats_by_category.csv\", index=False)\n",
    "\n",
    "# Quick glance: lowest agreement categories\n",
    "_display_cols = [\"category\", \"N\", \"percent_agreement\", \"cohen_kappa\", \"gwet_ac1\"]\n",
    "print(\"Lowest AC1 categories (R3 vs NLP):\")\n",
    "print(per_cat_r3_nlp[_display_cols].head(5).to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hypercap-CC-NLP (3.11.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
