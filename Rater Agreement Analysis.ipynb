{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0367c750",
   "metadata": {},
   "source": [
    "# Code to calculate agreement between Monique and Reyan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f8d736",
   "metadata": {},
   "source": [
    "Warning - this is all vibe-coded. Need to ensure it is accureate/correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b288c8f5",
   "metadata": {},
   "source": [
    "# 1  Annotation Framework: Multi-Label, Order-Invariant Agreement\n",
    "\n",
    "## 1.1  Context and purpose\n",
    "Each emergency-department visit receives up to five Reason-for-Visit (RFV1\u2013RFV5) labels drawn from the **17 top-level NHAMCS RVC groups** defined in the *MIMIC IV Chief Complaint \u2192 NHAMCS Reason-for-Visit Reviewer Manual v0.3*.\n",
    "These represent patient-stated reasons, not clinician diagnoses.\n",
    "\n",
    "Our goal is to quantify **inter-rater agreement** among multiple human annotators who each output an **unordered subset** of these groups for every visit.\n",
    "For analysis we ignore RFV ordering\u2014treating the set\n",
    "\\( S_{jr} = \\{\\text{categories assigned by rater r to visit j}\\} \\)\n",
    "as a simple set of 0\u20135 elements.\n",
    "\n",
    "## 1.2  Visit-level agreement classes\n",
    "For any two raters A and B:\n",
    "\n",
    "| Class | Definition | Intuition |\n",
    "|-------|-------------|-----------|\n",
    "| **Full** | \\(S_A = S_B\\) | Complete category match |\n",
    "| **Partial** | \\(S_A \\cap S_B \\neq \\varnothing\\) and \\(S_A \\ne S_B\\) | Share at least one category |\n",
    "| **None** | \\(S_A \\cap S_B = \\varnothing\\) | Disjoint label sets |\n",
    "\n",
    "For three raters,\n",
    "- **Full** = all identical;\n",
    "- **Partial** = any overlap between any pair;\n",
    "- **None** = pairwise disjoint.\n",
    "\n",
    "## 1.3  Set-similarity metrics\n",
    "Because order is meaningless, comparisons use **set-based overlaps**:\n",
    "\n",
    "| Metric | Formula | Notes |\n",
    "|---------|----------|-------|\n",
    "| **Jaccard** | \\(|S_A\u2229S_B| / |S_A\u222aS_B|\\) | Measures intersection over union |\n",
    "| **Overlap Coefficient** | \\(|S_A\u2229S_B| / \\min(|S_A|, |S_B|)\\) | Robust when one rater lists fewer items |\n",
    "| **F\u2081 (Set)** | \\(2|S_A\u2229S_B| / (|S_A|+|S_B|)\\) | Equivalent to S\u00f8rensen\u2013Dice |\n",
    "| **Micro-F\u2081 (Set)** | \\(2\u2211|S_A\u2229S_B| / \u2211(|S_A|+|S_B|)\\) | Corpus-level summary |\n",
    "\n",
    "These capture graded similarity beyond binary full/none counts.\n",
    "\n",
    "## 1.4  Three-rater adjudication logic\n",
    "When a third reviewer adjudicates:\n",
    "- Compute the distribution of *full / partial / none* across all three.\n",
    "- For disagreements between raters 1 and 2, evaluate whether rater 3:\n",
    "  - matches either rater,\n",
    "  - matches their union or intersection,\n",
    "  - introduces any new category, or\n",
    "  - provides a subset of the union.\n",
    "\n",
    "This quantifies how the final \u201creference standard\u201d was reached.\n",
    "\n",
    "\n",
    "# 2  Agreement Beyond Chance (Chance-Corrected Metrics)\n",
    "\n",
    "## 2.1  Binary expansion per category\n",
    "Convert the multi-label sets into a binary decision matrix over the 17 canonical RVC groups:\n",
    "for each visit \u00d7 rater \u00d7 category, record 1 if the category is present.\n",
    "This yields parallel binary classification tasks where standard chance-corrected statistics apply.\n",
    "\n",
    "## 2.2  Why Gwet\u2019s AC1\n",
    "Cohen\u2019s \u03ba is sensitive to marginal prevalence\u2014especially when most visits lack a given category (the *kappa paradox*).\n",
    "Gwet\u2019s AC1 provides a more stable estimate of \u201cagreement beyond chance\u201d in such sparse settings.\n",
    "\n",
    "For each category *g* with *m* raters:\n",
    "\n",
    "\\[\n",
    "A_j =\n",
    "\\frac{\\binom{n^+_j}{2} + \\binom{m-n^+_j}{2}}{\\binom{m}{2}},\n",
    "\\qquad\n",
    "P_o = \\text{mean}(A_j),\n",
    "\\qquad\n",
    "p = \\frac{1}{Nm}\\sum_{j,r} X_{jr}^{(g)},\n",
    "\\]\n",
    "\\[\n",
    "\\text{AC1} = \\frac{P_o - 2p(1-p)}{1 - 2p(1-p)}.\n",
    "\\]\n",
    "\n",
    "Report **per-category AC1** and **macro-average** across categories.\n",
    "Also compute **pairwise \u03ba** (for comparison) and **percent agreement** to provide a full view.\n",
    "\n",
    "## 2.3  Interpretation\n",
    "| Metric | What it captures | Typical reporting |\n",
    "|---------|------------------|-------------------|\n",
    "| **Percent agreement** | Raw consistency, ignores chance | Always include |\n",
    "| **Cohen\u2019s \u03ba** | Chance-corrected for two raters | Supplemental |\n",
    "| **Gwet\u2019s AC1** | Chance-corrected, robust to imbalance | Primary statistic |\n",
    "| **Multi-rater AC1** | Extension of AC1 to m > 2 | Summary across all raters |\n",
    "\n",
    "High percent agreement with high AC1 (\u2248 0.9\u20131.0) implies strong, reproducible labeling of the RVC groups defined in the Reviewer Manual v0.3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06a74025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Pairwise set-level (order-invariant) ===\n",
      "\n",
      "R1 vs R2:\n",
      "N_items         160.000000\n",
      "exact_rate        0.806250\n",
      "partial_rate      0.131250\n",
      "none_rate         0.062500\n",
      "mean_jaccard      0.856250\n",
      "mean_overlap      0.882292\n",
      "mean_f1_set       0.877440\n",
      "micro_f1_set      0.873950\n",
      "mean_len_a        1.493750\n",
      "mean_len_b        1.481250\n",
      "\n",
      "R1 vs R3:\n",
      "N_items         160.000000\n",
      "exact_rate        0.850000\n",
      "partial_rate      0.075000\n",
      "none_rate         0.075000\n",
      "mean_jaccard      0.879167\n",
      "mean_overlap      0.892708\n",
      "mean_f1_set       0.891190\n",
      "micro_f1_set      0.897275\n",
      "mean_len_a        1.493750\n",
      "mean_len_b        1.487500\n",
      "\n",
      "R2 vs R3:\n",
      "N_items         160.000000\n",
      "exact_rate        0.831250\n",
      "partial_rate      0.112500\n",
      "none_rate         0.056250\n",
      "mean_jaccard      0.876042\n",
      "mean_overlap      0.900000\n",
      "mean_f1_set       0.894583\n",
      "micro_f1_set      0.892632\n",
      "mean_len_a        1.481250\n",
      "mean_len_b        1.487500\n",
      "\n",
      "=== Three-rater set-level (full/partial/none) ===\n",
      "N_items         160.00000\n",
      "full_rate         0.75625\n",
      "partial_rate      0.23125\n",
      "none_rate         0.01250\n",
      "\n",
      "=== Chance-corrected (binary per category) ===\n",
      "R1 vs R2:\n",
      "macro_cohen_kappa          0.734211\n",
      "macro_gwet_ac1             0.978476\n",
      "macro_percent_agreement    0.982353\n",
      "micro_percent_agreement    0.982353\n",
      "\n",
      "R1 vs R3:\n",
      "macro_cohen_kappa          0.796093\n",
      "macro_gwet_ac1             0.979976\n",
      "macro_percent_agreement    0.984191\n",
      "micro_percent_agreement    0.984191\n",
      "\n",
      "R2 vs R3:\n",
      "macro_cohen_kappa          0.773404\n",
      "macro_gwet_ac1             0.979294\n",
      "macro_percent_agreement    0.983456\n",
      "micro_percent_agreement    0.983456\n",
      "\n",
      "=== Multi-rater AC1 (3 raters) ===\n",
      "macro_gwet_ac1=0.9793\n",
      "macro_percent_agreement=0.9833\n",
      "\n",
      "=== Adjudication (R3) for R1\u2260R2 ===\n",
      "N_disagreements                31.000000\n",
      "r3_equals_r1_rate               0.483871\n",
      "r3_equals_r2_rate               0.387097\n",
      "r3_equals_union_rate            0.096774\n",
      "r3_equals_intersection_rate     0.000000\n",
      "r3_introduces_new_rate          0.096774\n",
      "r3_subset_of_union_rate         0.903226\n",
      "\n",
      "\n",
      "Outputs written to: /Users/reblocke/Research/Hypercap-CC-NLP/Annotation/Full Annotations/Agreement Metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/mimiciv-tabular/lib/python3.11/site-packages/openpyxl/worksheet/_read_only.py:85: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  for idx, row in parser.parse():\n"
     ]
    }
   ],
   "source": [
    "# Agreement analysis for multi-label (order-invariant) RFV annotations.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Set\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "WORK_DIR = Path(os.getenv(\"WORK_DIR\", Path.cwd())).expanduser().resolve()\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "EXCEL_PATH = WORK_DIR / \"Annotation/Final 2025-10-14 Annotation Sample.xlsx\"  # update if needed\n",
    "\n",
    "SHEET = \"cohort_cc_sample\"\n",
    "DATA_SHEET = \"Data\"  # canonical category names in column \"RVC Categories\"\n",
    "\n",
    "outdir = WORK_DIR / \"Annotation/Full Annotations/Agreement Metrics\"\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def load_categories_from_sheet(xlsx_path: str, sheet: str = \"Data\", col: str = \"RVC Categories\") -> List[str]:\n",
    "    try:\n",
    "        cat_df = pd.read_excel(xlsx_path, sheet_name=sheet)\n",
    "        return [c for c in cat_df[col].dropna().astype(str).tolist() if c.strip()]\n",
    "    except Exception:\n",
    "        # Fallback in case the \"Data\" sheet is absent\n",
    "        return [\n",
    "            'Injuries & adverse effects','Symptom \u2013 Respiratory','Symptom \u2013 Circulatory',\n",
    "            'Symptom \u2013 Nervous','Symptom \u2013 Digestive','Symptom \u2013 Genitourinary',\n",
    "            'Symptom \u2013 Musculoskeletal','Symptom \u2013 Skin/Hair/Nails','Symptom \u2013 Eye/Ear',\n",
    "            'Symptom \u2013 General','Symptom \u2013 Psychological','Diseases (patient-stated diagnosis)',\n",
    "            'Abnormal test result','Diagnostic/Screening/Preventive','Treatment/Medication',\n",
    "            'Administrative','Uncodable/Unknown',\n",
    "        ]\n",
    "\n",
    "CANONICAL_CATS = load_categories_from_sheet(EXCEL_PATH, DATA_SHEET, \"RVC Categories\")\n",
    "\n",
    "def normalize_label(s: str) -> str:\n",
    "    \"\"\"Normalize minor punctuation/spacing so category strings match reliably.\"\"\"\n",
    "    if pd.isna(s): return None\n",
    "    s = str(s).strip()\n",
    "    if not s: return None\n",
    "    s = s.replace(\" - \", \" \u2013 \").replace(\"\u2014\", \"\u2013\").replace(\"-\", \"\u2013\")\n",
    "    s = \" \".join(s.split())\n",
    "    return s\n",
    "\n",
    "def extract_rater_sets(df: pd.DataFrame, rater: int) -> List[Set[str]]:\n",
    "    \"\"\"Return a list of sets: one set of categories per row for the given rater.\"\"\"\n",
    "    cols = [f\"annot{rater}_rvs{i}_cat\" for i in range(1, 6) if f\"annot{rater}_rvs{i}_cat\" in df.columns]\n",
    "    sets = []\n",
    "    for _, row in df[cols].iterrows():\n",
    "        labels = set()\n",
    "        for c in cols:\n",
    "            lab = normalize_label(row[c])\n",
    "            if lab:\n",
    "                labels.add(lab)\n",
    "        sets.append(labels)\n",
    "    return sets\n",
    "\n",
    "# --- set-based similarities ---\n",
    "def jaccard(a: Set[str], b: Set[str]) -> float:\n",
    "    u = len(a | b)\n",
    "    return 1.0 if u == 0 else len(a & b) / u\n",
    "\n",
    "def overlap_coeff(a: Set[str], b: Set[str]) -> float:\n",
    "    m = min(len(a), len(b))\n",
    "    return 1.0 if m == 0 and len(a) == len(b) == 0 else (0.0 if m == 0 else len(a & b) / m)\n",
    "\n",
    "def f1_set(a: Set[str], b: Set[str]) -> float:\n",
    "    denom = len(a) + len(b)\n",
    "    return 1.0 if denom == 0 else 2 * len(a & b) / denom\n",
    "\n",
    "def classify_pair(a: Set[str], b: Set[str]) -> str:\n",
    "    if a == b: return \"full\"\n",
    "    return \"partial\" if (a & b) else \"none\"\n",
    "\n",
    "def classify_three(a: Set[str], b: Set[str], c: Set[str]) -> str:\n",
    "    if a == b == c: return \"full\"\n",
    "    return \"partial\" if (a & b) or (a & c) or (b & c) else \"none\"\n",
    "\n",
    "def micro_f1_raters(A: List[Set[str]], B: List[Set[str]]) -> float:\n",
    "    tp = sum(len(a & b) for a, b in zip(A, B))\n",
    "    denom = sum(len(a) + len(b) for a, b in zip(A, B))\n",
    "    return 1.0 if denom == 0 else 2 * tp / denom\n",
    "\n",
    "# --- multi-label -> binary matrices over categories ---\n",
    "def flatten_binary_decisions(raters_sets: Dict[str, List[Set[str]]], categories: List[str]) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Return {rater: matrix (n_items, n_categories)} with 1/0 presence for each category.\"\"\"\n",
    "    n = len(next(iter(raters_sets.values())))\n",
    "    k = len(categories)\n",
    "    idx = {c: i for i, c in enumerate(categories)}\n",
    "    out = {}\n",
    "    for rater, sets in raters_sets.items():\n",
    "        M = np.zeros((n, k), dtype=int)\n",
    "        for j, s in enumerate(sets):\n",
    "            for lab in s:\n",
    "                lab = normalize_label(lab)\n",
    "                if lab in idx:\n",
    "                    M[j, idx[lab]] = 1\n",
    "        out[rater] = M\n",
    "    return out\n",
    "\n",
    "# --- pairwise chance-corrected (per category) ---\n",
    "def pairwise_binary_agreement_stats(M1: np.ndarray, M2: np.ndarray) -> pd.DataFrame:\n",
    "    \"\"\"Per-category percent agreement, Cohen's kappa, and Gwet's AC1 (binary).\"\"\"\n",
    "    n, k = M1.shape\n",
    "    rows = []\n",
    "    for c in range(k):\n",
    "        y1, y2 = M1[:, c], M2[:, c]\n",
    "        tp = int(((y1 == 1) & (y2 == 1)).sum())\n",
    "        tn = int(((y1 == 0) & (y2 == 0)).sum())\n",
    "        fp = int(((y1 == 0) & (y2 == 1)).sum())\n",
    "        fn = int(((y1 == 1) & (y2 == 0)).sum())\n",
    "        N = tp + tn + fp + fn\n",
    "        if N == 0:\n",
    "            rows.append(dict(category=c, N=0, percent_agreement=np.nan, cohen_kappa=np.nan, gwet_ac1=np.nan))\n",
    "            continue\n",
    "        Po = (tp + tn) / N\n",
    "        p1, p2 = y1.mean(), y2.mean()\n",
    "        Pe_kappa = p1 * p2 + (1 - p1) * (1 - p2)\n",
    "        kappa = (Po - Pe_kappa) / (1 - Pe_kappa) if (1 - Pe_kappa) != 0 else np.nan\n",
    "        pbar = (p1 + p2) / 2\n",
    "        Pe_ac1 = 2 * pbar * (1 - pbar)\n",
    "        ac1 = (Po - Pe_ac1) / (1 - Pe_ac1) if (1 - Pe_ac1) != 0 else np.nan\n",
    "        rows.append(dict(category=c, N=N, percent_agreement=Po, cohen_kappa=kappa, gwet_ac1=ac1))\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# --- multi-rater AC1 (binary, per category) ---\n",
    "def multirater_ac1_per_category(Ms: List[np.ndarray]) -> pd.DataFrame:\n",
    "    \"\"\"Gwet's AC1 for m>=2 raters, per category, using average pairwise agreement.\"\"\"\n",
    "    m = len(Ms)\n",
    "    n, k = Ms[0].shape\n",
    "    stack = np.stack(Ms, axis=0)  # (m, n, k)\n",
    "    denom_pairs = m * (m - 1) / 2\n",
    "    rows = []\n",
    "    for c in range(k):\n",
    "        M_c = stack[:, :, c]           # (m, n)\n",
    "        npos = M_c.sum(axis=0)         # length n\n",
    "        nneg = m - npos\n",
    "        Aj = (npos * (npos - 1) / 2 + nneg * (nneg - 1) / 2) / denom_pairs\n",
    "        Po = Aj.mean()\n",
    "        p = npos.sum() / (n * m)\n",
    "        Pe = 2 * p * (1 - p)\n",
    "        ac1 = (Po - Pe) / (1 - Pe) if (1 - Pe) != 0 else np.nan\n",
    "        rows.append(dict(category=c, percent_agreement=Po, gwet_ac1=ac1, prevalence=p))\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def summarize_set_agreement(r1_sets: List[Set[str]], r2_sets: List[Set[str]]) -> pd.DataFrame:\n",
    "    recs = []\n",
    "    for a, b in zip(r1_sets, r2_sets):\n",
    "        recs.append({\n",
    "            \"exact\": int(a == b),\n",
    "            \"partial\": int((a != b) and (len(a & b) > 0)),\n",
    "            \"none\": int(len(a & b) == 0),\n",
    "            \"jaccard\": jaccard(a, b),\n",
    "            \"overlap\": overlap_coeff(a, b),\n",
    "            \"f1_set\": f1_set(a, b),\n",
    "            \"len_a\": len(a),\n",
    "            \"len_b\": len(b),\n",
    "            \"len_inter\": len(a & b),\n",
    "            \"len_union\": len(a | b),\n",
    "        })\n",
    "    df = pd.DataFrame(recs)\n",
    "    summary = pd.Series({\n",
    "        \"N_items\": len(df),\n",
    "        \"exact_rate\": df[\"exact\"].mean(),\n",
    "        \"partial_rate\": df[\"partial\"].mean(),\n",
    "        \"none_rate\": df[\"none\"].mean(),\n",
    "        \"mean_jaccard\": df[\"jaccard\"].mean(),\n",
    "        \"mean_overlap\": df[\"overlap\"].mean(),\n",
    "        \"mean_f1_set\": df[\"f1_set\"].mean(),\n",
    "        \"micro_f1_set\": micro_f1_raters(r1_sets, r2_sets),\n",
    "        \"mean_len_a\": df[\"len_a\"].mean(),\n",
    "        \"mean_len_b\": df[\"len_b\"].mean(),\n",
    "    })\n",
    "    return df, summary\n",
    "\n",
    "def summarize_three_way(r1: List[Set[str]], r2: List[Set[str]], r3: List[Set[str]]) -> pd.Series:\n",
    "    labels = [classify_three(a, b, c) for a, b, c in zip(r1, r2, r3)]\n",
    "    N = len(labels)\n",
    "    counts = Counter(labels)\n",
    "    return pd.Series({\n",
    "        \"N_items\": N,\n",
    "        \"full_rate\": counts.get(\"full\", 0) / N,\n",
    "        \"partial_rate\": counts.get(\"partial\", 0) / N,\n",
    "        \"none_rate\": counts.get(\"none\", 0) / N\n",
    "    })\n",
    "\n",
    "def adjudication_resolution(r1, r2, r3) -> pd.Series:\n",
    "    recs = []\n",
    "    for a, b, c in zip(r1, r2, r3):\n",
    "        if a == b: \n",
    "            continue\n",
    "        u, inter = (a | b), (a & b)\n",
    "        recs.append({\n",
    "            \"r3_equals_r1\": int(c == a),\n",
    "            \"r3_equals_r2\": int(c == b),\n",
    "            \"r3_equals_union\": int(c == u),\n",
    "            \"r3_equals_intersection\": int((len(inter) > 0) and (c == inter)),\n",
    "            \"r3_introduces_new\": int(len(c - u) > 0),\n",
    "            \"r3_subset_of_union\": int(c <= u),\n",
    "        })\n",
    "    if not recs:\n",
    "        return pd.Series({\"N_disagreements\": 0})\n",
    "    df = pd.DataFrame(recs)\n",
    "    return pd.Series({\n",
    "        \"N_disagreements\": len(df),\n",
    "        \"r3_equals_r1_rate\": df[\"r3_equals_r1\"].mean(),\n",
    "        \"r3_equals_r2_rate\": df[\"r3_equals_r2\"].mean(),\n",
    "        \"r3_equals_union_rate\": df[\"r3_equals_union\"].mean(),\n",
    "        \"r3_equals_intersection_rate\": df[\"r3_equals_intersection\"].mean(),\n",
    "        \"r3_introduces_new_rate\": df[\"r3_introduces_new\"].mean(),\n",
    "        \"r3_subset_of_union_rate\": df[\"r3_subset_of_union\"].mean(),\n",
    "    })\n",
    "\n",
    "# -----------------------------\n",
    "# Run\n",
    "# -----------------------------\n",
    "df = pd.read_excel(EXCEL_PATH, sheet_name=SHEET)\n",
    "\n",
    "r1_sets = extract_rater_sets(df, 1)\n",
    "r2_sets = extract_rater_sets(df, 2)\n",
    "r3_sets = extract_rater_sets(df, 3)\n",
    "\n",
    "pair12_df, pair12_summary = summarize_set_agreement(r1_sets, r2_sets)\n",
    "pair13_df, pair13_summary = summarize_set_agreement(r1_sets, r3_sets)\n",
    "pair23_df, pair23_summary = summarize_set_agreement(r2_sets, r3_sets)\n",
    "\n",
    "three_summary = summarize_three_way(r1_sets, r2_sets, r3_sets)\n",
    "\n",
    "raters_sets = {\"r1\": r1_sets, \"r2\": r2_sets, \"r3\": r3_sets}\n",
    "bin_mats = flatten_binary_decisions(raters_sets, CANONICAL_CATS)\n",
    "\n",
    "pw12 = pairwise_binary_agreement_stats(bin_mats[\"r1\"], bin_mats[\"r2\"])\n",
    "pw13 = pairwise_binary_agreement_stats(bin_mats[\"r1\"], bin_mats[\"r3\"])\n",
    "pw23 = pairwise_binary_agreement_stats(bin_mats[\"r2\"], bin_mats[\"r3\"])\n",
    "\n",
    "name_map = {i: c for i, c in enumerate(CANONICAL_CATS)}\n",
    "for df_pw in (pw12, pw13, pw23):\n",
    "    df_pw[\"category_name\"] = df_pw[\"category\"].map(name_map)\n",
    "\n",
    "multi_ac1 = multirater_ac1_per_category([bin_mats[\"r1\"], bin_mats[\"r2\"], bin_mats[\"r3\"]])\n",
    "multi_ac1[\"category_name\"] = multi_ac1[\"category\"].map(name_map)\n",
    "\n",
    "def summarize_kappa_ac1(df_pw: pd.DataFrame) -> pd.Series:\n",
    "    return pd.Series({\n",
    "        \"macro_cohen_kappa\": df_pw[\"cohen_kappa\"].mean(),\n",
    "        \"macro_gwet_ac1\": df_pw[\"gwet_ac1\"].mean(),\n",
    "        \"macro_percent_agreement\": df_pw[\"percent_agreement\"].mean(),\n",
    "        \"micro_percent_agreement\": (df_pw[\"N\"] * df_pw[\"percent_agreement\"]).sum() / df_pw[\"N\"].sum()\n",
    "    })\n",
    "\n",
    "pair12_chance = summarize_kappa_ac1(pw12)\n",
    "pair13_chance = summarize_kappa_ac1(pw13)\n",
    "pair23_chance = summarize_kappa_ac1(pw23)\n",
    "\n",
    "multi_macro_ac1 = multi_ac1[\"gwet_ac1\"].mean()\n",
    "multi_macro_agree = multi_ac1[\"percent_agreement\"].mean()\n",
    "\n",
    "# -----------------------------\n",
    "# Outputs\n",
    "# -----------------------------\n",
    "\n",
    "pair12_df.to_csv(os.path.join(outdir, \"pair_R1_R2_set_metrics.csv\"), index=False)\n",
    "pair13_df.to_csv(os.path.join(outdir, \"pair_R1_R3_set_metrics.csv\"), index=False)\n",
    "pair23_df.to_csv(os.path.join(outdir, \"pair_R2_R3_set_metrics.csv\"), index=False)\n",
    "\n",
    "pw12.to_csv(os.path.join(outdir, \"pair_R1_R2_binary_stats.csv\"), index=False)\n",
    "pw13.to_csv(os.path.join(outdir, \"pair_R1_R3_binary_stats.csv\"), index=False)\n",
    "pw23.to_csv(os.path.join(outdir, \"pair_R2_R3_binary_stats.csv\"), index=False)\n",
    "\n",
    "multi_ac1.to_csv(os.path.join(outdir, \"all3_multirater_ac1_by_category.csv\"), index=False)\n",
    "\n",
    "adj_summary = adjudication_resolution(r1_sets, r2_sets, r3_sets)\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "=== Pairwise set-level (order-invariant) ===\n",
    "\n",
    "R1 vs R2:\n",
    "{pair12_summary.to_string()}\n",
    "\n",
    "R1 vs R3:\n",
    "{pair13_summary.to_string()}\n",
    "\n",
    "R2 vs R3:\n",
    "{pair23_summary.to_string()}\n",
    "\n",
    "=== Three-rater set-level (full/partial/none) ===\n",
    "{three_summary.to_string()}\n",
    "\n",
    "=== Chance-corrected (binary per category) ===\n",
    "R1 vs R2:\n",
    "{pair12_chance.to_string()}\n",
    "\n",
    "R1 vs R3:\n",
    "{pair13_chance.to_string()}\n",
    "\n",
    "R2 vs R3:\n",
    "{pair23_chance.to_string()}\n",
    "\n",
    "=== Multi-rater AC1 (3 raters) ===\n",
    "macro_gwet_ac1={multi_macro_ac1:.4f}\n",
    "macro_percent_agreement={multi_macro_agree:.4f}\n",
    "\n",
    "=== Adjudication (R3) for R1\u2260R2 ===\n",
    "{adj_summary.to_string()}\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(outdir, \"summary.txt\"), \"w\") as f:\n",
    "    f.write(summary_text)\n",
    "\n",
    "print(summary_text)\n",
    "print(\"\\nOutputs written to:\", os.path.abspath(outdir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5903cee",
   "metadata": {},
   "source": [
    "## Agreement with NLP Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fd18362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined R3 to NLP on (hadm_id, subject_id): 160 matched; 0 adjudicated rows had no NLP match.\n",
      "=== R3 (adjudicator) vs NLP: set-level (order-invariant) ===\n",
      "\n",
      "N_items         160.000000\n",
      "exact_rate        0.543750\n",
      "partial_rate      0.206250\n",
      "none_rate         0.250000\n",
      "mean_jaccard      0.638229\n",
      "mean_overlap      0.713542\n",
      "mean_f1_set       0.670982\n",
      "micro_f1_set      0.700935\n",
      "\n",
      "\n",
      "=== R3 vs NLP: chance-corrected (binary per category) ===\n",
      "\n",
      "macro_cohen_kappa          0.496737\n",
      "macro_gwet_ac1             0.959330\n",
      "macro_percent_agreement    0.966544\n",
      "micro_percent_agreement    0.966544\n",
      "\n",
      "Artifacts written to: /Users/reblocke/Research/Hypercap-CC-NLP/annotation_agreement_outputs_nlp\n"
     ]
    }
   ],
   "source": [
    "# --- R3 (adjudicator) vs NLP comparison (order-invariant, multi-label) ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Set, Dict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "try:\n",
    "    WORK_DIR\n",
    "except NameError:\n",
    "    load_dotenv()\n",
    "    WORK_DIR = Path(os.getenv(\"WORK_DIR\", Path.cwd())).expanduser().resolve()\n",
    "\n",
    "DATA_DIR = WORK_DIR / \"MIMIC tabular data\"\n",
    "\n",
    "# =========================\n",
    "# Config: update this path\n",
    "# =========================\n",
    "NLP_PATH = DATA_DIR / \"2025-10-14 MIMICIV all with CC_with_NLP.xlsx\"\n",
    "NLP_SHEET = 0  # or sheet name if needed\n",
    "NLP_MIN_SIM = None  # Optional: set to a float threshold (e.g., 0.45) to require RFVn_sim >= threshold\n",
    "\n",
    "# If your adjudication workbook isn't in memory as `df`, set it here:\n",
    "EXCEL_PATH = WORK_DIR / \"Annotation/Final 2025-10-14 Annotation Sample.xlsx\"\n",
    "SHEET = \"cohort_cc_sample\"\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers (duplicated for self-containment)\n",
    "# -----------------------------\n",
    "def normalize_label(s: str) -> str:\n",
    "    if pd.isna(s):\n",
    "        return None\n",
    "    s = str(s).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    s = s.replace(\" - \", \" \u2013 \").replace(\"\u2014\", \"\u2013\").replace(\"-\", \"\u2013\")\n",
    "    s = \" \".join(s.split())\n",
    "    return s\n",
    "\n",
    "def load_categories_from_sheet(xlsx_path: str, sheet: str = \"Data\", col: str = \"RVC Categories\") -> List[str]:\n",
    "    try:\n",
    "        cat_df = pd.read_excel(xlsx_path, sheet_name=sheet)\n",
    "        cats = [c for c in cat_df[col].dropna().astype(str).tolist() if c.strip()]\n",
    "        return cats\n",
    "    except Exception:\n",
    "        # Fallback list\n",
    "        return [\n",
    "            'Injuries & adverse effects','Symptom \u2013 Respiratory','Symptom \u2013 Circulatory',\n",
    "            'Symptom \u2013 Nervous','Symptom \u2013 Digestive','Symptom \u2013 Genitourinary',\n",
    "            'Symptom \u2013 Musculoskeletal','Symptom \u2013 Skin/Hair/Nails','Symptom \u2013 Eye/Ear',\n",
    "            'Symptom \u2013 General','Symptom \u2013 Psychological','Diseases (patient-stated diagnosis)',\n",
    "            'Abnormal test result','Diagnostic/Screening/Preventive','Treatment/Medication',\n",
    "            'Administrative','Uncodable/Unknown',\n",
    "        ]\n",
    "\n",
    "try:\n",
    "    CANONICAL_CATS  # use from prior cell if defined\n",
    "except NameError:\n",
    "    CANONICAL_CATS = load_categories_from_sheet(EXCEL_PATH, \"Data\", \"RVC Categories\")\n",
    "\n",
    "def extract_rater_sets_from_df(df: pd.DataFrame, rater_col_prefix: str) -> List[Set[str]]:\n",
    "    \"\"\"\n",
    "    rater_col_prefix = 'annot3' for adjudicator; columns must be like annot3_rvs1_cat .. annot3_rvs5_cat\n",
    "    \"\"\"\n",
    "    cols = [f\"{rater_col_prefix}_rvs{i}_cat\" for i in range(1, 6) if f\"{rater_col_prefix}_rvs{i}_cat\" in df.columns]\n",
    "    out = []\n",
    "    for _, row in df[cols].iterrows():\n",
    "        s = set()\n",
    "        for c in cols:\n",
    "            lab = normalize_label(row[c])\n",
    "            if lab:\n",
    "                s.add(lab)\n",
    "        out.append(s)\n",
    "    return out\n",
    "\n",
    "def extract_nlp_sets_from_df(df_nlp: pd.DataFrame, min_sim=None) -> List[Set[str]]:\n",
    "    \"\"\"\n",
    "    Build set of predicted categories using RFV1_name..RFV5_name (optionally filter by RFVn_sim >= min_sim).\n",
    "    \"\"\"\n",
    "    name_cols = [f\"RFV{i}_name\" for i in range(1, 6) if f\"RFV{i}_name\" in df_nlp.columns]\n",
    "    sim_cols  = [f\"RFV{i}_sim\"  for i in range(1, 6) if f\"RFV{i}_sim\"  in df_nlp.columns]\n",
    "    use_sim = min_sim is not None and len(sim_cols) == len(name_cols)\n",
    "\n",
    "    sets = []\n",
    "    for _, row in df_nlp.iterrows():\n",
    "        s = set()\n",
    "        for i in range(1, 6):\n",
    "            name_col = f\"RFV{i}_name\"\n",
    "            if name_col not in df_nlp.columns: \n",
    "                continue\n",
    "            name_val = normalize_label(row[name_col])\n",
    "            if not name_val:\n",
    "                continue\n",
    "            if use_sim:\n",
    "                sim_col = f\"RFV{i}_sim\"\n",
    "                sim_val = row.get(sim_col, np.nan)\n",
    "                if pd.isna(sim_val) or float(sim_val) < float(min_sim):\n",
    "                    continue\n",
    "            s.add(name_val)\n",
    "        sets.append(s)\n",
    "    return sets\n",
    "\n",
    "# --- set-based similarities ---\n",
    "def jaccard(a: Set[str], b: Set[str]) -> float:\n",
    "    u = len(a | b)\n",
    "    return 1.0 if u == 0 else len(a & b) / u\n",
    "\n",
    "def overlap_coeff(a: Set[str], b: Set[str]) -> float:\n",
    "    m = min(len(a), len(b))\n",
    "    if m == 0:\n",
    "        return 1.0 if len(a) == len(b) == 0 else 0.0\n",
    "    return len(a & b) / m\n",
    "\n",
    "def f1_set(a: Set[str], b: Set[str]) -> float:\n",
    "    denom = len(a) + len(b)\n",
    "    return 1.0 if denom == 0 else 2 * len(a & b) / denom\n",
    "\n",
    "def summarize_set_agreement(A_sets: List[Set[str]], B_sets: List[Set[str]]) -> (pd.DataFrame, pd.Series):\n",
    "    recs = []\n",
    "    for a, b in zip(A_sets, B_sets):\n",
    "        recs.append({\n",
    "            \"exact\": int(a == b),\n",
    "            \"partial\": int((a != b) and (len(a & b) > 0)),\n",
    "            \"none\": int(len(a & b) == 0),\n",
    "            \"jaccard\": jaccard(a, b),\n",
    "            \"overlap\": overlap_coeff(a, b),\n",
    "            \"f1_set\": f1_set(a, b),\n",
    "            \"len_a\": len(a),\n",
    "            \"len_b\": len(b),\n",
    "            \"len_inter\": len(a & b),\n",
    "            \"len_union\": len(a | b),\n",
    "        })\n",
    "    df = pd.DataFrame(recs)\n",
    "    summary = pd.Series({\n",
    "        \"N_items\": len(df),\n",
    "        \"exact_rate\": df[\"exact\"].mean(),\n",
    "        \"partial_rate\": df[\"partial\"].mean(),\n",
    "        \"none_rate\": df[\"none\"].mean(),\n",
    "        \"mean_jaccard\": df[\"jaccard\"].mean(),\n",
    "        \"mean_overlap\": df[\"overlap\"].mean(),\n",
    "        \"mean_f1_set\": df[\"f1_set\"].mean(),\n",
    "        \"micro_f1_set\": (2 * df[\"len_inter\"].sum()) / (df[\"len_a\"].sum() + df[\"len_b\"].sum()) if (df[\"len_a\"].sum()+df[\"len_b\"].sum())>0 else 1.0\n",
    "    })\n",
    "    return df, summary\n",
    "\n",
    "# --- multi-label -> binary matrices ---\n",
    "def flatten_binary_decisions(sets: List[Set[str]], categories: List[str]) -> np.ndarray:\n",
    "    n, k = len(sets), len(categories)\n",
    "    idx = {c: i for i, c in enumerate(categories)}\n",
    "    M = np.zeros((n, k), dtype=int)\n",
    "    for j, s in enumerate(sets):\n",
    "        for lab in s:\n",
    "            lab = normalize_label(lab)\n",
    "            if lab in idx:\n",
    "                M[j, idx[lab]] = 1\n",
    "    return M\n",
    "\n",
    "def pairwise_binary_agreement_stats(M1: np.ndarray, M2: np.ndarray, categories: List[str]) -> pd.DataFrame:\n",
    "    n, k = M1.shape\n",
    "    rows = []\n",
    "    for c in range(k):\n",
    "        y1, y2 = M1[:, c], M2[:, c]\n",
    "        tp = int(((y1 == 1) & (y2 == 1)).sum())\n",
    "        tn = int(((y1 == 0) & (y2 == 0)).sum())\n",
    "        fp = int(((y1 == 0) & (y2 == 1)).sum())\n",
    "        fn = int(((y1 == 1) & (y2 == 0)).sum())\n",
    "        N = tp + tn + fp + fn\n",
    "        if N == 0:\n",
    "            rows.append(dict(category_ix=c, category=categories[c], N=0, percent_agreement=np.nan, cohen_kappa=np.nan, gwet_ac1=np.nan))\n",
    "            continue\n",
    "        Po = (tp + tn) / N\n",
    "        p1, p2 = y1.mean(), y2.mean()\n",
    "        Pe_kappa = p1 * p2 + (1 - p1) * (1 - p2)\n",
    "        kappa = (Po - Pe_kappa) / (1 - Pe_kappa) if (1 - Pe_kappa) != 0 else np.nan\n",
    "        pbar = (p1 + p2) / 2\n",
    "        Pe_ac1 = 2 * pbar * (1 - pbar)\n",
    "        ac1 = (Po - Pe_ac1) / (1 - Pe_ac1) if (1 - Pe_ac1) != 0 else np.nan\n",
    "        rows.append(dict(\n",
    "            category_ix=c, category=categories[c], N=N,\n",
    "            tp=tp, tn=tn, fp=fp, fn=fn,\n",
    "            prevalence_r3=p1, prevalence_nlp=p2,\n",
    "            percent_agreement=Po, cohen_kappa=kappa, gwet_ac1=ac1\n",
    "        ))\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def summarize_kappa_ac1(df_pw: pd.DataFrame) -> pd.Series:\n",
    "    return pd.Series({\n",
    "        \"macro_cohen_kappa\": df_pw[\"cohen_kappa\"].mean(),\n",
    "        \"macro_gwet_ac1\": df_pw[\"gwet_ac1\"].mean(),\n",
    "        \"macro_percent_agreement\": df_pw[\"percent_agreement\"].mean(),\n",
    "        \"micro_percent_agreement\": (df_pw[\"tp\"].sum() + df_pw[\"tn\"].sum()) / df_pw[\"N\"].sum()\n",
    "    })\n",
    "\n",
    "# -----------------------------\n",
    "# Load adjudication and NLP\n",
    "# -----------------------------\n",
    "# Adjudication (R3)\n",
    "try:\n",
    "    df  # from earlier cell\n",
    "except NameError:\n",
    "    df = pd.read_excel(EXCEL_PATH, sheet_name=SHEET)\n",
    "\n",
    "r3_key_cols = [\"hadm_id\", \"subject_id\"]\n",
    "r3_cols = r3_key_cols + [c for c in df.columns if c.startswith(\"annot3_rvs\") and c.endswith(\"_cat\")]\n",
    "df_r3 = df[r3_cols].copy()\n",
    "\n",
    "# NLP predictions\n",
    "df_nlp = pd.read_excel(NLP_PATH, sheet_name=NLP_SHEET)\n",
    "nlp_key_cols = [\"hadm_id\", \"subject_id\"]\n",
    "nlp_name_cols = [f\"RFV{i}_name\" for i in range(1, 6)]\n",
    "present_name_cols = [c for c in nlp_name_cols if c in df_nlp.columns]\n",
    "if not set(nlp_key_cols).issubset(df_nlp.columns) or not present_name_cols:\n",
    "    raise ValueError(\"NLP file must contain hadm_id, subject_id, and RFV*_name columns.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Join + build sets\n",
    "# -----------------------------\n",
    "df_join = pd.merge(df_r3, df_nlp, on=[\"hadm_id\", \"subject_id\"], how=\"inner\")\n",
    "n_before = len(df_r3)\n",
    "n_after = len(df_join)\n",
    "n_dropped = n_before - n_after\n",
    "print(f\"Joined R3 to NLP on (hadm_id, subject_id): {n_after} matched; {n_dropped} adjudicated rows had no NLP match.\")\n",
    "\n",
    "# Build sets\n",
    "r3_sets = extract_rater_sets_from_df(df_join, rater_col_prefix=\"annot3\")\n",
    "nlp_sets = extract_nlp_sets_from_df(df_join, min_sim=NLP_MIN_SIM)\n",
    "\n",
    "# -----------------------------\n",
    "# Set-level agreement (visit-level)\n",
    "# -----------------------------\n",
    "pair_df, pair_summary = summarize_set_agreement(r3_sets, nlp_sets)\n",
    "\n",
    "# Diagnostics by visit (optional): missed vs added\n",
    "def list_diff(a: Set[str], b: Set[str]) -> (List[str], List[str]):\n",
    "    return sorted(a - b), sorted(b - a)\n",
    "\n",
    "missed, added = [], []\n",
    "for a, b in zip(r3_sets, nlp_sets):\n",
    "    m, ad = list_diff(a, b)\n",
    "    missed.append(\"; \".join(m))\n",
    "    added.append(\"; \".join(ad))\n",
    "\n",
    "visit_metrics = df_join[r3_key_cols].copy()\n",
    "visit_metrics[\"exact\"] = pair_df[\"exact\"]\n",
    "visit_metrics[\"partial\"] = pair_df[\"partial\"]\n",
    "visit_metrics[\"none\"] = pair_df[\"none\"]\n",
    "visit_metrics[\"jaccard\"] = pair_df[\"jaccard\"]\n",
    "visit_metrics[\"overlap\"] = pair_df[\"overlap\"]\n",
    "visit_metrics[\"f1_set\"] = pair_df[\"f1_set\"]\n",
    "visit_metrics[\"r3_size\"] = pair_df[\"len_a\"]\n",
    "visit_metrics[\"nlp_size\"] = pair_df[\"len_b\"]\n",
    "visit_metrics[\"missed_by_nlp\"] = missed\n",
    "visit_metrics[\"added_by_nlp\"] = added\n",
    "\n",
    "# -----------------------------\n",
    "# Chance-corrected per-category\n",
    "# -----------------------------\n",
    "M_r3  = flatten_binary_decisions(r3_sets, CANONICAL_CATS)\n",
    "M_nlp = flatten_binary_decisions(nlp_sets, CANONICAL_CATS)\n",
    "\n",
    "pw_stats = pairwise_binary_agreement_stats(M_r3, M_nlp, CANONICAL_CATS)\n",
    "chance_summary = summarize_kappa_ac1(pw_stats)\n",
    "\n",
    "# -----------------------------\n",
    "# Outputs\n",
    "# -----------------------------\n",
    "outdir = WORK_DIR / \"annotation_agreement_outputs_nlp\"\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "visit_metrics.to_csv(os.path.join(outdir, \"R3_vs_NLP_set_metrics_by_visit.csv\"), index=False)\n",
    "pw_stats.to_csv(os.path.join(outdir, \"R3_vs_NLP_binary_stats_by_category.csv\"), index=False)\n",
    "\n",
    "summary_lines = []\n",
    "summary_lines.append(\"=== R3 (adjudicator) vs NLP: set-level (order-invariant) ===\")\n",
    "summary_lines.append(pair_summary.to_string())\n",
    "summary_lines.append(\"\\n=== R3 vs NLP: chance-corrected (binary per category) ===\")\n",
    "summary_lines.append(chance_summary.to_string())\n",
    "summary_text = \"\\n\\n\".join(summary_lines)\n",
    "\n",
    "with open(os.path.join(outdir, \"R3_vs_NLP_summary.txt\"), \"w\") as f:\n",
    "    f.write(summary_text)\n",
    "\n",
    "print(summary_text)\n",
    "print(\"\\nArtifacts written to:\", os.path.abspath(outdir))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mimiciv-tabular)",
   "language": "python",
   "name": "mimiciv-tabular"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}