{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ab24b37",
   "metadata": {},
   "source": [
    "\n",
    "# MIMIC‑IV Hypercapnia Cohort — **ICD ∪ Physiologic Thresholds** (BigQuery)\n",
    "\n",
    "TODO: \n",
    "\n",
    "[ ] want to add ED-rendered diagnoses - a flag to split off whether the hypercapnic respiratory failure ICD was rendered in the ED or during the hospital stay. \n",
    "[ ] are the installation instructions at the beginning of this notebook consistent with the way we intend the notebook to be used? is it also consistent with the README.md instructions?\n",
    "\n",
    "**Goal:** Build an admissions‑level tabular dataset that **enrolls** any hospital admission (`hadm_id`) meeting **_any_** of:\n",
    "\n",
    "1. **ICD** codes for hypercapnic respiratory failure (legacy cohort).\n",
    "2. **Any arterial blood gas** (LAB or POC) with **PaCO₂ ≥ 45.0 mmHg** anywhere during the episode.\n",
    "3. **Any venous blood gas** (LAB or POC) with **PaCO₂ ≥ 50.0 mmHg** anywhere during the episode.\n",
    "\n",
    "Then, keep all downstream columns/logic from the current workflow:\n",
    "- Per‑code ICD indicators and an `any_hypercap_icd` flag.\n",
    "- Robust extraction of **first ABG** and **first VBG** (across LAB + POC) with standardized units (mmHg) and pairing logic.\n",
    "- Demographics/outcomes, NIH/OMB race & ethnicity, ED triage + first ED vitals, ICU meta (first stay + LOS), ventilation flags.\n",
    "- Sanity checks.\n",
    "\n",
    "> **Assumptions**\n",
    "> - You already configured BigQuery auth (`gcloud auth application-default login`) and `.env` variables as in the previous notebook.\n",
    "> - The PhysioNet hosting project is `physionet-data`.\n",
    "> - Datasets exist (e.g., `mimiciv_3_1_hosp`, `mimiciv_3_1_icu`, and an ED dataset such as `mimiciv_ed`). This notebook auto-detects the ED dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621c264b",
   "metadata": {},
   "source": [
    "**Execution timing note:** To capture per-cell runtimes, enable cell execution timing in VS Code (Notebook: Show Cell Execution Time) or enable ExecuteTime in Jupyter. Then re-run and save to allow runtime summaries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd99a86",
   "metadata": {},
   "source": [
    "## Pipeline stages (readable map)\n",
    "1. **Stage A — Config & helpers**: dataset IDs, thresholds, shared helper functions.\n",
    "2. **Stage B — Cohort spine**: ICD + gas thresholds → `hadm_list` / `ed_stay_list`.\n",
    "3. **Stage C — Bulk pulls**: one query per table where possible.\n",
    "4. **Stage D — Transforms**: panels, flags, timing, comorbidities, vitals.\n",
    "5. **Stage E — QA checks**: deterministic assertions and range checks.\n",
    "6. **Stage F — Outputs**: parquet + Excel exports.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceac90f",
   "metadata": {},
   "source": [
    "## SQL registry (centralized query templates)\n",
    "All BigQuery SQL is defined in one place below, then referenced by name in subsequent cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854e25ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:27:33.853028Z",
     "iopub.status.busy": "2026-02-05T23:27:33.852955Z",
     "iopub.status.idle": "2026-02-05T23:27:33.857695Z",
     "shell.execute_reply": "2026-02-05T23:27:33.857437Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Build ABG/VBG hypercapnia threshold flags from lab and ICU POC pCO2 measurements.\n",
    "\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "# Central SQL registry (define all query templates here)\n",
    "SQL = {}\n",
    "\n",
    "def sql(name: str) -> str:\n",
    "    if name not in SQL:\n",
    "        raise KeyError(f\"SQL template not found: {name}\")\n",
    "    return SQL[name]\n",
    "\n",
    "# SQL templates (populated below in-place to keep notebook linear)\n",
    "# Names: admit_sql, co2_thresholds_sql, cohort_icd_sql, counts_sql, demo_sql, ditems_sql, ed_counts_sql, ed_first_vitals_sql, ed_spine_sql, ed_to_icu_sql, ed_triage_sql, ed_vitals_sql, icd_sql, icu_meta_sql, icu_sql, labitems_sql, labs_sql, vent_chart_sql, vent_sql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3127a0f",
   "metadata": {},
   "source": [
    "**Rationale:** Define a reproducible, admission-level cohort that captures hypercapnia using complementary diagnostic (ICD) and physiologic (blood gas) criteria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1731fe",
   "metadata": {},
   "source": [
    "# MIMIC‑IV on BigQuery\n",
    "\n",
    "## Environment Bootstrap & Smoke Test\n",
    "\n",
    "Purpose: make a clean, reproducible start on a new machine.\n",
    "\n",
    "Outcome: verify auth, project config, and dataset access; provide a reusable BigQuery runner for the build notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a73e95",
   "metadata": {},
   "source": [
    "**Rationale:** Establish the BigQuery environment and dataset configuration so queries are consistent and reproducible across runs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8408759",
   "metadata": {},
   "source": [
    "## 0. Prerequisites (one-time)\n",
    "\n",
    "**Accounts & access**\n",
    "- PhysioNet access to MIMIC-IV on BigQuery; in BigQuery Console star project `physionet-data`.\n",
    "- A Google Cloud **Project ID** with BigQuery API enabled (this is your **billing** project).\n",
    "\n",
    "**CLI & environment**\n",
    "- Google Cloud SDK (gcloud) installed and on PATH.\n",
    "- Python environment created with `uv` (see README) and Jupyter kernel selected.\n",
    "- A project-local **`.env`** with the variables below.\n",
    "\n",
    "**.env variables**\n",
    "```ini\n",
    "MIMIC_BACKEND=bigquery\n",
    "WORK_PROJECT=<your-billing-project-id>\n",
    "BQ_PHYSIONET_PROJECT=physionet-data\n",
    "BQ_DATASET_HOSP=mimiciv_3_1_hosp\n",
    "BQ_DATASET_ICU=mimiciv_3_1_icu\n",
    "BQ_DATASET_ED=mimiciv_ed\n",
    "WORK_DIR=/path/to/Hypercap-CC-NLP\n",
    "# GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json\n",
    "```\n",
    "\n",
    "**Command line quickstart**\n",
    "```bash\n",
    "brew install --cask google-cloud-sdk\n",
    "gcloud init\n",
    "gcloud auth application-default login\n",
    "gcloud services enable bigquery.googleapis.com --project <your-billing-project-id>\n",
    "ls -l ~/.config/gcloud/application_default_credentials.json\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2adf93",
   "metadata": {},
   "source": [
    "**Rationale:** Verify access and credentials up front to prevent silent failures later in the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf7fc64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:27:33.859184Z",
     "iopub.status.busy": "2026-02-05T23:27:33.859085Z",
     "iopub.status.idle": "2026-02-05T23:27:35.484330Z",
     "shell.execute_reply": "2026-02-05T23:27:35.483803Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Set up project paths, environment variables, and BigQuery client connections for reproducible execution.\n",
    "\n",
    "# --- Imports & environment\n",
    "import os, re, json, math, textwrap\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "WORK_DIR = Path(os.getenv(\"WORK_DIR\", Path.cwd())).expanduser().resolve()\n",
    "DATA_DIR = WORK_DIR / \"MIMIC tabular data\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Backend selection (we use BigQuery)\n",
    "BACKEND = os.getenv(\"MIMIC_BACKEND\", \"bigquery\").strip().lower()\n",
    "assert BACKEND == \"bigquery\", \"This notebook is BigQuery-specific.\"\n",
    "\n",
    "WORK_PROJECT = os.getenv(\"WORK_PROJECT\", \"\").strip()  # your billing project\n",
    "PHYS = os.getenv(\"BQ_PHYSIONET_PROJECT\", \"physionet-data\").strip()  # hosting project (read-only)\n",
    "\n",
    "# Dataset preferences: resolved to accessible datasets in the next setup cell.\n",
    "HOSP = os.getenv(\"BQ_DATASET_HOSP\", \"mimiciv_3_1_hosp\").strip()\n",
    "ICU  = os.getenv(\"BQ_DATASET_ICU\",  \"mimiciv_3_1_icu\").strip()\n",
    "ED   = os.getenv(\"BQ_DATASET_ED\",   \"\").strip()\n",
    "\n",
    "# BigQuery client\n",
    "client = bigquery.Client(project=WORK_PROJECT)\n",
    "\n",
    "print(\"Project:\", WORK_PROJECT)\n",
    "print(\"PhysioNet host:\", PHYS)\n",
    "print(\"HOSP (pref):\", HOSP, \"ICU (pref):\", ICU, \"ED (pref):\", ED)\n",
    "print(\"WORK_DIR:\", WORK_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a0b813",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:27:35.486282Z",
     "iopub.status.busy": "2026-02-05T23:27:35.485993Z",
     "iopub.status.idle": "2026-02-05T23:27:35.491391Z",
     "shell.execute_reply": "2026-02-05T23:27:35.491169Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Define reusable BigQuery helpers and resolve dataset names across known naming variants.\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Fail fast on long-running queries instead of hanging indefinitely in nbconvert.\n",
    "BQ_QUERY_TIMEOUT_SECS = int(os.getenv(\"BQ_QUERY_TIMEOUT_SECS\", \"1800\"))\n",
    "\n",
    "# --- Helper: run SQL with optional named parameters\n",
    "def run_sql_bq(sql: str, params: dict | None = None) -> pd.DataFrame:\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    job_config.labels = {\"pipeline\": \"hypercapcc\", \"notebook\": \"cohort\"}\n",
    "    if params:\n",
    "        bq_params = []\n",
    "        for k, v in params.items():\n",
    "            if isinstance(v, (list, tuple, np.ndarray, pd.Series)):\n",
    "                # BigQuery ARRAY<INT64> if all ints; else ARRAY<STRING>\n",
    "                v_list = list(v)\n",
    "                if all(isinstance(x, (int, np.integer)) for x in v_list):\n",
    "                    bq_params.append(bigquery.ArrayQueryParameter(k, \"INT64\", list(map(int, v_list))))\n",
    "                else:\n",
    "                    bq_params.append(bigquery.ArrayQueryParameter(k, \"STRING\", list(map(str, v_list))))\n",
    "            else:\n",
    "                # scalar\n",
    "                if isinstance(v, (int, np.integer)):\n",
    "                    bq_params.append(bigquery.ScalarQueryParameter(k, \"INT64\", int(v)))\n",
    "                elif isinstance(v, float):\n",
    "                    bq_params.append(bigquery.ScalarQueryParameter(k, \"FLOAT64\", float(v)))\n",
    "                else:\n",
    "                    bq_params.append(bigquery.ScalarQueryParameter(k, \"STRING\", str(v)))\n",
    "        job_config.query_parameters = bq_params\n",
    "\n",
    "    job = client.query(sql, job_config=job_config)\n",
    "    try:\n",
    "        result = job.result(timeout=BQ_QUERY_TIMEOUT_SECS)\n",
    "    except Exception as exc:\n",
    "        try:\n",
    "            job.cancel()\n",
    "        except Exception:\n",
    "            pass\n",
    "        raise RuntimeError(\n",
    "            f\"BigQuery query failed or timed out after {BQ_QUERY_TIMEOUT_SECS}s (job_id={job.job_id}).\"\n",
    "        ) from exc\n",
    "\n",
    "    try:\n",
    "        return result.to_dataframe(create_bqstorage_client=True)\n",
    "    except TypeError:\n",
    "        return result.to_dataframe()\n",
    "\n",
    "# --- Helper: test if a fully-qualified table exists and is accessible\n",
    "def table_exists(fqtn: str) -> bool:\n",
    "    try:\n",
    "        _ = run_sql_bq(f\"SELECT 1 FROM `{fqtn}` LIMIT 1\")\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def resolve_dataset(preferred: str, candidates: list[str], probe_table: str, label: str) -> tuple[str, str]:\n",
    "    # Keep preferred first, then try known aliases.\n",
    "    ordered = []\n",
    "    if preferred:\n",
    "        ordered.append(preferred)\n",
    "    for cand in candidates:\n",
    "        if cand not in ordered:\n",
    "            ordered.append(cand)\n",
    "\n",
    "    for dataset in ordered:\n",
    "        fqtn = f\"{PHYS}.{dataset}.{probe_table}\"\n",
    "        if table_exists(fqtn):\n",
    "            return dataset, fqtn\n",
    "\n",
    "    raise RuntimeError(\n",
    "        f\"No accessible {label} dataset found for probe table '{probe_table}'. Tried: {ordered}\"\n",
    "    )\n",
    "\n",
    "# Resolve HOSP/ICU/ED names so notebook runs across v3.1 naming variants.\n",
    "HOSP, HOSP_PROBE = resolve_dataset(\n",
    "    preferred=HOSP,\n",
    "    candidates=[\"mimiciv_3_1_hosp\", \"mimiciv_v3_1_hosp\", \"mimiciv_hosp\"],\n",
    "    probe_table=\"admissions\",\n",
    "    label=\"HOSP\",\n",
    ")\n",
    "ICU, ICU_PROBE = resolve_dataset(\n",
    "    preferred=ICU,\n",
    "    candidates=[\"mimiciv_3_1_icu\", \"mimiciv_v3_1_icu\", \"mimiciv_icu\"],\n",
    "    probe_table=\"icustays\",\n",
    "    label=\"ICU\",\n",
    ")\n",
    "if not ED:\n",
    "    ED = \"mimiciv_ed\"\n",
    "ED, ED_PROBE = resolve_dataset(\n",
    "    preferred=ED,\n",
    "    candidates=[\"mimiciv_ed\", \"mimiciv_3_1_ed\", \"mimiciv_v3_1_ed\"],\n",
    "    probe_table=\"edstays\",\n",
    "    label=\"ED\",\n",
    ")\n",
    "\n",
    "RUN_METADATA = {\n",
    "    \"run_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"work_project\": WORK_PROJECT,\n",
    "    \"physionet_project\": PHYS,\n",
    "    \"datasets\": {\"hosp\": HOSP, \"icu\": ICU, \"ed\": ED},\n",
    "    \"dataset_probes\": {\"hosp\": HOSP_PROBE, \"icu\": ICU_PROBE, \"ed\": ED_PROBE},\n",
    "    \"notebook\": \"MIMICIV_hypercap_EXT_cohort.ipynb\",\n",
    "}\n",
    "\n",
    "print(\"Resolved datasets:\", RUN_METADATA[\"datasets\"])\n",
    "print(\"Dataset probes:\", RUN_METADATA[\"dataset_probes\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d112076",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:27:35.492477Z",
     "iopub.status.busy": "2026-02-05T23:27:35.492396Z",
     "iopub.status.idle": "2026-02-05T23:27:35.495801Z",
     "shell.execute_reply": "2026-02-05T23:27:35.495603Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Create reusable data-quality and merge guardrail helpers to prevent silent join errors.\n",
    "\n",
    "# --- Helper utilities for reproducibility and safe joins\n",
    "def require_cols(df: pd.DataFrame, cols: list[str], name: str) -> None:\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"{name} missing columns: {missing}\")\n",
    "\n",
    "def assert_unique(df: pd.DataFrame, key: str, name: str) -> None:\n",
    "    if df[key].duplicated().any():\n",
    "        n = int(df[key].duplicated().sum())\n",
    "        raise ValueError(f\"{name} has {n} duplicate {key} values\")\n",
    "\n",
    "def safe_merge(left: pd.DataFrame, right: pd.DataFrame, on: list[str] | str, how: str, name: str) -> pd.DataFrame:\n",
    "    # guard against accidental duplicate columns\n",
    "    overlap = set(left.columns) & set(right.columns)\n",
    "    if isinstance(on, str):\n",
    "        on_cols = {on}\n",
    "    else:\n",
    "        on_cols = set(on)\n",
    "    overlap = overlap - on_cols\n",
    "    if overlap:\n",
    "        raise ValueError(f\"{name} merge would duplicate columns: {sorted(overlap)}\")\n",
    "    return left.merge(right, on=on, how=how)\n",
    "\n",
    "def check_ranges(df: pd.DataFrame, ranges: dict[str, tuple[float, float]]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for col, (lo, hi) in ranges.items():\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        bad = df[col].notna() & ((df[col] < lo) | (df[col] > hi))\n",
    "        rows.append({\"col\": col, \"n_bad\": int(bad.sum())})\n",
    "    return pd.DataFrame(rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8a51c8",
   "metadata": {},
   "source": [
    "## 1) ICD cohort flags (hypercapnic respiratory failure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d8291e",
   "metadata": {},
   "source": [
    "**Rationale:** Capture diagnosis-based hypercapnia from ED and hospital discharge codes to define a broad, clinically recognized cohort.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3507bb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:27:35.496954Z",
     "iopub.status.busy": "2026-02-05T23:27:35.496854Z",
     "iopub.status.idle": "2026-02-05T23:27:37.842365Z",
     "shell.execute_reply": "2026-02-05T23:27:37.842047Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.\n",
    "\n",
    "# Target ICD codes (dotless, uppercase)\n",
    "ICD10_CODES = ['J9602','J9612','J9622','J9692','E662']\n",
    "ICD9_CODES  = ['27803']\n",
    "\n",
    "SQL[\"cohort_icd_sql\"] = f\"\"\"\n",
    "-- ICD-based cohort flags per admission\n",
    "WITH target_codes AS (\n",
    "  SELECT 'J9602' AS code, 10 AS ver UNION ALL\n",
    "  SELECT 'J9612', 10 UNION ALL\n",
    "  SELECT 'J9622', 10 UNION ALL\n",
    "  SELECT 'J9692', 10 UNION ALL\n",
    "  SELECT 'E662',  10 UNION ALL\n",
    "  SELECT '27803', 9\n",
    "),\n",
    "\n",
    "-- Hospital ICDs restricted to target codes\n",
    "hosp_dx AS (\n",
    "  SELECT\n",
    "    d.subject_id,\n",
    "    d.hadm_id,\n",
    "    UPPER(REPLACE(d.icd_code, '.', '')) AS code_norm,\n",
    "    d.icd_version\n",
    "  FROM `{PHYS}.{HOSP}.diagnoses_icd` d\n",
    "  JOIN target_codes t\n",
    "    ON t.ver = d.icd_version AND t.code = UPPER(REPLACE(d.icd_code, '.', ''))\n",
    "  WHERE d.hadm_id IS NOT NULL\n",
    "),\n",
    "\n",
    "-- Hospital flags per admission\n",
    "hosp_flags AS (\n",
    "  SELECT\n",
    "    subject_id, hadm_id,\n",
    "    MAX(IF(icd_version=10 AND code_norm='J9602',1,0)) AS ICD10_J9602,\n",
    "    MAX(IF(icd_version=10 AND code_norm='J9612',1,0)) AS ICD10_J9612,\n",
    "    MAX(IF(icd_version=10 AND code_norm='J9622',1,0)) AS ICD10_J9622,\n",
    "    MAX(IF(icd_version=10 AND code_norm='J9692',1,0)) AS ICD10_J9692,\n",
    "    MAX(IF(icd_version=10 AND code_norm='E662', 1,0)) AS ICD10_E662,\n",
    "    MAX(IF(icd_version=9  AND code_norm='27803',1,0)) AS ICD9_27803\n",
    "  FROM hosp_dx\n",
    "  GROUP BY subject_id, hadm_id\n",
    "),\n",
    "\n",
    "-- ED ICDs restricted to target codes (map to hadm via edstays)\n",
    "ed_dx AS (\n",
    "  SELECT\n",
    "    s.subject_id,\n",
    "    s.hadm_id,\n",
    "    s.stay_id,\n",
    "    s.intime AS ed_intime,\n",
    "    UPPER(REPLACE(d.icd_code, '.', '')) AS code_norm,\n",
    "    d.icd_version\n",
    "  FROM `{PHYS}.{ED}.diagnosis` d\n",
    "  JOIN `{PHYS}.{ED}.edstays` s\n",
    "    ON s.subject_id = d.subject_id AND s.stay_id = d.stay_id\n",
    "  JOIN target_codes t\n",
    "    ON t.ver = d.icd_version AND t.code = UPPER(REPLACE(d.icd_code, '.', ''))\n",
    "  WHERE s.hadm_id IS NOT NULL\n",
    "),\n",
    "\n",
    "-- ED flags per ED stay (so we can both: OR flags across stays and also pick earliest stay_id)\n",
    "ed_flags_by_stay AS (\n",
    "  SELECT\n",
    "    subject_id, hadm_id, stay_id, MIN(ed_intime) AS ed_intime,\n",
    "    MAX(IF(icd_version=10 AND code_norm='J9602',1,0)) AS ICD10_J9602,\n",
    "    MAX(IF(icd_version=10 AND code_norm='J9612',1,0)) AS ICD10_J9612,\n",
    "    MAX(IF(icd_version=10 AND code_norm='J9622',1,0)) AS ICD10_J9622,\n",
    "    MAX(IF(icd_version=10 AND code_norm='J9692',1,0)) AS ICD10_J9692,\n",
    "    MAX(IF(icd_version=10 AND code_norm='E662', 1,0)) AS ICD10_E662,\n",
    "    MAX(IF(icd_version=9  AND code_norm='27803',1,0)) AS ICD9_27803\n",
    "  FROM ed_dx\n",
    "  GROUP BY subject_id, hadm_id, stay_id\n",
    "),\n",
    "\n",
    "-- OR the ED flags across all ED stays mapped to the same hadm\n",
    "ed_flags_or AS (\n",
    "  SELECT\n",
    "    subject_id, hadm_id,\n",
    "    MAX(ICD10_J9602) AS ICD10_J9602,\n",
    "    MAX(ICD10_J9612) AS ICD10_J9612,\n",
    "    MAX(ICD10_J9622) AS ICD10_J9622,\n",
    "    MAX(ICD10_J9692) AS ICD10_J9692,\n",
    "    MAX(ICD10_E662 ) AS ICD10_E662,\n",
    "    MAX(ICD9_27803) AS ICD9_27803\n",
    "  FROM ed_flags_by_stay\n",
    "  GROUP BY subject_id, hadm_id\n",
    "),\n",
    "\n",
    "-- Earliest ED stay_id per hadm (NO UNNEST of aggregates; use [OFFSET(0)])\n",
    "ed_earliest AS (\n",
    "  SELECT\n",
    "    subject_id,\n",
    "    hadm_id,\n",
    "    (ARRAY_AGG(STRUCT(stay_id, ed_intime) ORDER BY ed_intime LIMIT 1))[OFFSET(0)].stay_id AS stay_id\n",
    "  FROM ed_flags_by_stay\n",
    "  GROUP BY subject_id, hadm_id\n",
    "),\n",
    "\n",
    "-- Bring flags and earliest stay_id together\n",
    "ed_by_hadm AS (\n",
    "  SELECT\n",
    "    f.subject_id,\n",
    "    f.hadm_id,\n",
    "    e.stay_id,\n",
    "    f.ICD10_J9602,\n",
    "    f.ICD10_J9612,\n",
    "    f.ICD10_J9622,\n",
    "    f.ICD10_J9692,\n",
    "    f.ICD10_E662,\n",
    "    f.ICD9_27803\n",
    "  FROM ed_flags_or f\n",
    "  LEFT JOIN ed_earliest e\n",
    "    USING (subject_id, hadm_id)\n",
    "),\n",
    "\n",
    "-- Combine ED and hospital flags at the admission level\n",
    "combined AS (\n",
    "  SELECT\n",
    "    COALESCE(h.subject_id, e.subject_id) AS subject_id,\n",
    "    COALESCE(h.hadm_id,     e.hadm_id)   AS hadm_id,\n",
    "    GREATEST(IFNULL(h.ICD10_J9602,0), IFNULL(e.ICD10_J9602,0)) AS ICD10_J9602,\n",
    "    GREATEST(IFNULL(h.ICD10_J9612,0), IFNULL(e.ICD10_J9612,0)) AS ICD10_J9612,\n",
    "    GREATEST(IFNULL(h.ICD10_J9622,0), IFNULL(e.ICD10_J9622,0)) AS ICD10_J9622,\n",
    "    GREATEST(IFNULL(h.ICD10_J9692,0), IFNULL(e.ICD10_J9692,0)) AS ICD10_J9692,\n",
    "    GREATEST(IFNULL(h.ICD10_E662 ,0), IFNULL(e.ICD10_E662 ,0)) AS ICD10_E662,\n",
    "    GREATEST(IFNULL(h.ICD9_27803,0), IFNULL(e.ICD9_27803,0)) AS ICD9_27803,\n",
    "    IF((IFNULL(h.ICD10_J9602,0)+IFNULL(h.ICD10_J9612,0)+IFNULL(h.ICD10_J9622,0)+IFNULL(h.ICD10_J9692,0)+IFNULL(h.ICD10_E662,0)+IFNULL(h.ICD9_27803,0)) > 0, 1, 0) AS any_hypercap_icd_hosp,\n",
    "    IF((IFNULL(e.ICD10_J9602,0)+IFNULL(e.ICD10_J9612,0)+IFNULL(e.ICD10_J9622,0)+IFNULL(e.ICD10_J9692,0)+IFNULL(e.ICD10_E662,0)+IFNULL(e.ICD9_27803,0)) > 0, 1, 0) AS any_hypercap_icd_ed\n",
    "  FROM hosp_flags h\n",
    "  FULL OUTER JOIN ed_by_hadm e\n",
    "    ON h.hadm_id = e.hadm_id\n",
    ")\n",
    "\n",
    "SELECT\n",
    "  subject_id, hadm_id,\n",
    "  ICD10_J9602, ICD10_J9612, ICD10_J9622, ICD10_J9692, ICD10_E662, ICD9_27803,\n",
    "  IF((ICD10_J9602+ICD10_J9612+ICD10_J9622+ICD10_J9692+ICD10_E662+ICD9_27803) > 0, 1, 0) AS any_hypercap_icd,\n",
    "  any_hypercap_icd_hosp,\n",
    "  any_hypercap_icd_ed,\n",
    "  CASE\n",
    "    WHEN any_hypercap_icd_hosp=1 AND any_hypercap_icd_ed=1 THEN 'ED+HOSP'\n",
    "    WHEN any_hypercap_icd_ed=1 THEN 'ED'\n",
    "    WHEN any_hypercap_icd_hosp=1 THEN 'HOSP'\n",
    "    ELSE 'NONE'\n",
    "  END AS icd_source\n",
    "FROM combined\n",
    "\"\"\"\n",
    "\n",
    "cohort_icd = run_sql_bq(sql(\"cohort_icd_sql\"))\n",
    "print(\"ICD cohort admissions:\", len(cohort_icd))\n",
    "cohort_icd.head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c29e962",
   "metadata": {},
   "source": [
    "## 2) Blood gas inclusion thresholds — ANY PaCO₂ meeting cutoffs (LAB + POC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409b779a",
   "metadata": {},
   "source": [
    "**Rationale:** Identify physiologic hypercapnia using ABG/VBG thresholds, independent of diagnostic coding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb134f8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:27:37.848986Z",
     "iopub.status.busy": "2026-02-05T23:27:37.848514Z",
     "iopub.status.idle": "2026-02-05T23:27:42.162983Z",
     "shell.execute_reply": "2026-02-05T23:27:42.162380Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.\n",
    "\n",
    "SQL[\"co2_thresholds_sql\"] = f\"\"\"\n",
    "/* ---- LAB (HOSP) pCO2 across entire dataset ---- */\n",
    "WITH hosp_cand AS (\n",
    "  SELECT\n",
    "    le.hadm_id, le.charttime, le.specimen_id,\n",
    "    COALESCE(\n",
    "      CAST(le.valuenum AS FLOAT64),\n",
    "      SAFE_CAST(REGEXP_EXTRACT(LOWER(le.value), r'(-?\\d+(?:\\.\\d+)?)') AS FLOAT64)\n",
    "    ) AS val,\n",
    "    LOWER(REPLACE(COALESCE(le.valueuom,''),' ','')) AS uom_nospace,\n",
    "    LOWER(di.label) AS lbl,\n",
    "    LOWER(COALESCE(di.fluid,'')) AS fl\n",
    "  FROM `{PHYS}.{HOSP}.labevents` le\n",
    "  JOIN `{PHYS}.{HOSP}.d_labitems` di ON di.itemid = le.itemid\n",
    "  WHERE (le.valuenum IS NOT NULL OR le.value IS NOT NULL)\n",
    "    AND (\n",
    "      LOWER(COALESCE(di.category,'')) LIKE '%blood gas%' OR\n",
    "      LOWER(di.label) LIKE '%pco2%' OR\n",
    "      REGEXP_CONTAINS(LOWER(di.label), r'\\bpa?\\s*co(?:2|₂)\\b')\n",
    "    )\n",
    "    AND NOT REGEXP_CONTAINS(LOWER(di.label),\n",
    "        r'(et\\s*co2|end[- ]?tidal|t\\s*co2|tco2|total\\s*co2|hco3|bicar)')\n",
    "),\n",
    "hosp_spec AS (\n",
    "  SELECT le.specimen_id, LOWER(COALESCE(le.value,'')) AS spec_val\n",
    "  FROM `{PHYS}.{HOSP}.labevents` le\n",
    "  JOIN `{PHYS}.{HOSP}.d_labitems` di ON di.itemid = le.itemid\n",
    "  WHERE le.specimen_id IS NOT NULL\n",
    "    AND REGEXP_CONTAINS(LOWER(di.label), r'(specimen|sample)')\n",
    "),\n",
    "hosp_pco2 AS (\n",
    "  SELECT\n",
    "    c.hadm_id, c.charttime,\n",
    "    CASE\n",
    "      WHEN REGEXP_CONTAINS(s.spec_val, r'arter') OR REGEXP_CONTAINS(s.spec_val, r'\\bart\\b') THEN 'arterial'\n",
    "      WHEN REGEXP_CONTAINS(s.spec_val, r'ven|mixed|central') THEN 'venous'\n",
    "      WHEN REGEXP_CONTAINS(c.fl, r'arter') THEN 'arterial'\n",
    "      WHEN REGEXP_CONTAINS(c.fl, r'ven') THEN 'venous'\n",
    "      WHEN c.fl LIKE '%arterial%' OR REGEXP_CONTAINS(c.lbl, r'\\b(abg|art|arterial|a[- ]?line)\\b') THEN 'arterial'\n",
    "      WHEN c.fl LIKE '%ven%'      OR REGEXP_CONTAINS(c.lbl, r'\\b(vbg|ven|venous|mixed|central)\\b') THEN 'venous'\n",
    "      ELSE 'other'\n",
    "    END AS site,\n",
    "    CASE WHEN c.uom_nospace='kpa' THEN c.val*7.50062 ELSE c.val END AS pco2_mmHg\n",
    "  FROM hosp_cand c\n",
    "  LEFT JOIN hosp_spec s USING (specimen_id)\n",
    "  WHERE c.val IS NOT NULL\n",
    "),\n",
    "hosp_pco2_std AS (\n",
    "  SELECT hadm_id, site, charttime, pco2_mmHg\n",
    "  FROM hosp_pco2\n",
    "  WHERE site IN ('arterial','venous','other') AND pco2_mmHg BETWEEN 5 AND 200\n",
    "),\n",
    "\n",
    "/* ---- ICU (POC) pCO2 across entire dataset ---- */\n",
    "icu_raw AS (\n",
    "  SELECT\n",
    "    ie.hadm_id,\n",
    "    ce.stay_id,\n",
    "    ce.charttime,\n",
    "    LOWER(di.label) AS lbl,\n",
    "    LOWER(REPLACE(COALESCE(ce.valueuom,''),' ','')) AS uom_nospace,\n",
    "    LOWER(COALESCE(ce.value,'')) AS valstr,\n",
    "    COALESCE(\n",
    "      CAST(ce.valuenum AS FLOAT64),\n",
    "      SAFE_CAST(REGEXP_EXTRACT(LOWER(ce.value), r'(-?\\d+(?:\\.\\d+)?)') AS FLOAT64)\n",
    "    ) AS val\n",
    "  FROM `{PHYS}.{ICU}.chartevents` ce\n",
    "  JOIN `{PHYS}.{ICU}.d_items`  di ON di.itemid = ce.itemid\n",
    "  JOIN `{PHYS}.{ICU}.icustays` ie ON ie.stay_id = ce.stay_id\n",
    "),\n",
    "icu_cand AS (\n",
    "  SELECT\n",
    "    hadm_id, stay_id, charttime, lbl, uom_nospace, valstr, val,\n",
    "    CASE\n",
    "      WHEN (\n",
    "            REGEXP_CONTAINS(lbl, r'(^|[^a-z])p(?:a)?\\s*co\\s*(?:2|₂)([^a-z]|$)')\n",
    "            OR uom_nospace IN ('mmhg','kpa')\n",
    "            OR REGEXP_CONTAINS(valstr, r'\\b(mm\\s*hg|kpa)\\b')\n",
    "           )\n",
    "           AND NOT REGEXP_CONTAINS(lbl,\n",
    "               r'(et\\s*co2|end[- ]?tidal|t\\s*co2|tco2|total\\s*co2|hco3|bicar|v\\s*co2|vco2|co2\\s*(prod|elimin|production|elimination))')\n",
    "      THEN 'pco2'\n",
    "      ELSE NULL\n",
    "    END AS analyte,\n",
    "    CASE\n",
    "      WHEN REGEXP_CONTAINS(lbl, r'\\b(abg|art|arterial|a[- ]?line)\\b') THEN 'arterial'\n",
    "      WHEN REGEXP_CONTAINS(lbl, r'\\b(vbg|ven|venous|mixed|central)\\b') THEN 'venous'\n",
    "      ELSE 'other'\n",
    "    END AS site\n",
    "  FROM icu_raw\n",
    "  WHERE val IS NOT NULL\n",
    "    AND (\n",
    "      REGEXP_CONTAINS(lbl, r'(^|[^a-z])p(?:a)?\\s*co\\s*(?:2|₂)([^a-z]|$)')\n",
    "      OR uom_nospace IN ('mmhg','kpa')\n",
    "      OR REGEXP_CONTAINS(valstr, r'\\b(mm\\s*hg|kpa)\\b')\n",
    "    )\n",
    "    AND NOT REGEXP_CONTAINS(lbl,\n",
    "        r'(et\\s*co2|end[- ]?tidal|t\\s*co2|tco2|total\\s*co2|hco3|bicar|v\\s*co2|vco2|co2\\s*(prod|elimin|production|elimination))')\n",
    "),\n",
    "icu_co2_std AS (\n",
    "  SELECT\n",
    "    hadm_id,\n",
    "    site,\n",
    "    charttime,\n",
    "    CASE WHEN uom_nospace='kpa' OR REGEXP_CONTAINS(valstr, r'\\bkpa\\b') THEN val*7.50062 ELSE val END AS pco2_mmHg\n",
    "  FROM icu_cand\n",
    "  WHERE analyte='pco2'\n",
    "    AND site IN ('arterial','venous','other')\n",
    "    AND (CASE WHEN uom_nospace='kpa' OR REGEXP_CONTAINS(valstr, r'\\bkpa\\b') THEN val*7.50062 ELSE val END) BETWEEN 5 AND 200\n",
    "),\n",
    "\n",
    "/* ---- Combine and threshold per admission ---- */\n",
    "all_pco2 AS (\n",
    "  SELECT * FROM hosp_pco2_std\n",
    "  UNION ALL\n",
    "  SELECT * FROM icu_co2_std\n",
    "),\n",
    "thresh AS (\n",
    "  SELECT\n",
    "    hadm_id,\n",
    "    MAX(IF(site='arterial' AND pco2_mmHg >= 45.0, 1, 0)) AS abg_hypercap_threshold,\n",
    "    MAX(IF(site='venous'   AND pco2_mmHg >= 50.0, 1, 0)) AS vbg_hypercap_threshold,\n",
    "    MAX(IF(site='other'    AND pco2_mmHg >= 50.0, 1, 0)) AS other_hypercap_threshold\n",
    "  FROM all_pco2\n",
    "  GROUP BY hadm_id\n",
    ")\n",
    "SELECT * FROM thresh\n",
    "\"\"\"\n",
    "\n",
    "co2_thresh = run_sql_bq(sql(\"co2_thresholds_sql\"))\n",
    "print(\"Admissions meeting thresholds:\", len(co2_thresh))\n",
    "co2_thresh.head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061d0066",
   "metadata": {},
   "source": [
    "## 3) Cohort union (ICD ∪ thresholds) and `hadm_list` for downstream queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c73cdc3",
   "metadata": {},
   "source": [
    "**Rationale:** Combine ICD and physiologic routes to maximize sensitivity, then define a stable hadm_id list for downstream joins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db1a7d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:27:42.170236Z",
     "iopub.status.busy": "2026-02-05T23:27:42.170070Z",
     "iopub.status.idle": "2026-02-05T23:27:42.222176Z",
     "shell.execute_reply": "2026-02-05T23:27:42.221935Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Combine ICD and gas-threshold ascertainment routes to produce the final hadm inclusion list.\n",
    "\n",
    "# Outer-join because thresholds can identify hadm_id with no ICD codes and vice versa\n",
    "cohort_any = cohort_icd.merge(co2_thresh, how=\"outer\", on=\"hadm_id\")\n",
    "\n",
    "# Fill missing flags with 0 where appropriate\n",
    "icd_cols = [\"ICD10_J9602\",\"ICD10_J9612\",\"ICD10_J9622\",\"ICD10_J9692\",\"ICD10_E662\",\"ICD9_27803\",\"any_hypercap_icd\",\"any_hypercap_icd_hosp\",\"any_hypercap_icd_ed\"]\n",
    "for c in icd_cols:\n",
    "    if c in cohort_any.columns:\n",
    "        cohort_any[c] = cohort_any[c].fillna(0).astype(int)\n",
    "\n",
    "for c in [\"abg_hypercap_threshold\",\"vbg_hypercap_threshold\",\"other_hypercap_threshold\"]:\n",
    "    if c in cohort_any.columns:\n",
    "        cohort_any[c] = cohort_any[c].fillna(0).astype(int)\n",
    "\n",
    "# Final enrollment flag\n",
    "cohort_any[\"pco2_threshold_any\"] = ((cohort_any[\"abg_hypercap_threshold\"]==1) | (cohort_any[\"vbg_hypercap_threshold\"]==1) | (cohort_any[\"other_hypercap_threshold\"]==1)).astype(int)\n",
    "cohort_any[\"enrolled_any\"] = ((cohort_any[\"any_hypercap_icd\"]==1) | (cohort_any[\"pco2_threshold_any\"]==1)).astype(int)\n",
    "\n",
    "print(\"ICD-only admissions        :\", int((cohort_any[\"any_hypercap_icd\"]==1).sum()))\n",
    "print(\"Threshold-only admissions  :\", int(((cohort_any[\"pco2_threshold_any\"]==1) & (cohort_any[\"any_hypercap_icd\"]==0)).sum()))\n",
    "print(\"Both ICD and threshold     :\", int(((cohort_any[\"pco2_threshold_any\"]==1) & (cohort_any[\"any_hypercap_icd\"]==1)).sum()))\n",
    "print(\"Total enrolled (union)     :\", int((cohort_any[\"enrolled_any\"]==1).sum()))\n",
    "\n",
    "# New hadm list used for the rest of the notebook\n",
    "hadm_list = cohort_any.loc[cohort_any[\"enrolled_any\"]==1, \"hadm_id\"].dropna().astype(\"int64\").tolist()\n",
    "len(hadm_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe110d70",
   "metadata": {},
   "source": [
    "## 4) First ABG and First VBG (LAB + POC, standardized to mmHg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd25d61",
   "metadata": {},
   "source": [
    "**Rationale:** Extract earliest ABG/VBG measurements to characterize baseline physiology with standardized units.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d61ad70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:27:42.223856Z",
     "iopub.status.busy": "2026-02-05T23:27:42.223683Z",
     "iopub.status.idle": "2026-02-05T23:27:47.947428Z",
     "shell.execute_reply": "2026-02-05T23:27:47.942418Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.\n",
    "\n",
    "params = {\"hadms\": hadm_list}\n",
    "\n",
    "bg_pairs_sql = rf\"\"\"\n",
    "WITH hadms AS (SELECT hadm_id FROM UNNEST(@hadms) AS hadm_id),\n",
    "\n",
    "/* ---------------- LAB (HOSP) ---------------- */\n",
    "hosp_cand AS (\n",
    "  SELECT\n",
    "    le.subject_id, le.hadm_id, le.charttime, le.specimen_id,\n",
    "    CAST(le.valuenum AS FLOAT64) AS val,\n",
    "    LOWER(COALESCE(le.valueuom,'')) AS uom,\n",
    "    LOWER(di.label) AS lbl,\n",
    "    LOWER(COALESCE(di.fluid,'')) AS fl,\n",
    "    LOWER(COALESCE(di.category,'')) AS cat\n",
    "  FROM `{PHYS}.{HOSP}.labevents`  le\n",
    "  JOIN `{PHYS}.{HOSP}.d_labitems` di ON di.itemid = le.itemid\n",
    "  JOIN hadms h ON h.hadm_id = le.hadm_id\n",
    "  WHERE le.valuenum IS NOT NULL\n",
    "    AND (\n",
    "         LOWER(COALESCE(di.category,'')) LIKE '%blood gas%' OR\n",
    "         LOWER(di.label) LIKE '%pco2%' OR\n",
    "         REGEXP_CONTAINS(LOWER(di.label), r'\\bph\\b') OR\n",
    "         REGEXP_CONTAINS(LOWER(di.label), r'\\bpa?\\s*co(?:2|₂)\\b')\n",
    "        )\n",
    "    AND NOT REGEXP_CONTAINS(LOWER(di.label), r'(et\\s*co2|end[- ]?tidal|t\\s*co2|tco2|total\\s*co2|hco3|bicar)')\n",
    "),\n",
    "hosp_spec AS (\n",
    "  SELECT le.specimen_id, LOWER(COALESCE(le.value,'')) AS spec_val\n",
    "  FROM `{PHYS}.{HOSP}.labevents` le\n",
    "  JOIN `{PHYS}.{HOSP}.d_labitems` di ON di.itemid = le.itemid\n",
    "  WHERE le.specimen_id IS NOT NULL\n",
    "    AND REGEXP_CONTAINS(LOWER(di.label), r'(specimen|sample)')\n",
    "),\n",
    "hosp_class AS (\n",
    "  SELECT\n",
    "    c.hadm_id, c.charttime, c.specimen_id, c.val, c.uom, c.lbl, c.fl,\n",
    "    CASE\n",
    "      WHEN REGEXP_CONTAINS(c.lbl, r'\\b(?:blood\\s*)?ph\\b') THEN 'ph'\n",
    "      WHEN (c.lbl LIKE '%pco2%' OR REGEXP_CONTAINS(c.lbl, r'\\bpa?\\s*co(?:2|₂)\\b')) THEN 'pco2'\n",
    "      ELSE NULL\n",
    "    END AS analyte,\n",
    "    CASE\n",
    "      WHEN REGEXP_CONTAINS(s.spec_val, r'arter') OR REGEXP_CONTAINS(s.spec_val, r'\\bart\\b') THEN 'arterial'\n",
    "      WHEN REGEXP_CONTAINS(s.spec_val, r'ven|mixed|central') THEN 'venous'\n",
    "      WHEN c.fl LIKE '%arterial%' OR REGEXP_CONTAINS(c.lbl, r'\\b(abg|art|arterial|a[- ]?line)\\b') THEN 'arterial'\n",
    "      WHEN c.fl LIKE '%ven%'      OR REGEXP_CONTAINS(c.lbl, r'\\b(vbg|ven|venous|mixed|central)\\b') THEN 'venous'\n",
    "      ELSE 'other'\n",
    "    END AS site\n",
    "  FROM hosp_cand c\n",
    "  LEFT JOIN hosp_spec s USING (specimen_id)\n",
    "),\n",
    "hosp_pairs AS (\n",
    "  SELECT\n",
    "    hadm_id, specimen_id,\n",
    "    MIN(charttime) AS sample_time,\n",
    "    MAX(IF(analyte='ph',   val, NULL)) AS ph,\n",
    "    MAX(IF(analyte='pco2', val, NULL)) AS pco2_raw,\n",
    "    (ARRAY_AGG(IF(analyte='pco2', uom, NULL) IGNORE NULLS LIMIT 1))[OFFSET(0)] AS pco2_uom,\n",
    "    (ARRAY_AGG(IF(analyte='ph',   uom, NULL) IGNORE NULLS LIMIT 1))[OFFSET(0)] AS ph_uom,\n",
    "    (ARRAY_AGG(site IGNORE NULLS LIMIT 1))[OFFSET(0)] AS site\n",
    "  FROM hosp_class\n",
    "  GROUP BY hadm_id, specimen_id\n",
    "  HAVING (ph IS NOT NULL OR pco2_raw IS NOT NULL) AND site IN ('arterial','venous','other')\n",
    "),\n",
    "hosp_pairs_std AS (\n",
    "  SELECT\n",
    "    hadm_id, specimen_id, sample_time, site,\n",
    "    ph, ph_uom,\n",
    "    CASE WHEN pco2_uom = 'kpa' THEN pco2_raw * 7.50062 ELSE pco2_raw END AS pco2_mmHg,\n",
    "    'mmhg' AS pco2_uom_norm\n",
    "  FROM hosp_pairs\n",
    "  WHERE (ph IS NULL OR (ph BETWEEN 6.3 AND 7.8))\n",
    "    AND (pco2_raw IS NULL OR (CASE WHEN pco2_uom='kpa' THEN pco2_raw*7.50062 ELSE pco2_raw END) BETWEEN 5 AND 200)\n",
    "),\n",
    "lab_abg AS (\n",
    "  SELECT hadm_id,\n",
    "         ph            AS lab_abg_ph,\n",
    "         ph_uom        AS lab_abg_ph_uom,\n",
    "         pco2_mmHg     AS lab_abg_paco2,\n",
    "         'mmhg'        AS lab_abg_paco2_uom,\n",
    "         sample_time   AS lab_abg_time\n",
    "  FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY hadm_id ORDER BY sample_time) rn\n",
    "        FROM hosp_pairs_std WHERE site='arterial') WHERE rn=1\n",
    "),\n",
    "lab_vbg AS (\n",
    "  SELECT hadm_id,\n",
    "         ph            AS lab_vbg_ph,\n",
    "         ph_uom        AS lab_vbg_ph_uom,\n",
    "         pco2_mmHg     AS lab_vbg_paco2,\n",
    "         'mmhg'        AS lab_vbg_paco2_uom,\n",
    "         sample_time   AS lab_vbg_time\n",
    "  FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY hadm_id ORDER BY sample_time) rn\n",
    "        FROM hosp_pairs_std WHERE site='venous') WHERE rn=1\n",
    "),\n",
    "lab_other AS (\n",
    "  SELECT hadm_id,\n",
    "         ph            AS lab_other_ph,\n",
    "         ph_uom        AS lab_other_ph_uom,\n",
    "         pco2_mmHg     AS lab_other_paco2,\n",
    "         'mmhg'        AS lab_other_paco2_uom,\n",
    "         sample_time   AS lab_other_time\n",
    "  FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY hadm_id ORDER BY sample_time) rn\n",
    "        FROM hosp_pairs_std WHERE site='other') WHERE rn=1\n",
    "),\n",
    "\n",
    "/* ---------------- POC (ICU) ---------------- */\n",
    "icu_raw AS (\n",
    "  SELECT\n",
    "    ie.hadm_id, ce.stay_id, ce.charttime,\n",
    "    LOWER(di.label) AS lbl,\n",
    "    LOWER(REPLACE(COALESCE(ce.valueuom,''),' ','')) AS uom_nospace,\n",
    "    LOWER(COALESCE(ce.value,'')) AS valstr,\n",
    "    COALESCE(\n",
    "      CAST(ce.valuenum AS FLOAT64),\n",
    "      SAFE_CAST(REGEXP_EXTRACT(LOWER(ce.value), r'(-?\\d+(?:\\.\\d+)?)') AS FLOAT64)\n",
    "    ) AS val\n",
    "  FROM `{PHYS}.{ICU}.chartevents` ce\n",
    "  JOIN `{PHYS}.{ICU}.d_items`  di ON di.itemid = ce.itemid\n",
    "  JOIN `{PHYS}.{ICU}.icustays` ie ON ie.stay_id = ce.stay_id\n",
    "  JOIN hadms h ON h.hadm_id = ie.hadm_id\n",
    "),\n",
    "icu_cand AS (\n",
    "  SELECT\n",
    "    hadm_id, stay_id, charttime, lbl, uom_nospace, valstr, val,\n",
    "    CASE\n",
    "      WHEN REGEXP_CONTAINS(lbl, r'(^|[^a-z])ph([^a-z]|$)') OR (val BETWEEN 6.3 AND 7.8) THEN 'ph'\n",
    "      WHEN (\n",
    "             REGEXP_CONTAINS(lbl, r'(^|[^a-z])p(?:a)?\\s*co\\s*(?:2|₂)([^a-z]|$)')\n",
    "             OR uom_nospace IN ('mmhg','kpa')\n",
    "             OR REGEXP_CONTAINS(valstr, r'\\b(mm\\s*hg|kpa)\\b')\n",
    "           )\n",
    "           AND NOT REGEXP_CONTAINS(lbl, r'(et\\s*co2|end[- ]?tidal|t\\s*co2|tco2|total\\s*co2|hco3|bicar|v\\s*co2|vco2|co2\\s*(prod|elimin|production|elimination))')\n",
    "      THEN 'pco2'\n",
    "      ELSE NULL\n",
    "    END AS analyte,\n",
    "    CASE\n",
    "      WHEN REGEXP_CONTAINS(lbl, r'\\b(abg|art|arterial|a[- ]?line)\\b') THEN 'arterial'\n",
    "      WHEN REGEXP_CONTAINS(lbl, r'\\b(vbg|ven|venous|mixed|central)\\b') THEN 'venous'\n",
    "      ELSE 'other'\n",
    "    END AS site\n",
    "  FROM icu_raw\n",
    "  WHERE val IS NOT NULL\n",
    "    AND (\n",
    "      REGEXP_CONTAINS(lbl, r'(^|[^a-z])ph([^a-z]|$)') OR\n",
    "      REGEXP_CONTAINS(lbl, r'(^|[^a-z])p(?:a)?\\s*co\\s*(?:2|₂)([^a-z]|$)') OR\n",
    "      uom_nospace IN ('mmhg','kpa') OR\n",
    "      REGEXP_CONTAINS(valstr, r'\\b(mm\\s*hg|kpa)\\b')\n",
    "    )\n",
    "    AND NOT REGEXP_CONTAINS(lbl, r'(et\\s*co2|end[- ]?tidal|t\\s*co2|tco2|total\\s*co2|hco3|bicar|v\\s*co2|vco2|co2\\s*(prod|elimin|production|elimination))')\n",
    "),\n",
    "icu_ph AS (\n",
    "  SELECT hadm_id, stay_id, charttime, val AS ph, site AS site_ph\n",
    "  FROM icu_cand WHERE analyte='ph'\n",
    "),\n",
    "icu_co2 AS (\n",
    "  SELECT hadm_id, stay_id, charttime, val AS pco2_raw, uom_nospace, valstr, site AS site_co2\n",
    "  FROM icu_cand WHERE analyte='pco2'\n",
    "),\n",
    "icu_pair_win AS (\n",
    "  SELECT\n",
    "    p.hadm_id, p.stay_id,\n",
    "    COALESCE(p.site_ph, c.site_co2) AS site,\n",
    "    p.charttime AS ph_time, c.charttime AS co2_time,\n",
    "    p.ph,\n",
    "    CASE\n",
    "      WHEN c.uom_nospace='kpa' OR REGEXP_CONTAINS(c.valstr, r'\\bkpa\\b') THEN 'kpa'\n",
    "      WHEN c.uom_nospace='mmhg' OR REGEXP_CONTAINS(c.valstr, r'mm\\s*hg') THEN 'mmhg'\n",
    "      ELSE c.uom_nospace\n",
    "    END AS pco2_uom_norm_raw,\n",
    "    c.pco2_raw,\n",
    "    ABS(TIMESTAMP_DIFF(c.charttime, p.charttime, SECOND)) AS dt_sec\n",
    "  FROM icu_ph p\n",
    "  JOIN icu_co2 c\n",
    "    ON c.hadm_id = p.hadm_id\n",
    "   AND c.stay_id = p.stay_id\n",
    "   AND (COALESCE(p.site_ph, c.site_co2) IN ('arterial','venous','other'))\n",
    "   AND ABS(TIMESTAMP_DIFF(c.charttime, p.charttime, MINUTE)) <= 10\n",
    "  QUALIFY ROW_NUMBER() OVER (\n",
    "    PARTITION BY p.hadm_id, p.stay_id, p.charttime\n",
    "    ORDER BY dt_sec\n",
    "  ) = 1\n",
    "),\n",
    "icu_pairs_std AS (\n",
    "  SELECT\n",
    "    hadm_id, stay_id, site,\n",
    "    LEAST(ph_time, co2_time) AS sample_time,\n",
    "    ph,\n",
    "    CAST(NULL AS STRING) AS ph_uom,              -- POC pH is unitless/null\n",
    "    CASE WHEN pco2_uom_norm_raw='kpa' THEN pco2_raw*7.50062 ELSE pco2_raw END AS pco2_mmHg,\n",
    "    'mmhg' AS pco2_uom_norm\n",
    "  FROM icu_pair_win\n",
    "  WHERE (ph BETWEEN 6.3 AND 7.8 OR ph IS NULL)\n",
    "    AND (CASE WHEN pco2_uom_norm_raw='kpa' THEN pco2_raw*7.50062 ELSE pco2_raw END) BETWEEN 5 AND 200\n",
    "),\n",
    "icu_solo_pco2_std AS (\n",
    "  SELECT\n",
    "    hadm_id, stay_id, site_co2 AS site,\n",
    "    charttime AS sample_time,\n",
    "    CAST(NULL AS FLOAT64) AS ph,\n",
    "    CAST(NULL AS STRING)  AS ph_uom,            -- no pH here\n",
    "    CASE WHEN uom_nospace='kpa' OR REGEXP_CONTAINS(valstr, r'\\bkpa\\b') THEN pco2_raw*7.50062 ELSE pco2_raw END AS pco2_mmHg,\n",
    "    'mmhg' AS pco2_uom_norm\n",
    "  FROM icu_co2\n",
    "  WHERE site_co2 IN ('arterial','venous','other')\n",
    "    AND pco2_raw BETWEEN 5 AND 200\n",
    "),\n",
    "icu_all AS (\n",
    "  SELECT * FROM icu_pairs_std\n",
    "  UNION ALL\n",
    "  SELECT * FROM icu_solo_pco2_std\n",
    "),\n",
    "\n",
    "poc_abg AS (\n",
    "  SELECT hadm_id,\n",
    "         ph            AS poc_abg_ph,\n",
    "         ph_uom        AS poc_abg_ph_uom,\n",
    "         pco2_mmHg     AS poc_abg_paco2,\n",
    "         'mmhg'        AS poc_abg_paco2_uom,\n",
    "         sample_time   AS poc_abg_time\n",
    "  FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY hadm_id ORDER BY sample_time) rn\n",
    "        FROM icu_all WHERE site='arterial') WHERE rn=1\n",
    "),\n",
    "poc_vbg AS (\n",
    "  SELECT hadm_id,\n",
    "         ph            AS poc_vbg_ph,\n",
    "         ph_uom        AS poc_vbg_ph_uom,\n",
    "         pco2_mmHg     AS poc_vbg_paco2,\n",
    "         'mmhg'        AS poc_vbg_paco2_uom,\n",
    "         sample_time   AS poc_vbg_time\n",
    "  FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY hadm_id ORDER BY sample_time) rn\n",
    "        FROM icu_all WHERE site='venous') WHERE rn=1\n",
    "),\n",
    "poc_other AS (\n",
    "  SELECT hadm_id,\n",
    "         ph            AS poc_other_ph,\n",
    "         ph_uom        AS poc_other_ph_uom,\n",
    "         pco2_mmHg     AS poc_other_paco2,\n",
    "         'mmhg'        AS poc_other_paco2_uom,\n",
    "         sample_time   AS poc_other_time\n",
    "  FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY hadm_id ORDER BY sample_time) rn\n",
    "        FROM icu_all WHERE site='other') WHERE rn=1\n",
    ")\n",
    "\n",
    "/* ---------------- Final one row per hadm ---------------- */\n",
    "SELECT\n",
    "  h.hadm_id,\n",
    "  -- LAB-ABG / LAB-VBG / LAB-OTHER\n",
    "  la.lab_abg_ph, la.lab_abg_ph_uom, la.lab_abg_paco2, la.lab_abg_paco2_uom, la.lab_abg_time,\n",
    "  lv.lab_vbg_ph, lv.lab_vbg_ph_uom, lv.lab_vbg_paco2, lv.lab_vbg_paco2_uom, lv.lab_vbg_time,\n",
    "  lo.lab_other_ph, lo.lab_other_ph_uom, lo.lab_other_paco2, lo.lab_other_paco2_uom, lo.lab_other_time,\n",
    "  -- POC-ABG / POC-VBG / POC-OTHER\n",
    "  pa.poc_abg_ph, pa.poc_abg_ph_uom, pa.poc_abg_paco2, pa.poc_abg_paco2_uom, pa.poc_abg_time,\n",
    "  pv.poc_vbg_ph, pv.poc_vbg_ph_uom, pv.poc_vbg_paco2, pv.poc_vbg_paco2_uom, pv.poc_vbg_time,\n",
    "  po.poc_other_ph, po.poc_other_ph_uom, po.poc_other_paco2, po.poc_other_paco2_uom, po.poc_other_time,\n",
    "  -- First ABG across LAB+POC\n",
    "  (SELECT AS STRUCT src, t, ph, pco2\n",
    "   FROM (SELECT 'LAB' AS src, la.lab_abg_time AS t, la.lab_abg_ph AS ph, la.lab_abg_paco2 AS pco2\n",
    "         UNION ALL\n",
    "         SELECT 'POC', pa.poc_abg_time, pa.poc_abg_ph, pa.poc_abg_paco2)\n",
    "   WHERE t IS NOT NULL\n",
    "   ORDER BY t LIMIT 1) AS first_abg,\n",
    "  -- First VBG across LAB+POC\n",
    "  (SELECT AS STRUCT src, t, ph, pco2\n",
    "   FROM (SELECT 'LAB' AS src, lv.lab_vbg_time AS t, lv.lab_vbg_ph AS ph, lv.lab_vbg_paco2 AS pco2\n",
    "         UNION ALL\n",
    "         SELECT 'POC', pv.poc_vbg_time, pv.poc_vbg_ph, pv.poc_vbg_paco2)\n",
    "   WHERE t IS NOT NULL\n",
    "   ORDER BY t LIMIT 1) AS first_vbg,\n",
    "  -- First OTHER-source pCO2 across LAB+POC\n",
    "  (SELECT AS STRUCT src, t, ph, pco2\n",
    "   FROM (SELECT 'LAB' AS src, lo.lab_other_time AS t, lo.lab_other_ph AS ph, lo.lab_other_paco2 AS pco2\n",
    "         UNION ALL\n",
    "         SELECT 'POC', po.poc_other_time, po.poc_other_ph, po.poc_other_paco2)\n",
    "   WHERE t IS NOT NULL\n",
    "   ORDER BY t LIMIT 1) AS first_other\n",
    "FROM hadms h\n",
    "LEFT JOIN lab_abg la USING (hadm_id)\n",
    "LEFT JOIN lab_vbg lv USING (hadm_id)\n",
    "LEFT JOIN lab_other lo USING (hadm_id)\n",
    "LEFT JOIN poc_abg pa USING (hadm_id)\n",
    "LEFT JOIN poc_vbg pv USING (hadm_id)\n",
    "LEFT JOIN poc_other po USING (hadm_id)\n",
    "\"\"\"\n",
    "\n",
    "bg_pairs = run_sql_bq(bg_pairs_sql, params)\n",
    "\n",
    "# Flatten STRUCTs for first_abg, first_vbg, and first_other\n",
    "for col in [\"first_abg\",\"first_vbg\",\"first_other\"]:\n",
    "    if col in bg_pairs.columns:\n",
    "        bg_pairs[f\"{col}_src\"]  = bg_pairs[col].apply(lambda x: x.get(\"src\") if isinstance(x, dict) else None)\n",
    "        bg_pairs[f\"{col}_time\"] = bg_pairs[col].apply(lambda x: x.get(\"t\")   if isinstance(x, dict) else None)\n",
    "        bg_pairs[f\"{col}_ph\"]   = bg_pairs[col].apply(lambda x: x.get(\"ph\")  if isinstance(x, dict) else None)\n",
    "        bg_pairs[f\"{col}_pco2\"] = bg_pairs[col].apply(lambda x: x.get(\"pco2\")if isinstance(x, dict) else None)\n",
    "        bg_pairs = bg_pairs.drop(columns=[col])\n",
    "\n",
    "bg_pairs.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eba414",
   "metadata": {},
   "source": [
    "## 5) Demographics & outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac36c218",
   "metadata": {},
   "source": [
    "**Rationale:** Build baseline covariates used for descriptive statistics and potential confounding adjustment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f2d449",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:27:47.978401Z",
     "iopub.status.busy": "2026-02-05T23:27:47.971151Z",
     "iopub.status.idle": "2026-02-05T23:27:52.911251Z",
     "shell.execute_reply": "2026-02-05T23:27:52.909299Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.\n",
    "\n",
    "\n",
    "SQL[\"demo_sql\"] = f\"\"\"\n",
    "WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x)\n",
    "SELECT\n",
    "  a.hadm_id,\n",
    "  a.subject_id,\n",
    "  a.admittime,\n",
    "  a.dischtime,\n",
    "  a.deathtime,\n",
    "  a.admission_type,\n",
    "  a.admission_location,\n",
    "  a.discharge_location,\n",
    "  a.insurance,\n",
    "  -- LOS (days)\n",
    "  TIMESTAMP_DIFF(a.dischtime, a.admittime, HOUR) / 24.0 AS hosp_los_days,\n",
    "  -- in-hospital death\n",
    "  IF(a.deathtime IS NOT NULL, 1, 0) AS death_in_hosp,\n",
    "  -- demographics\n",
    "  p.gender,\n",
    "  SAFE_CAST(ROUND(p.anchor_age + (EXTRACT(YEAR FROM a.admittime) - p.anchor_year), 1) AS FLOAT64) AS age_at_admit,\n",
    "  -- 30-day all-cause mortality from admission\n",
    "  IF(p.dod IS NOT NULL AND DATE_DIFF(DATE(p.dod), DATE(a.admittime), DAY) BETWEEN 0 AND 30, 1, 0) AS death_30d\n",
    "FROM `{PHYS}.{HOSP}.admissions` a\n",
    "JOIN hadms h USING (hadm_id)\n",
    "JOIN `{PHYS}.{HOSP}.patients` p USING (subject_id)\n",
    "\"\"\"\n",
    "demo = run_sql_bq(sql(\"demo_sql\"), {\"hadms\": hadm_list})\n",
    "print(\"Demo rows:\", len(demo))\n",
    "demo.head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fee13e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:27:52.918960Z",
     "iopub.status.busy": "2026-02-05T23:27:52.918659Z",
     "iopub.status.idle": "2026-02-05T23:27:52.927557Z",
     "shell.execute_reply": "2026-02-05T23:27:52.927313Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Create reusable data-quality and merge guardrail helpers to prevent silent join errors.\n",
    "\n",
    "# ==== Drop-in: safe merge utilities (one cell, run once) ====\n",
    "import pandas as pd\n",
    "from typing import Iterable, Optional, Literal\n",
    "\n",
    "def _ensure_Int64(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Coerce to pandas nullable Int64 (preserves NA).\"\"\"\n",
    "    return pd.to_numeric(s, errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "def strip_subject_cols(fr: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove any subject_id-like columns from a frame (e.g., 'subject_id', 'Subject_ID').\"\"\"\n",
    "    return fr.drop(columns=[c for c in fr.columns if c.lower().startswith(\"subject_id\")],\n",
    "                   errors=\"ignore\")\n",
    "\n",
    "def safe_merge_on_hadm(\n",
    "    left: pd.DataFrame,\n",
    "    right: pd.DataFrame,\n",
    "    *,\n",
    "    right_name: str,\n",
    "    take: Optional[Iterable[str]] = None,\n",
    "    order_by: Optional[Iterable[str]] = None,\n",
    "    check_subject: Literal[False, \"warn\", \"raise\"] = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Left-merge 'right' into 'left' on hadm_id, returning a copy of left with right's columns.\n",
    "    - Dedupes right on hadm_id (optionally using order_by to pick the first row).\n",
    "    - Optionally restricts right columns via `take`.\n",
    "    - Optionally audits subject_id agreement before dropping subject_id from right.\n",
    "    - Always strips subject_id-like columns from the right to prevent *_x/_y suffixes.\n",
    "    - Raises if any *_x/_y suffixes still appear (indicates overlapping names besides hadm_id).\n",
    "    \"\"\"\n",
    "    if \"hadm_id\" not in left.columns:\n",
    "        raise KeyError(f\"left frame lacks hadm_id before merging {right_name}\")\n",
    "    if \"hadm_id\" not in right.columns:\n",
    "        raise KeyError(f\"{right_name} lacks hadm_id\")\n",
    "\n",
    "    L = left.copy()\n",
    "    R = right.copy()\n",
    "\n",
    "    # Standardize dtypes of keys\n",
    "    L[\"hadm_id\"] = _ensure_Int64(L[\"hadm_id\"])\n",
    "    R[\"hadm_id\"] = _ensure_Int64(R[\"hadm_id\"])\n",
    "    if \"subject_id\" in L.columns:\n",
    "        L[\"subject_id\"] = _ensure_Int64(L[\"subject_id\"])\n",
    "    if \"subject_id\" in R.columns:\n",
    "        R[\"subject_id\"] = _ensure_Int64(R[\"subject_id\"])\n",
    "\n",
    "    # Dedupe RIGHT by hadm_id (optionally order_by first)\n",
    "    if order_by:\n",
    "        R = (R.sort_values(list(order_by))\n",
    "               .drop_duplicates(subset=[\"hadm_id\"], keep=\"first\"))\n",
    "    else:\n",
    "        R = R.drop_duplicates(subset=[\"hadm_id\"], keep=\"first\")\n",
    "\n",
    "    # Optional subject_id consistency audit (before stripping)\n",
    "    if check_subject and (\"subject_id\" in L.columns) and (\"subject_id\" in R.columns):\n",
    "        # Join only on hadm_id where both sides have subject_id\n",
    "        tmp = (L[[\"hadm_id\", \"subject_id\"]]\n",
    "                 .merge(R[[\"hadm_id\", \"subject_id\"]],\n",
    "                        on=\"hadm_id\", how=\"inner\", suffixes=(\"_L\",\"_R\")))\n",
    "        mism = (tmp[\"subject_id_L\"].notna() & tmp[\"subject_id_R\"].notna() &\n",
    "                (tmp[\"subject_id_L\"] != tmp[\"subject_id_R\"]))\n",
    "        n_mism = int(mism.sum())\n",
    "        if n_mism > 0:\n",
    "            sample_ids = tmp.loc[mism, \"hadm_id\"].head(10).tolist()\n",
    "            msg = (f\"[{right_name}] subject_id mismatch on {n_mism} hadm_id(s). \"\n",
    "                   f\"Examples: {sample_ids}\")\n",
    "            if check_subject == \"raise\":\n",
    "                raise ValueError(msg)\n",
    "            else:\n",
    "                print(\"WARNING:\", msg)\n",
    "\n",
    "    # Limit right columns (avoid accidental overlaps)\n",
    "    if take is not None:\n",
    "        keep = [\"hadm_id\"] + [c for c in take if c != \"hadm_id\"]\n",
    "        R = R[[c for c in keep if c in R.columns]]\n",
    "\n",
    "    # Always strip subject_id-like columns from right to prevent *_x/_y\n",
    "    R = strip_subject_cols(R)\n",
    "\n",
    "    # Final merge\n",
    "    out = L.merge(R, on=\"hadm_id\", how=\"left\", suffixes=(\"\", \"\"))\n",
    "\n",
    "    # Guard: no suffixes should be present\n",
    "    bad = [c for c in out.columns if c.endswith(\"_x\") or c.endswith(\"_y\")]\n",
    "    if bad:\n",
    "        raise RuntimeError(\n",
    "            f\"Merge with {right_name} produced suffixed columns {bad}. \"\n",
    "            \"You likely have overlapping column names other than hadm_id.\"\n",
    "        )\n",
    "    return out\n",
    "\n",
    "print(\"Safe merge helpers loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c42e42",
   "metadata": {},
   "source": [
    "## 6) NIH/OMB race & ethnicity (ED + Hospital)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af02bf38",
   "metadata": {},
   "source": [
    "**Rationale:** Harmonize race/ethnicity across sources using NIH/OMB categories for consistent reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb57c105",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:27:52.928856Z",
     "iopub.status.busy": "2026-02-05T23:27:52.928768Z",
     "iopub.status.idle": "2026-02-05T23:27:57.565332Z",
     "shell.execute_reply": "2026-02-05T23:27:57.562958Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.\n",
    "\n",
    "race_eth_sql = rf\"\"\"\n",
    "WITH hadms AS (\n",
    "  SELECT x AS hadm_id\n",
    "  FROM UNNEST(@hadms) AS x\n",
    "),\n",
    "\n",
    "-- Hospital admission \"race\" text\n",
    "hosp AS (\n",
    "  SELECT a.hadm_id, LOWER(TRIM(a.race)) AS race_hosp_raw\n",
    "  FROM `{PHYS}.{HOSP}.admissions` a\n",
    "  JOIN hadms hm USING (hadm_id)\n",
    "),\n",
    "\n",
    "-- Earliest ED stay leading to the admission; take its \"race\" text if present\n",
    "ed_first AS (\n",
    "  SELECT\n",
    "    e.hadm_id,\n",
    "    (ARRAY_AGG(STRUCT(e.intime AS intime, LOWER(TRIM(e.race)) AS race_ed_raw)\n",
    "               ORDER BY e.intime ASC LIMIT 1))[OFFSET(0)] AS pick\n",
    "  FROM `{PHYS}.{ED}.edstays` e\n",
    "  JOIN hadms hm USING (hadm_id)\n",
    "  GROUP BY e.hadm_id\n",
    "),\n",
    "ed AS (\n",
    "  SELECT hadm_id, pick.race_ed_raw\n",
    "  FROM ed_first\n",
    "),\n",
    "\n",
    "-- Combine ED + Hospital for maximum coverage\n",
    "comb AS (\n",
    "  SELECT\n",
    "    hm.hadm_id,\n",
    "    ho.race_hosp_raw,\n",
    "    ed.race_ed_raw,\n",
    "    TRIM(REGEXP_REPLACE(CONCAT(COALESCE(ho.race_hosp_raw,''), ' ', COALESCE(ed.race_ed_raw,'')), r'\\s+', ' ')) AS race_text_any\n",
    "  FROM hadms hm\n",
    "  LEFT JOIN hosp ho USING (hadm_id)\n",
    "  LEFT JOIN ed   ed USING (hadm_id)\n",
    "),\n",
    "\n",
    "-- Tokenization to OMB families + Hispanic ethnicity\n",
    "tok AS (\n",
    "  SELECT\n",
    "    hadm_id, race_hosp_raw, race_ed_raw, race_text_any,\n",
    "\n",
    "    -- Ethnicity (Hispanic)\n",
    "    REGEXP_CONTAINS(race_text_any, r'\\b(hispanic|latinx|latino|latina)\\b') AS is_hisp,\n",
    "\n",
    "    -- Race families (use boundaries to reduce false positives)\n",
    "    REGEXP_CONTAINS(race_text_any, r'american\\s+indian|\\balaska\\b') AS is_aian,\n",
    "    REGEXP_CONTAINS(race_text_any, r'\\basian\\b') AS is_asian,\n",
    "    REGEXP_CONTAINS(race_text_any, r'\\b(black|african\\s+american)\\b') AS is_black,\n",
    "    REGEXP_CONTAINS(race_text_any, r'hawaiian|pacific\\s+islander') AS is_nhopi,\n",
    "    REGEXP_CONTAINS(race_text_any, r'\\bwhite\\b|caucasian') AS is_white,\n",
    "\n",
    "    -- Unknown/other indicators\n",
    "    REGEXP_CONTAINS(race_text_any, r'unknown|other|declined|unable|not\\s+reported|missing|null') AS is_unknown_any,\n",
    "\n",
    "    -- Multi-race hints\n",
    "    REGEXP_CONTAINS(race_text_any, r'(two|2)\\s+or\\s+more|multi|biracial|multiracial') AS is_multi_hint\n",
    "  FROM comb\n",
    "),\n",
    "\n",
    "-- Decide ethnicity per NIH\n",
    "ethn AS (\n",
    "  SELECT\n",
    "    hadm_id, race_hosp_raw, race_ed_raw, race_text_any,\n",
    "    CASE\n",
    "      WHEN is_hisp THEN 'Hispanic or Latino'\n",
    "      WHEN (race_text_any IS NULL OR race_text_any = '' OR is_unknown_any) THEN 'Unknown or Not Reported'\n",
    "      ELSE 'Not Hispanic or Latino'\n",
    "    END AS nih_ethnicity,\n",
    "    (CAST(is_aian AS INT64) + CAST(is_asian AS INT64) + CAST(is_black AS INT64)\n",
    "     + CAST(is_nhopi AS INT64) + CAST(is_white AS INT64)) AS race_hits,\n",
    "    is_aian, is_asian, is_black, is_nhopi, is_white, is_multi_hint, is_unknown_any\n",
    "  FROM tok\n",
    "),\n",
    "\n",
    "-- Decide race per NIH/OMB (1997)\n",
    "race_assign AS (\n",
    "  SELECT\n",
    "    hadm_id, race_hosp_raw, race_ed_raw, race_text_any, nih_ethnicity,\n",
    "    CASE\n",
    "      WHEN race_hits >= 2 OR is_multi_hint THEN 'More than one race'\n",
    "      WHEN is_aian THEN 'American Indian or Alaska Native'\n",
    "      WHEN is_asian THEN 'Asian'\n",
    "      WHEN is_black THEN 'Black or African American'\n",
    "      WHEN is_nhopi THEN 'Native Hawaiian or Other Pacific Islander'\n",
    "      WHEN is_white THEN 'White'\n",
    "      WHEN is_unknown_any OR race_text_any IS NULL OR race_text_any = '' THEN 'Unknown or Not Reported'\n",
    "      ELSE 'Unknown or Not Reported'\n",
    "    END AS nih_race\n",
    "  FROM ethn\n",
    ")\n",
    "\n",
    "SELECT hadm_id, race_hosp_raw, race_ed_raw, nih_race, nih_ethnicity\n",
    "FROM race_assign\n",
    "\"\"\"\n",
    "race_eth = run_sql_bq(race_eth_sql, {\"hadms\": hadm_list})\n",
    "print(\"Race/Eth rows:\", len(race_eth))\n",
    "race_eth.head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46324987",
   "metadata": {},
   "source": [
    "## 7) ED triage (linked to hadm) and first ED vitals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2cbc23",
   "metadata": {},
   "source": [
    "**Rationale:** Capture ED presentation features (vitals and chief complaint) for symptom and severity analyses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36d4797",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:27:57.573313Z",
     "iopub.status.busy": "2026-02-05T23:27:57.573179Z",
     "iopub.status.idle": "2026-02-05T23:28:07.451915Z",
     "shell.execute_reply": "2026-02-05T23:28:07.450078Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Pull ED triage/first-vitals by loading ED tables once and filtering to cohort hadm_ids in pandas.\n",
    "\n",
    "hadms_for_ed = set(\n",
    "    pd.Series(hadm_list)\n",
    "    .dropna()\n",
    "    .astype(\"int64\")\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "# 1) ED stay map (full table), then restrict to cohort hadm_ids locally.\n",
    "SQL[\"edmap_all_sql\"] = f\"\"\"\n",
    "SELECT stay_id, hadm_id, intime\n",
    "FROM `{PHYS}.{ED}.edstays`\n",
    "WHERE hadm_id IS NOT NULL\n",
    "\"\"\"\n",
    "edmap_all = run_sql_bq(sql(\"edmap_all_sql\"))\n",
    "edmap_all[\"hadm_id\"] = pd.to_numeric(edmap_all[\"hadm_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "edmap_all[\"stay_id\"] = pd.to_numeric(edmap_all[\"stay_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "edmap_all[\"intime\"] = pd.to_datetime(edmap_all[\"intime\"], errors=\"coerce\")\n",
    "\n",
    "edmap = edmap_all[edmap_all[\"hadm_id\"].isin(hadms_for_ed)].copy()\n",
    "edmap = edmap.drop_duplicates(subset=[\"stay_id\", \"hadm_id\"])\n",
    "print(\"ED map rows (cohort):\", len(edmap))\n",
    "\n",
    "if edmap.empty:\n",
    "    ed_triage = pd.DataFrame(columns=[\n",
    "        \"hadm_id\", \"ed_triage_temp\", \"ed_triage_hr\", \"ed_triage_rr\",\n",
    "        \"ed_triage_o2sat\", \"ed_triage_sbp\", \"ed_triage_dbp\", \"ed_triage_pain\",\n",
    "        \"ed_triage_acuity\", \"ed_triage_cc\",\n",
    "    ])\n",
    "    ed_first = pd.DataFrame(columns=[\n",
    "        \"hadm_id\", \"ed_first_vitals_time\", \"ed_first_temp\", \"ed_first_hr\",\n",
    "        \"ed_first_rr\", \"ed_first_o2sat\", \"ed_first_sbp\", \"ed_first_dbp\",\n",
    "        \"ed_first_rhythm\", \"ed_first_pain\",\n",
    "    ])\n",
    "else:\n",
    "    # 2) ED triage (full table), then reduce to earliest ED stay per hadm.\n",
    "    SQL[\"ed_triage_all_sql\"] = f\"\"\"\n",
    "    SELECT\n",
    "      stay_id,\n",
    "      temperature    AS ed_triage_temp,\n",
    "      heartrate      AS ed_triage_hr,\n",
    "      resprate       AS ed_triage_rr,\n",
    "      o2sat          AS ed_triage_o2sat,\n",
    "      sbp            AS ed_triage_sbp,\n",
    "      dbp            AS ed_triage_dbp,\n",
    "      pain           AS ed_triage_pain,\n",
    "      acuity         AS ed_triage_acuity,\n",
    "      chiefcomplaint AS ed_triage_cc\n",
    "    FROM `{PHYS}.{ED}.triage`\n",
    "    \"\"\"\n",
    "    tri_all = run_sql_bq(sql(\"ed_triage_all_sql\"))\n",
    "    tri_all[\"stay_id\"] = pd.to_numeric(tri_all[\"stay_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    tri_merged = (\n",
    "        edmap[[\"stay_id\", \"hadm_id\", \"intime\"]]\n",
    "        .merge(tri_all, on=\"stay_id\", how=\"left\")\n",
    "    )\n",
    "    ed_triage = (\n",
    "        tri_merged\n",
    "        .sort_values([\"hadm_id\", \"intime\"], na_position=\"last\")\n",
    "        .drop_duplicates(subset=[\"hadm_id\"], keep=\"first\")\n",
    "        .drop(columns=[\"stay_id\", \"intime\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    print(\"ED triage rows:\", len(ed_triage))\n",
    "\n",
    "    # 3) First vitals per stay across full ED vitals table, then reduce to earliest vitals-time per hadm.\n",
    "    SQL[\"ed_first_vitals_all_sql\"] = f\"\"\"\n",
    "    WITH vs_ranked AS (\n",
    "      SELECT\n",
    "        v.stay_id,\n",
    "        v.charttime,\n",
    "        v.temperature,\n",
    "        v.heartrate,\n",
    "        v.resprate,\n",
    "        v.o2sat,\n",
    "        v.sbp,\n",
    "        v.dbp,\n",
    "        v.rhythm,\n",
    "        v.pain,\n",
    "        ROW_NUMBER() OVER (PARTITION BY v.stay_id ORDER BY v.charttime) AS rn\n",
    "      FROM `{PHYS}.{ED}.vitalsign` v\n",
    "    )\n",
    "    SELECT\n",
    "      stay_id,\n",
    "      charttime    AS ed_first_vitals_time,\n",
    "      temperature  AS ed_first_temp,\n",
    "      heartrate    AS ed_first_hr,\n",
    "      resprate     AS ed_first_rr,\n",
    "      o2sat        AS ed_first_o2sat,\n",
    "      sbp          AS ed_first_sbp,\n",
    "      dbp          AS ed_first_dbp,\n",
    "      rhythm       AS ed_first_rhythm,\n",
    "      pain         AS ed_first_pain\n",
    "    FROM vs_ranked\n",
    "    WHERE rn = 1\n",
    "    \"\"\"\n",
    "    first_stay_all = run_sql_bq(sql(\"ed_first_vitals_all_sql\"))\n",
    "    first_stay_all[\"stay_id\"] = pd.to_numeric(first_stay_all[\"stay_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    first_stay_all[\"ed_first_vitals_time\"] = pd.to_datetime(first_stay_all[\"ed_first_vitals_time\"], errors=\"coerce\")\n",
    "\n",
    "    first_merged = (\n",
    "        edmap[[\"stay_id\", \"hadm_id\"]]\n",
    "        .merge(first_stay_all, on=\"stay_id\", how=\"left\")\n",
    "    )\n",
    "    ed_first = (\n",
    "        first_merged\n",
    "        .sort_values([\"hadm_id\", \"ed_first_vitals_time\"], na_position=\"last\")\n",
    "        .drop_duplicates(subset=[\"hadm_id\"], keep=\"first\")\n",
    "        .drop(columns=[\"stay_id\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    print(\"ED first vitals rows:\", len(ed_first))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097df55c",
   "metadata": {},
   "source": [
    "## 8) ICU meta (first ICU stay, LOS days)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a6f8d8",
   "metadata": {},
   "source": [
    "**Rationale:** Summarize ICU exposure and length of stay to contextualize disease severity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356b3163",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:28:07.459678Z",
     "iopub.status.busy": "2026-02-05T23:28:07.459486Z",
     "iopub.status.idle": "2026-02-05T23:28:12.271671Z",
     "shell.execute_reply": "2026-02-05T23:28:12.271065Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Build first-ICU metadata via full-table pull + local hadm filter to avoid large ARRAY parameter stalls.\n",
    "\n",
    "SQL[\"icu_all_sql\"] = f\"\"\"\n",
    "SELECT\n",
    "  hadm_id,\n",
    "  stay_id AS first_icu_stay_id,\n",
    "  intime  AS icu_intime,\n",
    "  outtime AS icu_outtime\n",
    "FROM `{PHYS}.{ICU}.icustays`\n",
    "WHERE hadm_id IS NOT NULL\n",
    "\"\"\"\n",
    "icu_all = run_sql_bq(sql(\"icu_all_sql\"))\n",
    "icu_all[\"hadm_id\"] = pd.to_numeric(icu_all[\"hadm_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "icu_all[\"first_icu_stay_id\"] = pd.to_numeric(icu_all[\"first_icu_stay_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "icu_all[\"icu_intime\"] = pd.to_datetime(icu_all[\"icu_intime\"], errors=\"coerce\")\n",
    "icu_all[\"icu_outtime\"] = pd.to_datetime(icu_all[\"icu_outtime\"], errors=\"coerce\")\n",
    "\n",
    "hadms_for_icu = set(pd.Series(hadm_list).dropna().astype(\"int64\").tolist())\n",
    "icu_all = icu_all[icu_all[\"hadm_id\"].isin(hadms_for_icu)].copy()\n",
    "\n",
    "icu_meta = (\n",
    "    icu_all\n",
    "    .sort_values([\"hadm_id\", \"icu_intime\", \"first_icu_stay_id\"], na_position=\"last\")\n",
    "    .drop_duplicates(subset=[\"hadm_id\"], keep=\"first\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "icu_meta[\"icu_los_days\"] = (\n",
    "    (icu_meta[\"icu_outtime\"] - icu_meta[\"icu_intime\"]).dt.total_seconds() / 86400.0\n",
    ")\n",
    "\n",
    "print(\"ICU meta rows:\", len(icu_meta))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04241211",
   "metadata": {},
   "source": [
    "## 9) Ventilation flags (ICD procedures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae8dc04",
   "metadata": {},
   "source": [
    "**Rationale:** Identify IMV/NIV exposure as clinically relevant respiratory support indicators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d411472",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:28:12.274724Z",
     "iopub.status.busy": "2026-02-05T23:28:12.274498Z",
     "iopub.status.idle": "2026-02-05T23:28:16.846522Z",
     "shell.execute_reply": "2026-02-05T23:28:16.845688Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Build IMV/NIV ICD flags from targeted procedure codes, then filter to cohort hadm_ids locally.\n",
    "\n",
    "SQL[\"vent_proc_sql\"] = f\"\"\"\n",
    "SELECT\n",
    "  hadm_id,\n",
    "  icd_version,\n",
    "  REPLACE(icd_code, '.', '') AS code_norm\n",
    "FROM `{PHYS}.{HOSP}.procedures_icd`\n",
    "WHERE\n",
    "  (icd_version = 10 AND icd_code IN ('5A1935Z','5A1945Z','5A1955Z','0BH17EZ','0BH18EZ','5A09357','5A09457','5A09557'))\n",
    "  OR\n",
    "  (icd_version = 9 AND REPLACE(icd_code, '.', '') IN ('9670','9671','9672','9604','9390','9391','9399'))\n",
    "\"\"\"\n",
    "\n",
    "vent_proc = run_sql_bq(sql(\"vent_proc_sql\"))\n",
    "vent_proc[\"hadm_id\"] = pd.to_numeric(vent_proc[\"hadm_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "vent_proc[\"icd_version\"] = pd.to_numeric(vent_proc[\"icd_version\"], errors=\"coerce\").astype(\"Int64\")\n",
    "vent_proc[\"code_norm\"] = vent_proc[\"code_norm\"].astype(str)\n",
    "\n",
    "hadms_for_vent = set(pd.Series(hadm_list).dropna().astype(\"int64\").tolist())\n",
    "vent_proc = vent_proc[vent_proc[\"hadm_id\"].isin(hadms_for_vent)].copy()\n",
    "\n",
    "imv_codes_10 = {\"5A1935Z\", \"5A1945Z\", \"5A1955Z\", \"0BH17EZ\", \"0BH18EZ\"}\n",
    "imv_codes_9 = {\"9670\", \"9671\", \"9672\", \"9604\"}\n",
    "niv_codes_10 = {\"5A09357\", \"5A09457\", \"5A09557\"}\n",
    "niv_codes_9 = {\"9390\", \"9391\", \"9399\"}\n",
    "\n",
    "vent_proc[\"imv_hit\"] = (\n",
    "    ((vent_proc[\"icd_version\"] == 10) & vent_proc[\"code_norm\"].isin(imv_codes_10)) |\n",
    "    ((vent_proc[\"icd_version\"] == 9) & vent_proc[\"code_norm\"].isin(imv_codes_9))\n",
    ")\n",
    "vent_proc[\"niv_hit\"] = (\n",
    "    ((vent_proc[\"icd_version\"] == 10) & vent_proc[\"code_norm\"].isin(niv_codes_10)) |\n",
    "    ((vent_proc[\"icd_version\"] == 9) & vent_proc[\"code_norm\"].isin(niv_codes_9))\n",
    ")\n",
    "\n",
    "vent = (\n",
    "    vent_proc\n",
    "    .groupby(\"hadm_id\", as_index=False)\n",
    "    .agg(\n",
    "        imv_flag=(\"imv_hit\", \"max\"),\n",
    "        niv_flag=(\"niv_hit\", \"max\"),\n",
    "    )\n",
    ")\n",
    "vent[\"imv_flag\"] = vent[\"imv_flag\"].astype(int)\n",
    "vent[\"niv_flag\"] = vent[\"niv_flag\"].astype(int)\n",
    "vent[\"any_vent_flag\"] = ((vent[\"imv_flag\"] == 1) | (vent[\"niv_flag\"] == 1)).astype(int)\n",
    "\n",
    "# Preserve one row per hadm in hadm_list even if no vent procedure codes were found.\n",
    "vent = pd.DataFrame({\"hadm_id\": pd.Series(hadm_list, dtype=\"Int64\")}).drop_duplicates().merge(vent, on=\"hadm_id\", how=\"left\")\n",
    "vent[[\"imv_flag\", \"niv_flag\", \"any_vent_flag\"]] = vent[[\"imv_flag\", \"niv_flag\", \"any_vent_flag\"]].fillna(0).astype(int)\n",
    "\n",
    "print(\"Vent rows:\", len(vent))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291cd724",
   "metadata": {},
   "source": [
    "and from charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9834f4e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:28:16.860574Z",
     "iopub.status.busy": "2026-02-05T23:28:16.860310Z",
     "iopub.status.idle": "2026-02-05T23:28:21.067975Z",
     "shell.execute_reply": "2026-02-05T23:28:21.067119Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Extract earliest NIV/IMV charttimes using prefiltered ventilation itemids for better performance.\n",
    "\n",
    "SQL[\"vent_chart_sql\"] = f\"\"\"\n",
    "WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x),\n",
    "\n",
    "stays AS (\n",
    "  SELECT DISTINCT hadm_id, stay_id\n",
    "  FROM `{PHYS}.{ICU}.icustays`\n",
    "  WHERE hadm_id IN (SELECT hadm_id FROM hadms)\n",
    "),\n",
    "\n",
    "vent_itemids AS (\n",
    "  SELECT itemid, LOWER(label) AS lbl\n",
    "  FROM `{PHYS}.{ICU}.d_items`\n",
    "  WHERE REGEXP_CONTAINS(LOWER(label), r'(vent|ventilator|mode|bipap|bi[- ]?pap|cpap|nippv|niv|mask|ett|endotracheal)')\n",
    "),\n",
    "\n",
    "cand AS (\n",
    "  SELECT\n",
    "    s.hadm_id,\n",
    "    ce.charttime,\n",
    "    vi.lbl,\n",
    "    LOWER(COALESCE(ce.value, '')) AS valstr\n",
    "  FROM `{PHYS}.{ICU}.chartevents` ce\n",
    "  JOIN stays s ON s.stay_id = ce.stay_id\n",
    "  JOIN vent_itemids vi ON vi.itemid = ce.itemid\n",
    "),\n",
    "\n",
    "flags AS (\n",
    "  SELECT\n",
    "    hadm_id,\n",
    "    MIN(IF(\n",
    "          REGEXP_CONTAINS(lbl, r'(non[- ]?invasive|niv|nippv|bipap|bi[- ]?pap|cpap)')\n",
    "          OR REGEXP_CONTAINS(valstr, r'(non[- ]?invasive|niv|nippv|bipap|bi[- ]?pap|cpap)'),\n",
    "          charttime, NULL)) AS first_niv_time,\n",
    "    MIN(IF(\n",
    "          REGEXP_CONTAINS(lbl, r'(invasive ventilation|endotracheal|ett|mech(|anical)? vent|ventilator mode|ac/|simv|prvc|aprv|pcv|vcv|assist\\s*control)')\n",
    "          OR REGEXP_CONTAINS(valstr, r'(invasive ventilation|endotracheal|ett|ac/|simv|prvc|aprv|pcv|vcv|assist\\s*control)'),\n",
    "          charttime, NULL)) AS first_imv_time,\n",
    "    MAX(CASE\n",
    "          WHEN REGEXP_CONTAINS(lbl, r'(non[- ]?invasive|niv|nippv|bipap|bi[- ]?pap|cpap)')\n",
    "            OR REGEXP_CONTAINS(valstr, r'(non[- ]?invasive|niv|nippv|bipap|bi[- ]?pap|cpap)')\n",
    "          THEN 1 ELSE 0 END) AS niv_chart_flag,\n",
    "    MAX(CASE\n",
    "          WHEN REGEXP_CONTAINS(lbl, r'(invasive ventilation|endotracheal|ett|mech(|anical)? vent|ventilator mode|ac/|simv|prvc|aprv|pcv|vcv|assist\\s*control)')\n",
    "            OR REGEXP_CONTAINS(valstr, r'(invasive ventilation|endotracheal|ett|ac/|simv|prvc|aprv|pcv|vcv|assist\\s*control)')\n",
    "          THEN 1 ELSE 0 END) AS imv_chart_flag\n",
    "  FROM cand\n",
    "  GROUP BY hadm_id\n",
    ")\n",
    "\n",
    "SELECT hadm_id, niv_chart_flag, imv_chart_flag, first_niv_time, first_imv_time\n",
    "FROM flags\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    vent_chart = run_sql_bq(sql(\"vent_chart_sql\"), {\"hadms\": hadm_list})\n",
    "except Exception as e:\n",
    "    print(\"WARNING: vent chart extraction failed; falling back to ICD-only vent timing.\", e)\n",
    "    vent_chart = pd.DataFrame(columns=[\"hadm_id\", \"niv_chart_flag\", \"imv_chart_flag\", \"first_niv_time\", \"first_imv_time\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c096d4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:28:21.072082Z",
     "iopub.status.busy": "2026-02-05T23:28:21.071597Z",
     "iopub.status.idle": "2026-02-05T23:28:21.135604Z",
     "shell.execute_reply": "2026-02-05T23:28:21.135295Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Derive respiratory support flags and timing fields for IMV/NIV exposure.\n",
    "\n",
    "# If your existing ICD-only result is called `vent`, rename for clarity:\n",
    "vent_proc = vent.copy()\n",
    "\n",
    "# Outer merge so we keep hadm_ids that appear in only one source\n",
    "vent_combined = vent_proc.merge(vent_chart, on=\"hadm_id\", how=\"outer\")\n",
    "\n",
    "# Fill missing with 0 before taking maxima\n",
    "for c in [\"imv_flag\",\"niv_flag\",\"any_vent_flag\",\"imv_chart_flag\",\"niv_chart_flag\"]:\n",
    "    if c in vent_combined.columns:\n",
    "        vent_combined[c] = vent_combined[c].fillna(0).astype(\"Int64\")\n",
    "\n",
    "# Final \"any-source\" flags\n",
    "vent_combined[\"imv_flag\"]       = vent_combined[[\"imv_flag\",\"imv_chart_flag\"]].max(axis=1).astype(\"Int64\")\n",
    "vent_combined[\"niv_flag\"]       = vent_combined[[\"niv_flag\",\"niv_chart_flag\"]].max(axis=1).astype(\"Int64\")\n",
    "vent_combined[\"any_vent_flag\"]  = vent_combined[[\"imv_flag\",\"niv_flag\"]].max(axis=1).astype(\"Int64\")\n",
    "\n",
    "vent_combined = vent_combined[[\"hadm_id\",\"imv_flag\",\"niv_flag\",\"any_vent_flag\",\"first_imv_time\",\"first_niv_time\"]]\n",
    "print(\"After combining ICD + chart signals:\",\n",
    "      \"\\nIMV=1:\", int((vent_combined[\"imv_flag\"]==1).sum()),\n",
    "      \"\\nNIV=1:\", int((vent_combined[\"niv_flag\"]==1).sum()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f81ddd8",
   "metadata": {},
   "source": [
    "## 10) Assemble final DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af830bbe",
   "metadata": {},
   "source": [
    "**Rationale:** Merge all derived features into a single analytic table keyed by hadm_id.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43442b5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:28:21.137245Z",
     "iopub.status.busy": "2026-02-05T23:28:21.137138Z",
     "iopub.status.idle": "2026-02-05T23:28:21.388908Z",
     "shell.execute_reply": "2026-02-05T23:28:21.388627Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Extract and standardize first ABG/VBG physiology fields for baseline characterization.\n",
    "\n",
    "# Canonical base (carries authoritative subject_id)\n",
    "df = demo.copy()\n",
    "\n",
    "# Cohort flags / thresholds / labs / etc.\n",
    "df = safe_merge_on_hadm(df, cohort_any, right_name=\"cohort_any\", check_subject=\"warn\")\n",
    "df = safe_merge_on_hadm(df, bg_pairs,   right_name=\"bg_pairs\")\n",
    "df = safe_merge_on_hadm(df, race_eth,   right_name=\"race_eth\")\n",
    "df = safe_merge_on_hadm(df, ed_triage,  right_name=\"ed_triage\")\n",
    "df = safe_merge_on_hadm(df, ed_first,   right_name=\"ed_first\")\n",
    "df = safe_merge_on_hadm(df, icu_meta,   right_name=\"icu_meta\")\n",
    "df = safe_merge_on_hadm(df, vent_combined,       right_name=\"vent_combined\")\n",
    "\n",
    "# Anchor to first ED presentation (per admission)\n",
    "if \"ed_intime_first\" in globals():\n",
    "    df = safe_merge_on_hadm(df, ed_intime_first, right_name=\"ed_intime_first\")\n",
    "\n",
    "# Derived timing: first NIV/IMV relative to ED presentation\n",
    "if \"ed_intime_first\" in df.columns and \"first_imv_time\" in df.columns:\n",
    "    df[\"dt_first_imv_hours\"] = (df[\"first_imv_time\"] - df[\"ed_intime_first\"]).dt.total_seconds() / 3600.0\n",
    "if \"ed_intime_first\" in df.columns and \"first_niv_time\" in df.columns:\n",
    "    df[\"dt_first_niv_hours\"] = (df[\"first_niv_time\"] - df[\"ed_intime_first\"]).dt.total_seconds() / 3600.0\n",
    "\n",
    "# ABG/VBG before IMV (hadm-level)\n",
    "if {\"first_abg_time\", \"first_imv_time\"}.issubset(df.columns):\n",
    "    df[\"abg_before_imv\"] = (\n",
    "        df[\"first_abg_time\"].notna() & df[\"first_imv_time\"].notna() &\n",
    "        (df[\"first_abg_time\"] < df[\"first_imv_time\"])\n",
    "    ).astype(\"Int64\")\n",
    "if {\"first_vbg_time\", \"first_imv_time\"}.issubset(df.columns):\n",
    "    df[\"vbg_before_imv\"] = (\n",
    "        df[\"first_vbg_time\"].notna() & df[\"first_imv_time\"].notna() &\n",
    "        (df[\"first_vbg_time\"] < df[\"first_imv_time\"])\n",
    "    ).astype(\"Int64\")\n",
    "\n",
    "# Canonical hadm-level threshold normalization.\n",
    "# This consolidates explicit threshold columns with parsed ABG/VBG/other pCO2 fields\n",
    "# so later merges cannot silently zero-out source-specific thresholds.\n",
    "def _num_series(frame: pd.DataFrame, col: str) -> pd.Series:\n",
    "    if col in frame.columns:\n",
    "        return pd.to_numeric(frame[col], errors=\"coerce\")\n",
    "    return pd.Series(np.nan, index=frame.index)\n",
    "\n",
    "\n",
    "def _any_ge(frame: pd.DataFrame, cols: list[str], threshold: float) -> pd.Series:\n",
    "    avail = [c for c in cols if c in frame.columns]\n",
    "    if not avail:\n",
    "        return pd.Series(0, index=frame.index, dtype=\"Int64\")\n",
    "    mat = pd.concat([_num_series(frame, c) for c in avail], axis=1)\n",
    "    return mat.ge(threshold).any(axis=1).astype(\"Int64\")\n",
    "\n",
    "\n",
    "existing_abg = _num_series(df, \"abg_hypercap_threshold\").fillna(0).astype(int)\n",
    "existing_vbg = _num_series(df, \"vbg_hypercap_threshold\").fillna(0).astype(int)\n",
    "existing_other = _num_series(df, \"other_hypercap_threshold\").fillna(0).astype(int)\n",
    "\n",
    "abg_from_values = _any_ge(df, [\"lab_abg_paco2\", \"poc_abg_paco2\", \"first_abg_pco2\"], 45.0)\n",
    "vbg_from_values = _any_ge(df, [\"lab_vbg_paco2\", \"poc_vbg_paco2\", \"first_vbg_pco2\"], 50.0)\n",
    "other_from_values = _any_ge(df, [\"first_other_pco2\"], 50.0)\n",
    "\n",
    "df[\"abg_hypercap_threshold\"] = ((existing_abg == 1) | (abg_from_values == 1)).astype(\"Int64\")\n",
    "df[\"vbg_hypercap_threshold\"] = ((existing_vbg == 1) | (vbg_from_values == 1)).astype(\"Int64\")\n",
    "df[\"other_hypercap_threshold\"] = ((existing_other == 1) | (other_from_values == 1)).astype(\"Int64\")\n",
    "\n",
    "df[\"pco2_threshold_any\"] = (\n",
    "    (df[\"abg_hypercap_threshold\"] == 1)\n",
    "    | (df[\"vbg_hypercap_threshold\"] == 1)\n",
    "    | (df[\"other_hypercap_threshold\"] == 1)\n",
    ").astype(\"Int64\")\n",
    "\n",
    "if \"any_hypercap_icd\" in df.columns:\n",
    "    df[\"enrolled_any\"] = ((pd.to_numeric(df[\"any_hypercap_icd\"], errors=\"coerce\").fillna(0).astype(int) == 1) | (df[\"pco2_threshold_any\"] == 1)).astype(\"Int64\")\n",
    "\n",
    "print(\n",
    "    \"Threshold sanity (hadm-level):\",\n",
    "    \"ABG=\", int(df[\"abg_hypercap_threshold\"].fillna(0).sum()),\n",
    "    \"VBG=\", int(df[\"vbg_hypercap_threshold\"].fillna(0).sum()),\n",
    "    \"OTHER=\", int(df[\"other_hypercap_threshold\"].fillna(0).sum()),\n",
    "    \"ANY=\", int(df[\"pco2_threshold_any\"].fillna(0).sum()),\n",
    ")\n",
    "\n",
    "print(\"Final df rows:\", len(df), \"cols:\", len(df.columns))\n",
    "\n",
    "# Safety checks\n",
    "assert \"subject_id\" in df.columns, \"subject_id missing from final df\"\n",
    "assert not any(c.endswith(\"_x\") or c.endswith(\"_y\") for c in df.columns), \"Found suffixed columns\"\n",
    "print(\"Final df rows:\", len(df), \"cols:\", len(df.columns))\n",
    "df.head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806f96a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:28:21.390537Z",
     "iopub.status.busy": "2026-02-05T23:28:21.390426Z",
     "iopub.status.idle": "2026-02-05T23:28:25.562999Z",
     "shell.execute_reply": "2026-02-05T23:28:25.562504Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.\n",
    "\n",
    "# --- Cohort flow counts (ED / ICU / blood gas / hypercapnia / CC) ---\n",
    "\n",
    "# 1) Dataset-level ED counts\n",
    "SQL[\"ed_counts_sql\"] = f\"\"\"\n",
    "SELECT\n",
    "  COUNT(*) AS total_ed_encounters,\n",
    "  COUNTIF(hadm_id IS NOT NULL) AS ed_encounters_with_hadm\n",
    "FROM `{PHYS}.{ED}.edstays`\n",
    "\"\"\"\n",
    "\n",
    "SQL[\"ed_to_icu_sql\"] = f\"\"\"\n",
    "SELECT\n",
    "  COUNT(DISTINCT e.hadm_id) AS ed_to_icu_hadm,\n",
    "  COUNT(DISTINCT e.stay_id) AS ed_to_icu_edstays\n",
    "FROM `{PHYS}.{ED}.edstays` e\n",
    "JOIN `{PHYS}.{ICU}.icustays` i USING (hadm_id)\n",
    "WHERE e.hadm_id IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    ed_counts = run_sql_bq(sql(\"ed_counts_sql\"))\n",
    "    ed_to_icu = run_sql_bq(sql(\"ed_to_icu_sql\"))\n",
    "except Exception as e:\n",
    "    print(\"Warning: ED/ICU counts query failed:\", e)\n",
    "    ed_counts = None\n",
    "    ed_to_icu = None\n",
    "\n",
    "# 2) Cohort-level counts (admission-level)\n",
    "cohort_union = int((cohort_any[\"enrolled_any\"] == 1).sum()) if \"cohort_any\" in globals() and \"enrolled_any\" in cohort_any.columns else len(hadm_list)\n",
    "cohort_df_n = len(df)\n",
    "\n",
    "# Any blood gas present (ABG/VBG, LAB/POC)\n",
    "co2_cols = [c for c in [\n",
    "    \"lab_abg_paco2\", \"lab_vbg_paco2\", \"poc_abg_paco2\", \"poc_vbg_paco2\"\n",
    "] if c in df.columns]\n",
    "any_bg = int(df[co2_cols].notna().any(axis=1).sum()) if co2_cols else None\n",
    "\n",
    "# Hypercapnia thresholds and ICD\n",
    "icd_count = int((df[\"any_hypercap_icd\"] == 1).sum()) if \"any_hypercap_icd\" in df.columns else None\n",
    "threshold_count = int((df[\"pco2_threshold_any\"] == 1).sum()) if \"pco2_threshold_any\" in df.columns else None\n",
    "\n",
    "# ED chief complaint missing / present (within cohort)\n",
    "if \"ed_triage_cc\" in df.columns:\n",
    "    mask_cc_present = df[\"ed_triage_cc\"].notna() & (df[\"ed_triage_cc\"].astype(str).str.strip() != \"\")\n",
    "    cc_present = int(mask_cc_present.sum())\n",
    "    cc_missing = int((~mask_cc_present).sum())\n",
    "else:\n",
    "    cc_present = None\n",
    "    cc_missing = None\n",
    "\n",
    "# ED→ICU within cohort (admissions with ED triage data and ICU stay)\n",
    "if \"first_icu_stay_id\" in df.columns and \"ed_triage_cc\" in df.columns:\n",
    "    cohort_ed_to_icu = int((df[\"first_icu_stay_id\"].notna() & mask_cc_present).sum())\n",
    "else:\n",
    "    cohort_ed_to_icu = None\n",
    "\n",
    "rows = []\n",
    "if ed_counts is not None:\n",
    "    rows.append({\"step\": \"Total ED encounters (edstays)\", \"count\": int(ed_counts.loc[0, \"total_ed_encounters\"]), \"scope\": \"All ED dataset\"})\n",
    "    rows.append({\"step\": \"ED encounters with hadm_id\", \"count\": int(ed_counts.loc[0, \"ed_encounters_with_hadm\"]), \"scope\": \"All ED dataset\"})\n",
    "if ed_to_icu is not None:\n",
    "    rows.append({\"step\": \"ED→ICU admissions (distinct hadm_id)\", \"count\": int(ed_to_icu.loc[0, \"ed_to_icu_hadm\"]), \"scope\": \"All ED+ICU\"})\n",
    "    rows.append({\"step\": \"ED→ICU ED-stays (distinct stay_id)\", \"count\": int(ed_to_icu.loc[0, \"ed_to_icu_edstays\"]), \"scope\": \"All ED+ICU\"})\n",
    "\n",
    "rows.append({\"step\": \"Cohort admissions (union ICD ∪ thresholds)\", \"count\": cohort_union, \"scope\": \"Cohort\"})\n",
    "rows.append({\"step\": \"Cohort admissions after merges (df rows)\", \"count\": cohort_df_n, \"scope\": \"Cohort\"})\n",
    "if any_bg is not None:\n",
    "    rows.append({\"step\": \"Cohort with any ABG/VBG (LAB or POC)\", \"count\": any_bg, \"scope\": \"Cohort\"})\n",
    "if threshold_count is not None:\n",
    "    rows.append({\"step\": \"Cohort meeting hypercapnia thresholds\", \"count\": threshold_count, \"scope\": \"Cohort\"})\n",
    "if icd_count is not None:\n",
    "    rows.append({\"step\": \"Cohort meeting ICD code criteria\", \"count\": icd_count, \"scope\": \"Cohort\"})\n",
    "if cc_present is not None:\n",
    "    rows.append({\"step\": \"Cohort with ED chief complaint present\", \"count\": cc_present, \"scope\": \"Cohort\"})\n",
    "if cc_missing is not None:\n",
    "    rows.append({\"step\": \"Cohort excluded for missing ED chief complaint\", \"count\": cc_missing, \"scope\": \"Cohort\"})\n",
    "if cohort_ed_to_icu is not None:\n",
    "    rows.append({\"step\": \"Cohort ED→ICU (ED CC present + ICU stay)\", \"count\": cohort_ed_to_icu, \"scope\": \"Cohort\"})\n",
    "\n",
    "flow_counts = pd.DataFrame(rows)\n",
    "flow_counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f3d240",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:28:25.571607Z",
     "iopub.status.busy": "2026-02-05T23:28:25.571337Z",
     "iopub.status.idle": "2026-02-05T23:28:25.584250Z",
     "shell.execute_reply": "2026-02-05T23:28:25.583956Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Quantify overlap between ascertainment routes so non-exclusive cohorts are explicit.\n",
    "\n",
    "# --- Ascertainment overlap counts (ABG/VBG/ICD) ---\n",
    "\n",
    "required = [\"abg_hypercap_threshold\", \"vbg_hypercap_threshold\", \"other_hypercap_threshold\", \"any_hypercap_icd\"]\n",
    "missing = [c for c in required if c not in df.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing required columns for overlap counts: {missing}\")\n",
    "\n",
    "abg = pd.to_numeric(df[\"abg_hypercap_threshold\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "vbg = pd.to_numeric(df[\"vbg_hypercap_threshold\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "other = pd.to_numeric(df[\"other_hypercap_threshold\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "gas_any = (\n",
    "    pd.to_numeric(df.get(\"pco2_threshold_any\", None), errors=\"coerce\")\n",
    "    if \"pco2_threshold_any\" in df.columns else (abg | vbg | other)\n",
    ")\n",
    "if hasattr(gas_any, \"fillna\"):\n",
    "    gas_any = gas_any.fillna(0).astype(int)\n",
    "else:\n",
    "    gas_any = gas_any.astype(int)\n",
    "\n",
    "icd = pd.to_numeric(df[\"any_hypercap_icd\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "total_n = len(df)\n",
    "ngas = int((gas_any == 1).sum())\n",
    "\n",
    "abg_vbg_overlap = pd.DataFrame([\n",
    "    {\"group\": \"ABG-only\", \"count\": int(((abg==1) & (vbg==0) & (other==0)).sum())},\n",
    "    {\"group\": \"VBG-only\", \"count\": int(((vbg==1) & (abg==0) & (other==0)).sum())},\n",
    "    {\"group\": \"Other-only\", \"count\": int(((other==1) & (abg==0) & (vbg==0)).sum())},\n",
    "    {\"group\": \"Mixed (>=2 routes)\", \"count\": int((((abg + vbg + other) >= 2).sum()))},\n",
    "])\n",
    "if ngas > 0:\n",
    "    abg_vbg_overlap[\"pct_of_gas\"] = (abg_vbg_overlap[\"count\"] / ngas * 100).round(1)\n",
    "else:\n",
    "    abg_vbg_overlap[\"pct_of_gas\"] = 0.0\n",
    "abg_vbg_overlap[\"pct_of_cohort\"] = (abg_vbg_overlap[\"count\"] / max(total_n,1) * 100).round(1)\n",
    "\n",
    "icd_gas_overlap = pd.DataFrame([\n",
    "    {\"group\": \"ICD+Gas\", \"count\": int(((icd==1) & (gas_any==1)).sum())},\n",
    "    {\"group\": \"ICD-only\", \"count\": int(((icd==1) & (gas_any==0)).sum())},\n",
    "    {\"group\": \"Gas-only\", \"count\": int(((icd==0) & (gas_any==1)).sum())},\n",
    "    {\"group\": \"Neither\", \"count\": int(((icd==0) & (gas_any==0)).sum())},\n",
    "])\n",
    "icd_gas_overlap[\"pct_of_cohort\"] = (icd_gas_overlap[\"count\"] / max(total_n,1) * 100).round(1)\n",
    "\n",
    "print(\"ABG/VBG/Other overlap (among gas-positive):\")\n",
    "print(abg_vbg_overlap.to_string(index=False))\n",
    "print(\"ICD vs Gas overlap (cohort-level):\")\n",
    "print(icd_gas_overlap.to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb306488",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:28:25.586576Z",
     "iopub.status.busy": "2026-02-05T23:28:25.586427Z",
     "iopub.status.idle": "2026-02-05T23:28:25.675546Z",
     "shell.execute_reply": "2026-02-05T23:28:25.675256Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Capture ED presentation context (chief complaint, acuity, and early vitals) at the ED-stay level.\n",
    "\n",
    "# --- Missingness summary (chief complaint, race/ethnicity, ED triage/vitals) ---\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "# Chief complaint missingness (ED triage CC)\n",
    "if \"ed_triage_cc\" in df.columns:\n",
    "    cc_present = df[\"ed_triage_cc\"].notna() & (df[\"ed_triage_cc\"].astype(str).str.strip() != \"\")\n",
    "    summary_rows.append({\n",
    "        \"variable\": \"ed_triage_cc_present\",\n",
    "        \"missing_n\": int((~cc_present).sum()),\n",
    "        \"missing_pct\": float((~cc_present).mean())\n",
    "    })\n",
    "\n",
    "# Race/Ethnicity missingness (NIH categories + raw sources)\n",
    "unknown_tokens = {\n",
    "    \"unknown or not reported\",\n",
    "    \"unknown\",\n",
    "    \"not reported\",\n",
    "    \"missing\",\n",
    "    \"declined\",\n",
    "    \"unable\"\n",
    "}\n",
    "\n",
    "def _missing_rate(series):\n",
    "    if series is None:\n",
    "        return None, None\n",
    "    s = series.astype(str).str.strip()\n",
    "    is_missing = series.isna() | (s == \"\") | s.str.lower().isin(unknown_tokens)\n",
    "    return int(is_missing.sum()), float(is_missing.mean())\n",
    "\n",
    "for col in [\"nih_race\", \"nih_ethnicity\", \"race_hosp_raw\", \"race_ed_raw\"]:\n",
    "    if col in df.columns:\n",
    "        m_n, m_p = _missing_rate(df[col])\n",
    "        summary_rows.append({\n",
    "            \"variable\": col,\n",
    "            \"missing_n\": m_n,\n",
    "            \"missing_pct\": m_p\n",
    "        })\n",
    "\n",
    "missing_summary = pd.DataFrame(summary_rows)\n",
    "print(\"Missingness summary (key variables):\")\n",
    "missing_summary\n",
    "\n",
    "# ED triage + first ED vitals missingness\n",
    "triage_cols = [c for c in df.columns if c.startswith(\"ed_triage_\")]\n",
    "first_cols  = [c for c in df.columns if c.startswith(\"ed_first_\")]\n",
    "\n",
    "vital_cols = triage_cols + first_cols\n",
    "if vital_cols:\n",
    "    miss_tbl = (\n",
    "        pd.DataFrame({\"variable\": vital_cols})\n",
    "        .assign(\n",
    "            missing_n=lambda d: [int(df[c].isna().sum()) for c in d[\"variable\"]],\n",
    "            missing_pct=lambda d: [float(df[c].isna().mean()) for c in d[\"variable\"]]\n",
    "        )\n",
    "        .sort_values(\"missing_pct\", ascending=False)\n",
    "    )\n",
    "    print(\"Missingness summary (ED triage + first ED vitals):\")\n",
    "    miss_tbl\n",
    "else:\n",
    "    miss_tbl = pd.DataFrame(columns=[\"variable\", \"missing_n\", \"missing_pct\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470e172e",
   "metadata": {},
   "source": [
    "## 11) Sanity checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0761e99a",
   "metadata": {},
   "source": [
    "**Rationale:** Run QC checks to validate units, flags, and basic data integrity before export.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f00e72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:28:25.678325Z",
     "iopub.status.busy": "2026-02-05T23:28:25.678230Z",
     "iopub.status.idle": "2026-02-05T23:28:25.707266Z",
     "shell.execute_reply": "2026-02-05T23:28:25.706979Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Derive respiratory support flags and timing fields for IMV/NIV exposure.\n",
    "\n",
    "\n",
    "# ICU LOS negative?\n",
    "if {\"icu_los_days\",\"first_icu_stay_id\"}.issubset(df.columns):\n",
    "    neg_los = int((df[\"icu_los_days\"] < 0).fillna(False).sum())\n",
    "    print(\"Negative ICU LOS rows:\", neg_los)\n",
    "\n",
    "# Vent flags consistency\n",
    "vent_cols = {\"imv_flag\",\"niv_flag\",\"any_vent_flag\"}\n",
    "if vent_cols.issubset(df.columns):\n",
    "    any_calc = ((df[\"imv_flag\"]==1) | (df[\"niv_flag\"]==1)).fillna(False).astype(int)\n",
    "    any_flag = pd.to_numeric(df[\"any_vent_flag\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    mism = int((any_calc != any_flag).sum())\n",
    "    print(\"any_vent_flag mismatches vs (imv|niv):\", mism)\n",
    "\n",
    "# UOMs: expect mmhg only\n",
    "uom_cols = [c for c in df.columns if c.endswith(\"_paco2_uom\")]\n",
    "for c in uom_cols:\n",
    "    vals = sorted(pd.Series(df[c]).dropna().astype(str).str.lower().str.strip().unique().tolist())\n",
    "    print(c, vals)\n",
    "\n",
    "# ABG/VBG coverage QC\n",
    "def qc_pair(df, ph_col, co2_col, label, ph_lo=6.3, ph_hi=7.8, co2_lo=5, co2_hi=200):\n",
    "    ph  = pd.to_numeric(df.get(ph_col), errors=\"coerce\")\n",
    "    co2 = pd.to_numeric(df.get(co2_col), errors=\"coerce\")\n",
    "    return {\n",
    "        \"pair\": label,\n",
    "        \"present_any\":  int(((ph.notna()) | (co2.notna())).sum()),\n",
    "        \"present_both\": int(((ph.notna()) & (co2.notna())).sum()),\n",
    "        \"only_ph\":      int(((ph.notna()) & (~co2.notna())).sum()),\n",
    "        \"only_pco2\":    int(((co2.notna()) & (~ph.notna())).sum()),\n",
    "        \"ph_oob\":       int((((ph  < ph_lo)  | (ph  > ph_hi))  & ph.notna()).sum()),\n",
    "        \"pco2_oob\":     int((((co2 < co2_lo) | (co2 > co2_hi)) & co2.notna()).sum()),\n",
    "    }\n",
    "\n",
    "qc = pd.DataFrame([\n",
    "    qc_pair(df, \"lab_abg_ph\",\"lab_abg_paco2\",\"LAB ABG\"),\n",
    "    qc_pair(df, \"lab_vbg_ph\",\"lab_vbg_paco2\",\"LAB VBG\"),\n",
    "    qc_pair(df, \"poc_abg_ph\",\"poc_abg_paco2\",\"POC ABG\"),\n",
    "    qc_pair(df, \"poc_vbg_ph\",\"poc_vbg_paco2\",\"POC VBG\"),\n",
    "])\n",
    "qc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9634bf",
   "metadata": {},
   "source": [
    "## 12) Save to Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e5f147",
   "metadata": {},
   "source": [
    "**Rationale:** Persist cohort outputs for downstream annotation and NLP analyses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd24b5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:28:25.708595Z",
     "iopub.status.busy": "2026-02-05T23:28:25.708520Z",
     "iopub.status.idle": "2026-02-05T23:28:58.919172Z",
     "shell.execute_reply": "2026-02-05T23:28:58.918765Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Optional archive export of the full hadm-level cohort workbook.\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "WRITE_ARCHIVE_XLSX_EXPORTS = os.getenv(\"WRITE_ARCHIVE_XLSX_EXPORTS\", \"0\") == \"1\"\n",
    "\n",
    "if WRITE_ARCHIVE_XLSX_EXPORTS:\n",
    "    prior_runs_dir = DATA_DIR / \"prior runs\"\n",
    "    prior_runs_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = prior_runs_dir / f\"mimic_hypercap_EXT_bq_abg_vbg_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx\"\n",
    "    with pd.ExcelWriter(out_path, engine=\"openpyxl\") as xw:\n",
    "        df.to_excel(xw, sheet_name=\"cohort\", index=False)\n",
    "        try:\n",
    "            qc.to_excel(xw, sheet_name=\"qc_abg_vbg\", index=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "    out_path\n",
    "else:\n",
    "    print(\"Skipping archive hadm-level workbook export (set WRITE_ARCHIVE_XLSX_EXPORTS=1 to enable).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf28e47c",
   "metadata": {},
   "source": [
    "## Create Annotation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a607a4",
   "metadata": {},
   "source": [
    "**Rationale:** Create ED chief-complaint subsets and a reproducible sample for manual annotation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd4b4ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:28:58.922440Z",
     "iopub.status.busy": "2026-02-05T23:28:58.922292Z",
     "iopub.status.idle": "2026-02-05T23:29:12.312008Z",
     "shell.execute_reply": "2026-02-05T23:29:12.311643Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Capture ED presentation context (chief complaint, acuity, and early vitals) at the ED-stay level.\n",
    "\n",
    "# ---- Extra exports: (1) ED chief-complaint only; (2) random sample of 160 patients ----\n",
    "from datetime import datetime\n",
    "\n",
    "# Dated artifacts go to archive folder (optional)\n",
    "prior_runs_dir = DATA_DIR / \"prior runs\"\n",
    "prior_runs_dir.mkdir(parents=True, exist_ok=True)\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "WRITE_ARCHIVE_XLSX_EXPORTS = os.getenv(\"WRITE_ARCHIVE_XLSX_EXPORTS\", \"0\") == \"1\"\n",
    "\n",
    "# 1) Filter to rows with a non-empty ED chief complaint\n",
    "if \"ed_triage_cc\" not in df.columns:\n",
    "    raise KeyError(\n",
    "        \"Column 'ed_triage_cc' not found in df. \"\n",
    "        \"Ensure the ED triage merge cell ran earlier.\"\n",
    "    )\n",
    "\n",
    "mask_cc = df[\"ed_triage_cc\"].notna() & (df[\"ed_triage_cc\"].astype(str).str.strip() != \"\")\n",
    "df_cc = df.loc[mask_cc].copy()\n",
    "\n",
    "print(f\"ED-CC present rows: {len(df_cc)} of {len(df)} \"\n",
    "      f\"({(len(df_cc) / max(len(df),1)):.1%} of cohort).\")\n",
    "\n",
    "# Save ED-CC-only cohort only when archive exports are enabled\n",
    "if WRITE_ARCHIVE_XLSX_EXPORTS:\n",
    "    out_path_cc = prior_runs_dir / f\"mimic_hypercap_EXT_EDcc_only_bq_abg_vbg_{timestamp}.xlsx\"\n",
    "    with pd.ExcelWriter(out_path_cc, engine=\"openpyxl\") as xw:\n",
    "        df_cc.to_excel(xw, sheet_name=\"cohort_cc_only\", index=False)\n",
    "        try:\n",
    "            qc.to_excel(xw, sheet_name=\"qc_abg_vbg\", index=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "    print(\"Saved:\", out_path_cc)\n",
    "\n",
    "# 2) Random sample of n = 160 patients (distinct subject_id), one row per patient\n",
    "if \"subject_id\" not in df_cc.columns:\n",
    "    raise KeyError(\"Column 'subject_id' missing; cannot sample by patient.\")\n",
    "\n",
    "# Make a one-row-per-patient frame by earliest admission\n",
    "if \"admittime\" in df_cc.columns:\n",
    "    df_cc_one = (\n",
    "        df_cc.sort_values([\"subject_id\", \"admittime\"])\n",
    "             .groupby(\"subject_id\", as_index=False)\n",
    "             .head(1)\n",
    "    )\n",
    "else:\n",
    "    # Fallback if admittime not present: choose the smallest hadm_id per patient\n",
    "    df_cc_one = (\n",
    "        df_cc.sort_values([\"subject_id\", \"hadm_id\"])\n",
    "             .groupby(\"subject_id\", as_index=False)\n",
    "             .head(1)\n",
    "    )\n",
    "\n",
    "N = 160\n",
    "n_avail = len(df_cc_one)\n",
    "n_take = min(N, n_avail)\n",
    "if n_avail < N:\n",
    "    print(f\"Warning: only {n_avail} unique patients with ED chief complaint; sampling all of them.\")\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "df_cc_sample = df_cc_one.sample(n=n_take, random_state=RANDOM_SEED)\n",
    "\n",
    "# Save the sample only when archive exports are enabled\n",
    "if WRITE_ARCHIVE_XLSX_EXPORTS:\n",
    "    out_path_cc_sample = prior_runs_dir / f\"mimic_hypercap_EXT_EDcc_sample{n_take}_bq_abg_vbg_{timestamp}.xlsx\"\n",
    "    with pd.ExcelWriter(out_path_cc_sample, engine=\"openpyxl\") as xw:\n",
    "        df_cc_sample.to_excel(xw, sheet_name=\"cohort_cc_sample\", index=False)\n",
    "        try:\n",
    "            qc.to_excel(xw, sheet_name=\"qc_abg_vbg\", index=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "    print(\"Saved:\", out_path_cc_sample)\n",
    "else:\n",
    "    print(\"Skipping ED-CC archive exports (set WRITE_ARCHIVE_XLSX_EXPORTS=1 to enable).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e098c6",
   "metadata": {},
   "source": [
    "# ED-stay cohort expansion (timing, severity, comorbidity, outcomes)\n",
    "\n",
    "**Rationale:** Build a one-row-per-ED-stay analytic extract with time-anchored gas phenotypes, key comorbidities, and outcomes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496b296e",
   "metadata": {},
   "source": [
    "## Phase 0 — Inventory & missing-field registry\n",
    "\n",
    "**Rationale:** Detect which fields already exist and only add missing fields to avoid redundant extraction or join explosions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50acefa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:29:12.317678Z",
     "iopub.status.busy": "2026-02-05T23:29:12.317544Z",
     "iopub.status.idle": "2026-02-05T23:29:12.325914Z",
     "shell.execute_reply": "2026-02-05T23:29:12.325591Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Inventory available fields and identify missing target variables before enrichment steps.\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Identify current cohort dataframe (admission-level)\n",
    "if 'df' not in globals():\n",
    "    raise NameError(\"Expected admission-level df to exist before inventory step.\")\n",
    "\n",
    "ED_KEY = \"ed_stay_id\"  # target key for ED-level cohort\n",
    "\n",
    "print(\"Current admission-level df:\", df.shape)\n",
    "print(\"Current columns count:\", len(df.columns))\n",
    "if ED_KEY in df.columns:\n",
    "    print(\"ED stay unique count:\", int(df[ED_KEY].nunique()))\n",
    "else:\n",
    "    print(\"ED stay unique count: ED_KEY not in columns\")\n",
    "\n",
    "# Persist columns snapshot\n",
    "cols_out = WORK_DIR / \"current_columns.json\"\n",
    "cols_out.write_text(json.dumps(sorted(df.columns), indent=2))\n",
    "print(\"Wrote:\", cols_out)\n",
    "\n",
    "\n",
    "\n",
    "# Persist ED-stay columns snapshot (if ed_df exists)\n",
    "if \"ed_df\" in globals():\n",
    "    ed_cols_out = WORK_DIR / \"ed_columns.json\"\n",
    "    ed_cols_out.write_text(json.dumps(sorted(ed_df.columns), indent=2))\n",
    "    print(\"Wrote:\", ed_cols_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fff412f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:29:12.327012Z",
     "iopub.status.busy": "2026-02-05T23:29:12.326932Z",
     "iopub.status.idle": "2026-02-05T23:29:12.331191Z",
     "shell.execute_reply": "2026-02-05T23:29:12.330976Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Capture ED presentation context (chief complaint, acuity, and early vitals) at the ED-stay level.\n",
    "\n",
    "# Target field registry\n",
    "TARGET_RAW_FIELDS = {\n",
    "    \"ed_edstays\": [\n",
    "        \"ed_stay_id\",\"subject_id\",\"hadm_id\",\"ed_intime\",\"ed_outtime\",\"ed_intime_first\",\n",
    "        \"arrival_transport\",\"disposition\",\"ed_gender\",\"ed_race\",\n",
    "    ],\n",
    "    \"ed_triage\": [\n",
    "        \"ed_triage_temp\",\"ed_triage_hr\",\"ed_triage_rr\",\"ed_triage_o2sat\",\n",
    "        \"ed_triage_sbp\",\"ed_triage_dbp\",\"ed_triage_pain\",\"ed_triage_acuity\",\"ed_triage_cc\",\n",
    "    ],\n",
    "    \"ed_vitals_first\": [\n",
    "        \"ed_first_vitals_time\",\"ed_first_temp\",\"ed_first_hr\",\"ed_first_rr\",\n",
    "        \"ed_first_o2sat\",\"ed_first_sbp\",\"ed_first_dbp\",\"ed_first_rhythm\",\"ed_first_pain\",\n",
    "    ],\n",
    "    \"admissions\": [\n",
    "        \"admittime\",\"dischtime\",\"deathtime\",\"hospital_expire_flag\",\n",
    "        \"admission_type\",\"admission_location\",\"discharge_location\",\n",
    "        \"insurance\",\"language\",\"marital_status\",\"hosp_race\",\n",
    "    ],\n",
    "    \"icu\": [\n",
    "        \"icu_stay_id\",\"icu_intime_first\",\"icu_outtime_last\",\"icu_los_total\",\"n_icu_stays\",\n",
    "        \"first_careunit\",\"last_careunit\",\n",
    "    ],\n",
    "    \"labs_gas\": [\n",
    "        \"first_gas_time\",\"first_pco2\",\"first_ph\",\"first_hco3\",\"first_lactate\",\n",
    "        \"max_pco2_0_6h\",\"min_ph_0_6h\",\"max_pco2_0_24h\",\"min_ph_0_24h\",\n",
    "        \"flag_abg_hypercapnia\",\"flag_vbg_hypercapnia\",\"flag_other_hypercapnia\",\"flag_any_gas_hypercapnia\",\n",
    "        \"gas_source_other_rate\",\n",
    "        \"dt_first_imv_hours\",\"dt_first_niv_hours\",\"first_other_time\",\n",
    "    ],\n",
    "    \"omr\": [\n",
    "        \"bmi_closest_pre_ed\",\"height_closest_pre_ed\",\"weight_closest_pre_ed\",\n",
    "    ],\n",
    "    \"dx_flags\": [\n",
    "        \"flag_copd\",\"flag_osa_ohs\",\"flag_chf\",\"flag_neuromuscular\",\n",
    "        \"flag_opioid_substance\",\"flag_pneumonia\",\n",
    "    ],\n",
    "    \"timing\": [\n",
    "        \"dt_first_qualifying_gas_hours\",\"presenting_hypercapnia\",\"late_hypercapnia\",\n",
    "        \"dt_first_imv_hours\",\"dt_first_niv_hours\",\"abg_before_imv\",\"vbg_before_imv\",\n",
    "        \"ph_band\",\"hco3_band\",\"lactate_band\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "TARGET_DERIVED_FIELDS = [\n",
    "    \"hospital_los_hours\",\"in_hospital_death\",\n",
    "]\n",
    "\n",
    "TARGET_FIELDS = sorted({c for v in TARGET_RAW_FIELDS.values() for c in v} | set(TARGET_DERIVED_FIELDS))\n",
    "\n",
    "# Grouped missing report\n",
    "missing_by_group = {}\n",
    "for group, cols in TARGET_RAW_FIELDS.items():\n",
    "    missing_by_group[group] = [c for c in cols if c not in df.columns]\n",
    "\n",
    "missing_derived = [c for c in TARGET_DERIVED_FIELDS if c not in df.columns]\n",
    "missing_by_group[\"derived\"] = missing_derived\n",
    "\n",
    "missing_flat = [c for cols in missing_by_group.values() for c in cols]\n",
    "print(\"Missing fields total:\", len(missing_flat))\n",
    "for group, cols in missing_by_group.items():\n",
    "    if cols:\n",
    "        print(f\"- {group}: {cols}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6d4c97",
   "metadata": {},
   "source": [
    "## Phase 1 — ED encounter spine and ED enrichment (one row per ED stay)\n",
    "\n",
    "**Rationale:** Create a dedicated ED-stay-level cohort with ED-specific attributes to avoid mixing admission- and ED-level grains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b2122f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:29:12.332845Z",
     "iopub.status.busy": "2026-02-05T23:29:12.332770Z",
     "iopub.status.idle": "2026-02-05T23:29:17.176556Z",
     "shell.execute_reply": "2026-02-05T23:29:17.176076Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.\n",
    "\n",
    "# ED stay spine (rename stay_id to ed_stay_id)\n",
    "\n",
    "SQL[\"ed_spine_sql\"] = f\"\"\"\n",
    "WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x)\n",
    "SELECT\n",
    "  s.stay_id AS ed_stay_id,\n",
    "  s.subject_id,\n",
    "  s.hadm_id,\n",
    "  s.intime AS ed_intime,\n",
    "  s.outtime AS ed_outtime,\n",
    "  s.arrival_transport,\n",
    "  s.disposition,\n",
    "  s.gender AS ed_gender,\n",
    "  s.race   AS ed_race\n",
    "FROM `{PHYS}.{ED}.edstays` s\n",
    "JOIN hadms h ON h.hadm_id = s.hadm_id\n",
    "\"\"\"\n",
    "\n",
    "ed_spine = run_sql_bq(sql(\"ed_spine_sql\"), {\"hadms\": hadm_list})\n",
    "print(\"ED spine rows:\", len(ed_spine), \"unique ed_stay_id:\", ed_spine[\"ed_stay_id\"].nunique())\n",
    "\n",
    "# ensure uniqueness\n",
    "if ed_spine[\"ed_stay_id\"].nunique() != len(ed_spine):\n",
    "    raise ValueError(\"ed_stay_id not unique in ED spine\")\n",
    "\n",
    "# First ED presentation time per admission\n",
    "ed_intime_first = (\n",
    "    ed_spine.groupby(\"hadm_id\", as_index=False)[\"ed_intime\"]\n",
    "    .min()\n",
    "    .rename(columns={\"ed_intime\": \"ed_intime_first\"})\n",
    ")\n",
    "\n",
    "# Start ED-level df\n",
    "ed_df = ed_spine.copy()\n",
    "ed_df = ed_df.merge(ed_intime_first, on=\"hadm_id\", how=\"left\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8c7170",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:29:17.178746Z",
     "iopub.status.busy": "2026-02-05T23:29:17.178513Z",
     "iopub.status.idle": "2026-02-05T23:29:26.174995Z",
     "shell.execute_reply": "2026-02-05T23:29:26.174607Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.\n",
    "\n",
    "# ED triage and first ED vitals (reuse existing logic if present; otherwise join)\n",
    "\n",
    "def _needs_cols(df, cols):\n",
    "    return (df is None) or any(c not in df.columns for c in cols)\n",
    "\n",
    "# Use existing ed_triage / ed_first if already in memory from earlier cells\n",
    "try:\n",
    "    _ = ed_triage\n",
    "except NameError:\n",
    "    ed_triage = None\n",
    "\n",
    "try:\n",
    "    _ = ed_first\n",
    "except NameError:\n",
    "    ed_first = None\n",
    "\n",
    "# Force re-query if required keys are missing\n",
    "if _needs_cols(locals().get('ed_triage', None), ['ed_stay_id', 'hadm_id']):\n",
    "    ed_triage = None\n",
    "if _needs_cols(locals().get('ed_first', None), ['ed_stay_id']):\n",
    "    ed_first = None\n",
    "\n",
    "# If missing, prefer in-memory tables loaded earlier in the notebook.\n",
    "# This avoids re-running expensive ED queries during nbconvert.\n",
    "if ed_triage is None:\n",
    "    if 'edmap' in globals() and 'tri_all' in globals():\n",
    "        ed_triage = (\n",
    "            edmap[['stay_id', 'hadm_id']]\n",
    "            .merge(tri_all, on='stay_id', how='left')\n",
    "            .rename(columns={'stay_id': 'ed_stay_id'})\n",
    "        )\n",
    "        print('ED triage rows (from in-memory tables):', len(ed_triage))\n",
    "    else:\n",
    "        ed_triage_sql = f\"\"\"\n",
    "        WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x)\n",
    "        SELECT\n",
    "          s.stay_id AS ed_stay_id,\n",
    "          s.hadm_id,\n",
    "          t.temperature    AS ed_triage_temp,\n",
    "          t.heartrate      AS ed_triage_hr,\n",
    "          t.resprate       AS ed_triage_rr,\n",
    "          t.o2sat          AS ed_triage_o2sat,\n",
    "          t.sbp            AS ed_triage_sbp,\n",
    "          t.dbp            AS ed_triage_dbp,\n",
    "          t.pain           AS ed_triage_pain,\n",
    "          t.acuity         AS ed_triage_acuity,\n",
    "          t.chiefcomplaint AS ed_triage_cc\n",
    "        FROM `{PHYS}.{ED}.edstays` s\n",
    "        JOIN hadms h ON h.hadm_id = s.hadm_id\n",
    "        LEFT JOIN `{PHYS}.{ED}.triage` t ON t.stay_id = s.stay_id\n",
    "        \"\"\"\n",
    "        ed_triage = run_sql_bq(ed_triage_sql, {'hadms': hadm_list})\n",
    "        print('ED triage rows (fallback query):', len(ed_triage))\n",
    "\n",
    "if ed_first is None:\n",
    "    if 'edmap' in globals() and 'first_stay_all' in globals():\n",
    "        ed_first = (\n",
    "            edmap[['stay_id', 'hadm_id']]\n",
    "            .merge(first_stay_all, on='stay_id', how='left')\n",
    "            .rename(columns={'stay_id': 'ed_stay_id'})\n",
    "        )\n",
    "        print('ED first vitals rows (from in-memory tables):', len(ed_first))\n",
    "    else:\n",
    "        ed_first_vitals_sql = f\"\"\"\n",
    "        WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x),\n",
    "        edmap AS (\n",
    "          SELECT stay_id, hadm_id\n",
    "          FROM `{PHYS}.{ED}.edstays`\n",
    "          WHERE hadm_id IN (SELECT hadm_id FROM hadms)\n",
    "        ),\n",
    "        vs AS (\n",
    "          SELECT stay_id, charttime, temperature, heartrate, resprate, o2sat, sbp, dbp, rhythm, pain\n",
    "          FROM `{PHYS}.{ED}.vitalsign`\n",
    "        ),\n",
    "        first_vs AS (\n",
    "          SELECT\n",
    "            v.stay_id,\n",
    "            (ARRAY_AGG(STRUCT(v.charttime, v.temperature, v.heartrate, v.resprate, v.o2sat, v.sbp, v.dbp, v.rhythm, v.pain)\n",
    "                       ORDER BY v.charttime LIMIT 1))[OFFSET(0)] AS pick\n",
    "          FROM vs v\n",
    "          JOIN edmap m USING (stay_id)\n",
    "          GROUP BY v.stay_id\n",
    "        )\n",
    "        SELECT\n",
    "          f.stay_id AS ed_stay_id,\n",
    "          m.hadm_id,\n",
    "          pick.charttime AS ed_first_vitals_time,\n",
    "          pick.temperature AS ed_first_temp,\n",
    "          pick.heartrate AS ed_first_hr,\n",
    "          pick.resprate AS ed_first_rr,\n",
    "          pick.o2sat AS ed_first_o2sat,\n",
    "          pick.sbp AS ed_first_sbp,\n",
    "          pick.dbp AS ed_first_dbp,\n",
    "          pick.rhythm AS ed_first_rhythm,\n",
    "          pick.pain AS ed_first_pain\n",
    "        FROM first_vs f\n",
    "        JOIN edmap m ON m.stay_id = f.stay_id\n",
    "        \"\"\"\n",
    "        ed_first = run_sql_bq(ed_first_vitals_sql, {'hadms': hadm_list})\n",
    "        print('ED first vitals rows (fallback query):', len(ed_first))\n",
    "\n",
    "# Debug columns before merge\n",
    "print('ed_triage cols:', list(ed_triage.columns))\n",
    "print('ed_first cols:', list(ed_first.columns))\n",
    "print('ed_df cols:', list(ed_df.columns))\n",
    "\n",
    "if 'ed_stay_id' not in ed_df.columns:\n",
    "    raise KeyError('ed_df missing ed_stay_id; ensure ED spine cell ran.')\n",
    "\n",
    "# Merge ED triage + vitals onto ed_df with available keys\n",
    "merge_keys_triage = [k for k in [\"ed_stay_id\", \"hadm_id\"] if k in ed_df.columns and k in ed_triage.columns]\n",
    "if not merge_keys_triage:\n",
    "    raise KeyError(\"No common keys between ed_df and ed_triage\")\n",
    "ed_df = ed_df.merge(ed_triage, on=merge_keys_triage, how=\"left\")\n",
    "merge_keys_first = [k for k in [\"ed_stay_id\", \"hadm_id\"] if k in ed_df.columns and k in ed_first.columns]\n",
    "if not merge_keys_first:\n",
    "    raise KeyError(\"No common keys between ed_df and ed_first\")\n",
    "ed_df = ed_df.merge(ed_first, on=merge_keys_first, how=\"left\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a112697d",
   "metadata": {},
   "source": [
    "## Phase 2 — Hospital admission context and outcomes\n",
    "\n",
    "**Rationale:** Add admission-level outcomes and demographics for downstream stratification and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aad284",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:29:26.177725Z",
     "iopub.status.busy": "2026-02-05T23:29:26.177573Z",
     "iopub.status.idle": "2026-02-05T23:29:30.732362Z",
     "shell.execute_reply": "2026-02-05T23:29:30.732080Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.\n",
    "\n",
    "# Admissions fields\n",
    "SQL[\"admit_sql\"] = f\"\"\"\n",
    "WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x)\n",
    "SELECT\n",
    "  a.hadm_id,\n",
    "  a.admittime,\n",
    "  a.dischtime,\n",
    "  a.deathtime,\n",
    "  a.hospital_expire_flag,\n",
    "  a.admission_type,\n",
    "  a.admission_location,\n",
    "  a.discharge_location,\n",
    "  a.insurance,\n",
    "  a.language,\n",
    "  a.marital_status,\n",
    "  a.race AS hosp_race\n",
    "FROM `{PHYS}.{HOSP}.admissions` a\n",
    "JOIN hadms h USING (hadm_id)\n",
    "\"\"\"\n",
    "\n",
    "admit = run_sql_bq(sql(\"admit_sql\"), {\"hadms\": hadm_list})\n",
    "print(\"Admissions rows:\", len(admit))\n",
    "\n",
    "ed_df = ed_df.merge(admit, on=\"hadm_id\", how=\"left\")\n",
    "\n",
    "# Merge ventilation flags/times (hadm-level)\n",
    "if \"vent_combined\" in globals():\n",
    "    ed_df = ed_df.merge(vent_combined, on=\"hadm_id\", how=\"left\")\n",
    "\n",
    "# Derived outcomes\n",
    "ed_df[\"hospital_los_hours\"] = (ed_df[\"dischtime\"] - ed_df[\"admittime\"]).dt.total_seconds() / 3600.0\n",
    "ed_df[\"in_hospital_death\"] = ((ed_df[\"hospital_expire_flag\"] == 1) | ed_df[\"deathtime\"].notna()).astype(\"int64\")\n",
    "\n",
    "# Concordance check\n",
    "discord = (\n",
    "    ((ed_df[\"hospital_expire_flag\"] == 1) & ed_df[\"deathtime\"].isna()) |\n",
    "    ((ed_df[\"hospital_expire_flag\"] == 0) & ed_df[\"deathtime\"].notna())\n",
    ")\n",
    "print(\"Admissions discordance (expire_flag vs deathtime):\", int(discord.sum()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4904f650",
   "metadata": {},
   "source": [
    "## Phase 3 — ICU timing and LOS\n",
    "\n",
    "**Rationale:** Capture ICU exposure, timing, and total LOS for severity stratification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5174a456",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:29:30.733897Z",
     "iopub.status.busy": "2026-02-05T23:29:30.733685Z",
     "iopub.status.idle": "2026-02-05T23:29:35.053680Z",
     "shell.execute_reply": "2026-02-05T23:29:35.053290Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.\n",
    "\n",
    "# ICU stays (aggregate per hadm)\n",
    "SQL[\"icu_sql\"] = f\"\"\"\n",
    "WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x)\n",
    "SELECT\n",
    "  i.hadm_id,\n",
    "  i.stay_id AS icu_stay_id,\n",
    "  i.intime,\n",
    "  i.outtime,\n",
    "  i.los,\n",
    "  i.first_careunit,\n",
    "  i.last_careunit\n",
    "FROM `{PHYS}.{ICU}.icustays` i\n",
    "JOIN hadms h USING (hadm_id)\n",
    "\"\"\"\n",
    "\n",
    "icu = run_sql_bq(sql(\"icu_sql\"), {\"hadms\": hadm_list})\n",
    "print(\"ICU stay rows:\", len(icu))\n",
    "\n",
    "if len(icu) > 0:\n",
    "    icu_agg = (\n",
    "        icu.sort_values([\"hadm_id\", \"intime\"]).groupby(\"hadm_id\", as_index=False)\n",
    "        .agg(\n",
    "            icu_intime_first=(\"intime\", \"min\"),\n",
    "            icu_outtime_last=(\"outtime\", \"max\"),\n",
    "            icu_los_total=(\"los\", \"sum\"),\n",
    "            n_icu_stays=(\"icu_stay_id\", \"nunique\"),\n",
    "            first_careunit=(\"first_careunit\", \"first\"),\n",
    "            last_careunit=(\"last_careunit\", \"last\"),\n",
    "        )\n",
    "    )\n",
    "    ed_df = ed_df.merge(icu_agg, on=\"hadm_id\", how=\"left\")\n",
    "else:\n",
    "    print(\"No ICU stays found for cohort.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7764f3",
   "metadata": {},
   "source": [
    "## Phase 4 — ED longitudinal vitals (0–6h)\n",
    "\n",
    "**Rationale:** Summarize early ED vitals for severity phenotyping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d01b4d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:29:35.058846Z",
     "iopub.status.busy": "2026-02-05T23:29:35.058726Z",
     "iopub.status.idle": "2026-02-05T23:29:40.665134Z",
     "shell.execute_reply": "2026-02-05T23:29:40.664762Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Build ED vitals features robustly without a single large BigQuery join that can stall.\n",
    "\n",
    "# ED vitals long + aggregates (0-6h)\n",
    "\n",
    "# Build the ED stay map from in-memory cohort tables to keep query scope tight and deterministic.\n",
    "if \"ed_df\" not in globals():\n",
    "    raise NameError(\"ed_df is required before ED vitals extraction\")\n",
    "\n",
    "required_cols = [\"ed_stay_id\", \"hadm_id\", \"ed_intime\"]\n",
    "missing_required = [c for c in required_cols if c not in ed_df.columns]\n",
    "if missing_required:\n",
    "    raise KeyError(f\"ed_df missing required columns for ED vitals extraction: {missing_required}\")\n",
    "\n",
    "edmap_local = (\n",
    "    ed_df[required_cols]\n",
    "    .dropna(subset=[\"ed_stay_id\"])\n",
    "    .drop_duplicates(subset=[\"ed_stay_id\"])\n",
    "    .copy()\n",
    ")\n",
    "edmap_local[\"ed_stay_id\"] = pd.to_numeric(edmap_local[\"ed_stay_id\"], errors=\"coerce\")\n",
    "edmap_local = edmap_local.dropna(subset=[\"ed_stay_id\"])\n",
    "edmap_local[\"ed_stay_id\"] = edmap_local[\"ed_stay_id\"].astype(int)\n",
    "\n",
    "stay_ids = sorted(edmap_local[\"ed_stay_id\"].unique().tolist())\n",
    "print(\"ED stays for vitals pull:\", len(stay_ids))\n",
    "\n",
    "SQL[\"ed_vitals_sql\"] = f\"\"\"\n",
    "SELECT\n",
    "  stay_id AS ed_stay_id,\n",
    "  charttime,\n",
    "  temperature,\n",
    "  heartrate,\n",
    "  resprate,\n",
    "  o2sat,\n",
    "  sbp,\n",
    "  dbp,\n",
    "  rhythm,\n",
    "  pain\n",
    "FROM `{PHYS}.{ED}.vitalsign`\n",
    "WHERE stay_id IN UNNEST(@stay_ids)\n",
    "\"\"\"\n",
    "\n",
    "# Query in chunks so we avoid one very large parameter payload/job.\n",
    "chunk_size = 5000\n",
    "vitals_chunks = []\n",
    "for i in range(0, len(stay_ids), chunk_size):\n",
    "    chunk = stay_ids[i:i + chunk_size]\n",
    "    part = run_sql_bq(sql(\"ed_vitals_sql\"), {\"stay_ids\": chunk})\n",
    "    if len(part) > 0:\n",
    "        part[\"ed_stay_id\"] = pd.to_numeric(part[\"ed_stay_id\"], errors=\"coerce\")\n",
    "        part = part.dropna(subset=[\"ed_stay_id\"])\n",
    "        part[\"ed_stay_id\"] = part[\"ed_stay_id\"].astype(int)\n",
    "        vitals_chunks.append(part)\n",
    "    print(f\"ED vitals chunk {i // chunk_size + 1}: rows={len(part)}\")\n",
    "\n",
    "if vitals_chunks:\n",
    "    ed_vitals_long = pd.concat(vitals_chunks, ignore_index=True)\n",
    "else:\n",
    "    ed_vitals_long = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"ed_stay_id\", \"charttime\", \"temperature\", \"heartrate\", \"resprate\",\n",
    "            \"o2sat\", \"sbp\", \"dbp\", \"rhythm\", \"pain\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "ed_vitals_long = ed_vitals_long.merge(edmap_local, on=\"ed_stay_id\", how=\"inner\")\n",
    "print(\"ED vitals long rows:\", len(ed_vitals_long))\n",
    "\n",
    "# Window filter: 0-6h from ED intime\n",
    "ed_vitals_long[\"dt_hours\"] = (ed_vitals_long[\"charttime\"] - ed_vitals_long[\"ed_intime\"]).dt.total_seconds() / 3600.0\n",
    "in_6h = ed_vitals_long[\"dt_hours\"].between(0, 6, inclusive=\"both\")\n",
    "\n",
    "agg = (\n",
    "    ed_vitals_long.loc[in_6h]\n",
    "    .groupby(\"ed_stay_id\", as_index=False)\n",
    "    .agg(\n",
    "        max_heartrate_0_6h=(\"heartrate\", \"max\"),\n",
    "        max_resprate_0_6h=(\"resprate\", \"max\"),\n",
    "        min_o2sat_0_6h=(\"o2sat\", \"min\"),\n",
    "        min_sbp_0_6h=(\"sbp\", \"min\"),\n",
    "        n_vitals_0_6h=(\"charttime\", \"count\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "ed_df = ed_df.merge(agg, on=\"ed_stay_id\", how=\"left\")\n",
    "\n",
    "# Range warnings (report only, do not drop)\n",
    "range_checks = {\n",
    "    \"heartrate\": (0, 300),\n",
    "    \"resprate\": (0, 80),\n",
    "    \"o2sat\": (0, 100),\n",
    "    \"sbp\": (0, 300),\n",
    "}\n",
    "for col, (lo, hi) in range_checks.items():\n",
    "    bad = ed_vitals_long[col].notna() & (~ed_vitals_long[col].between(lo, hi))\n",
    "    if bad.any():\n",
    "        print(f\"Warning: {col} out of range count:\", int(bad.sum()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae91c8d5",
   "metadata": {},
   "source": [
    "## Phase 5 — Robust lab discovery + gas panels\n",
    "\n",
    "**Rationale:** Capture blood gas and key chemistry labs with label-robust item discovery and unit normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a485417b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:29:40.667198Z",
     "iopub.status.busy": "2026-02-05T23:29:40.667073Z",
     "iopub.status.idle": "2026-02-05T23:29:46.782014Z",
     "shell.execute_reply": "2026-02-05T23:29:46.781574Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Discover lab item IDs robustly and record cohort-aware item frequency for auditability.\n",
    "\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Discover itemids from d_labitems\n",
    "SQL[\"labitems_sql\"] = f\"\"\"\n",
    "SELECT itemid, label, fluid, category\n",
    "FROM `{PHYS}.{HOSP}.d_labitems`\n",
    "\"\"\"\n",
    "\n",
    "labitems = run_sql_bq(sql(\"labitems_sql\"))\n",
    "\n",
    "patterns = {\n",
    "    \"gas_pco2\": re.compile(r\"\\bp\\s*co2\\b|pco2|pco₂\", re.I),\n",
    "    \"gas_ph\": re.compile(r\"\\bph\\b\", re.I),\n",
    "    \"gas_hco3\": re.compile(r\"hco3|bicarbonate\", re.I),\n",
    "    \"gas_lactate\": re.compile(r\"lactate\", re.I),\n",
    "    \"gas_specimen\": re.compile(r\"specimen|source|type\", re.I),\n",
    "    \"chem_creatinine\": re.compile(r\"creatinine\", re.I),\n",
    "    \"chem_sodium\": re.compile(r\"\\bsodium\\b\", re.I),\n",
    "    \"chem_chloride\": re.compile(r\"\\bchloride\\b\", re.I),\n",
    "    \"chem_total_co2\": re.compile(r\"carbon dioxide|total co2|\\bco2\\b\", re.I),\n",
    "    \"cbc_hemoglobin\": re.compile(r\"hemoglobin\", re.I),\n",
    "}\n",
    "\n",
    "# Category filters limit false matches (for example chemistry CO2 vs blood-gas pCO2)\n",
    "cat_gas = re.compile(r\"blood\\s*gas|blood gas|arterial|venous\", re.I)\n",
    "cat_chem = re.compile(r\"chemistry|chem|blood\", re.I)\n",
    "cat_cbc = re.compile(r\"hematology|cbc\", re.I)\n",
    "\n",
    "matches = {}\n",
    "for name, pat in patterns.items():\n",
    "    dfm = labitems.copy()\n",
    "    dfm = dfm[dfm[\"label\"].str.contains(pat, na=False)]\n",
    "    if name.startswith(\"gas_\"):\n",
    "        dfm = dfm[dfm[\"category\"].str.contains(cat_gas, na=False)]\n",
    "    elif name.startswith(\"chem_\"):\n",
    "        dfm = dfm[dfm[\"category\"].str.contains(cat_chem, na=False)]\n",
    "    elif name.startswith(\"cbc_\"):\n",
    "        dfm = dfm[dfm[\"category\"].str.contains(cat_cbc, na=False)]\n",
    "    matches[name] = dfm[[\"itemid\", \"label\", \"category\"]]\n",
    "\n",
    "# Build lab_item_map with cohort counts. Run hadm counts in chunks to avoid one oversized query.\n",
    "itemids_all = sorted({int(i) for dfm in matches.values() for i in dfm[\"itemid\"].tolist()})\n",
    "\n",
    "SQL[\"counts_sql\"] = f\"\"\"\n",
    "WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x)\n",
    "SELECT itemid, COUNT(*) AS n\n",
    "FROM `{PHYS}.{HOSP}.labevents`\n",
    "WHERE hadm_id IN (SELECT hadm_id FROM hadms)\n",
    "  AND itemid IN UNNEST(@itemids)\n",
    "GROUP BY itemid\n",
    "\"\"\"\n",
    "\n",
    "counts_cols = [\"itemid\", \"n\"]\n",
    "if itemids_all and len(hadm_list) > 0:\n",
    "    hadm_chunk_size = 10000\n",
    "    count_parts = []\n",
    "    for i in range(0, len(hadm_list), hadm_chunk_size):\n",
    "        hadm_chunk = hadm_list[i:i + hadm_chunk_size]\n",
    "        part = run_sql_bq(sql(\"counts_sql\"), {\"hadms\": hadm_chunk, \"itemids\": itemids_all})\n",
    "        if len(part) > 0:\n",
    "            count_parts.append(part)\n",
    "        print(f\"Lab item count chunk {i // hadm_chunk_size + 1}: rows={len(part)}\")\n",
    "\n",
    "    if count_parts:\n",
    "        counts = (\n",
    "            pd.concat(count_parts, ignore_index=True)\n",
    "            .groupby(\"itemid\", as_index=False)[\"n\"]\n",
    "            .sum()\n",
    "        )\n",
    "    else:\n",
    "        counts = pd.DataFrame(columns=counts_cols)\n",
    "else:\n",
    "    counts = pd.DataFrame(columns=counts_cols)\n",
    "\n",
    "lab_item_map = {}\n",
    "for name, dfm in matches.items():\n",
    "    tmp = dfm.merge(counts, on=\"itemid\", how=\"left\").fillna({\"n\": 0})\n",
    "    lab_item_map[name] = {\n",
    "        \"pattern\": patterns[name].pattern,\n",
    "        \"items\": tmp.sort_values(\"n\", ascending=False).to_dict(orient=\"records\"),\n",
    "    }\n",
    "\n",
    "lab_item_map_path = WORK_DIR / \"lab_item_map.json\"\n",
    "lab_item_map_path.write_text(json.dumps(lab_item_map, indent=2))\n",
    "print(\"Wrote:\", lab_item_map_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fa3fcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:29:46.785897Z",
     "iopub.status.busy": "2026-02-05T23:29:46.785733Z",
     "iopub.status.idle": "2026-02-05T23:29:51.830123Z",
     "shell.execute_reply": "2026-02-05T23:29:51.829778Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Extract cohort labs and apply ED-time windows in pandas for predictable runtime.\n",
    "\n",
    "# Extract labevents and apply ED 0-24h windows locally\n",
    "\n",
    "# Assemble itemid lists\n",
    "itemid_sets = {k: [int(x[\"itemid\"]) for x in v[\"items\"]] for k, v in lab_item_map.items()}\n",
    "all_itemids = sorted({i for v in itemid_sets.values() for i in v})\n",
    "\n",
    "if \"ed_df\" not in globals():\n",
    "    raise NameError(\"ed_df is required before lab window extraction\")\n",
    "\n",
    "ed_windows = (\n",
    "    ed_df[[\"ed_stay_id\", \"hadm_id\", \"ed_intime\"]]\n",
    "    .dropna(subset=[\"ed_stay_id\", \"hadm_id\", \"ed_intime\"])\n",
    "    .drop_duplicates()\n",
    "    .copy()\n",
    ")\n",
    "ed_windows[\"hadm_id\"] = pd.to_numeric(ed_windows[\"hadm_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "ed_windows = ed_windows.dropna(subset=[\"hadm_id\"])\n",
    "ed_windows[\"hadm_id\"] = ed_windows[\"hadm_id\"].astype(int)\n",
    "\n",
    "SQL[\"labs_sql\"] = f\"\"\"\n",
    "SELECT\n",
    "  subject_id,\n",
    "  hadm_id,\n",
    "  itemid,\n",
    "  charttime,\n",
    "  specimen_id,\n",
    "  valuenum,\n",
    "  valueuom\n",
    "FROM `{PHYS}.{HOSP}.labevents`\n",
    "WHERE hadm_id IN UNNEST(@hadms)\n",
    "  AND itemid IN UNNEST(@itemids)\n",
    "\"\"\"\n",
    "\n",
    "# Run in hadm chunks; merge to ED windows in pandas to keep each query lightweight.\n",
    "labs_parts = []\n",
    "if all_itemids and len(hadm_list) > 0:\n",
    "    hadm_chunk_size = 5000\n",
    "    for i in range(0, len(hadm_list), hadm_chunk_size):\n",
    "        hadm_chunk = hadm_list[i:i + hadm_chunk_size]\n",
    "        part = run_sql_bq(sql(\"labs_sql\"), {\"hadms\": hadm_chunk, \"itemids\": all_itemids})\n",
    "        raw_n = len(part)\n",
    "\n",
    "        if raw_n > 0:\n",
    "            part[\"hadm_id\"] = pd.to_numeric(part[\"hadm_id\"], errors=\"coerce\")\n",
    "            part = part.dropna(subset=[\"hadm_id\"])\n",
    "            part[\"hadm_id\"] = part[\"hadm_id\"].astype(int)\n",
    "\n",
    "            part = part.merge(ed_windows, on=\"hadm_id\", how=\"inner\")\n",
    "            in_window = (\n",
    "                (part[\"charttime\"] >= part[\"ed_intime\"]) &\n",
    "                (part[\"charttime\"] <= (part[\"ed_intime\"] + pd.Timedelta(hours=24)))\n",
    "            )\n",
    "            part = part.loc[in_window, [\n",
    "                \"ed_stay_id\", \"subject_id\", \"hadm_id\", \"itemid\", \"charttime\", \"specimen_id\", \"valuenum\", \"valueuom\"\n",
    "            ]]\n",
    "            if len(part) > 0:\n",
    "                labs_parts.append(part)\n",
    "\n",
    "        print(f\"Labs window chunk {i // hadm_chunk_size + 1}: raw={raw_n}, kept={len(part) if raw_n > 0 else 0}\")\n",
    "\n",
    "if labs_parts:\n",
    "    labs_long = pd.concat(labs_parts, ignore_index=True)\n",
    "else:\n",
    "    labs_long = pd.DataFrame(\n",
    "        columns=[\"ed_stay_id\", \"subject_id\", \"hadm_id\", \"itemid\", \"charttime\", \"specimen_id\", \"valuenum\", \"valueuom\"]\n",
    "    )\n",
    "\n",
    "print(\"Labs long rows:\", len(labs_long))\n",
    "\n",
    "# Unit audit for pCO2\n",
    "pco2_ids = itemid_sets.get(\"gas_pco2\", [])\n",
    "unit_audit = (\n",
    "    labs_long.loc[labs_long[\"itemid\"].isin(pco2_ids)]\n",
    "    .groupby(\"valueuom\", dropna=False)\n",
    "    .size().reset_index(name=\"n\")\n",
    ")\n",
    "unit_audit_path = WORK_DIR / \"lab_unit_audit.csv\"\n",
    "unit_audit.to_csv(unit_audit_path, index=False)\n",
    "print(\"Wrote:\", unit_audit_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90790b61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:29:51.831734Z",
     "iopub.status.busy": "2026-02-05T23:29:51.831630Z",
     "iopub.status.idle": "2026-02-05T23:29:52.109767Z",
     "shell.execute_reply": "2026-02-05T23:29:52.109481Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Reconstruct gas panels and normalize units before ED-stay level aggregation.\n",
    "\n",
    "# Reconstruct gas panels by specimen_id within ED stay\n",
    "\n",
    "pco2_ids = set(itemid_sets.get(\"gas_pco2\", []))\n",
    "ph_ids = set(itemid_sets.get(\"gas_ph\", []))\n",
    "hco3_ids = set(itemid_sets.get(\"gas_hco3\", []))\n",
    "lact_ids = set(itemid_sets.get(\"gas_lactate\", []))\n",
    "\n",
    "labs = labs_long.copy()\n",
    "\n",
    "# Convert pCO2 kPa to mmHg when needed\n",
    "is_kpa = labs[\"valueuom\"].astype(str).str.lower().str.contains(\"kpa\", na=False)\n",
    "mask_pco2 = labs[\"itemid\"].isin(pco2_ids)\n",
    "if mask_pco2.any() and is_kpa.any():\n",
    "    labs.loc[mask_pco2 & is_kpa, \"valuenum\"] = labs.loc[mask_pco2 & is_kpa, \"valuenum\"] * 7.50062\n",
    "    labs.loc[mask_pco2 & is_kpa, \"valueuom\"] = \"mmHg\"\n",
    "\n",
    "# panel by specimen_id\n",
    "panel = (\n",
    "    labs.groupby([\"ed_stay_id\",\"specimen_id\"], as_index=False)\n",
    "    .agg(panel_time=(\"charttime\",\"min\"))\n",
    ")\n",
    "\n",
    "# attach analytes\n",
    "\n",
    "def pick_analyte(df, ids, name):\n",
    "    tmp = df.loc[df[\"itemid\"].isin(ids), [\"ed_stay_id\",\"specimen_id\",\"valuenum\"]]\n",
    "    tmp = tmp.rename(columns={\"valuenum\": name})\n",
    "    return tmp.groupby([\"ed_stay_id\",\"specimen_id\"], as_index=False).first()\n",
    "\n",
    "if pco2_ids:\n",
    "    panel = panel.merge(pick_analyte(labs, pco2_ids, \"pco2\"), on=[\"ed_stay_id\",\"specimen_id\"], how=\"left\")\n",
    "if ph_ids:\n",
    "    panel = panel.merge(pick_analyte(labs, ph_ids, \"ph\"), on=[\"ed_stay_id\",\"specimen_id\"], how=\"left\")\n",
    "if hco3_ids:\n",
    "    panel = panel.merge(pick_analyte(labs, hco3_ids, \"hco3\"), on=[\"ed_stay_id\",\"specimen_id\"], how=\"left\")\n",
    "if lact_ids:\n",
    "    panel = panel.merge(pick_analyte(labs, lact_ids, \"lactate\"), on=[\"ed_stay_id\",\"specimen_id\"], how=\"left\")\n",
    "\n",
    "# First panel per ED stay\n",
    "first_panel = (\n",
    "    panel.sort_values([\"ed_stay_id\",\"panel_time\"]).groupby(\"ed_stay_id\", as_index=False).first()\n",
    ")\n",
    "\n",
    "# 0–6h and 0–24h extrema\n",
    "panel = panel.merge(ed_df[[\"ed_stay_id\",\"ed_intime\"]], on=\"ed_stay_id\", how=\"left\")\n",
    "panel[\"dt_hours\"] = (panel[\"panel_time\"] - panel[\"ed_intime\"]).dt.total_seconds() / 3600.0\n",
    "\n",
    "# Ensure expected panel columns exist even if analyte is absent\n",
    "for col in [\"pco2\", \"ph\", \"hco3\", \"lactate\"]:\n",
    "    if col not in panel.columns:\n",
    "        panel[col] = pd.NA\n",
    "\n",
    "p06 = panel.loc[panel[\"dt_hours\"].between(0,6, inclusive=\"both\")]\n",
    "p24 = panel.loc[panel[\"dt_hours\"].between(0,24, inclusive=\"both\")]\n",
    "\n",
    "agg06 = p06.groupby(\"ed_stay_id\", as_index=False).agg(max_pco2_0_6h=(\"pco2\",\"max\"), min_ph_0_6h=(\"ph\",\"min\"))\n",
    "agg24 = p24.groupby(\"ed_stay_id\", as_index=False).agg(max_pco2_0_24h=(\"pco2\",\"max\"), min_ph_0_24h=(\"ph\",\"min\"))\n",
    "\n",
    "ed_df = ed_df.merge(first_panel[[\"ed_stay_id\",\"panel_time\",\"pco2\",\"ph\",\"hco3\",\"lactate\"]].rename(\n",
    "    columns={\"panel_time\":\"first_gas_time\",\"pco2\":\"first_pco2\",\"ph\":\"first_ph\",\"hco3\":\"first_hco3\",\"lactate\":\"first_lactate\"}\n",
    "), on=\"ed_stay_id\", how=\"left\")\n",
    "\n",
    "ed_df = ed_df.merge(agg06, on=\"ed_stay_id\", how=\"left\")\n",
    "ed_df = ed_df.merge(agg24, on=\"ed_stay_id\", how=\"left\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8759509b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:29:52.111340Z",
     "iopub.status.busy": "2026-02-05T23:29:52.111264Z",
     "iopub.status.idle": "2026-02-05T23:29:57.540600Z",
     "shell.execute_reply": "2026-02-05T23:29:57.540256Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Reconstruct gas panels and normalize units before ED-stay level aggregation.\n",
    "\n",
    "# ABG vs VBG classification (specimen- and label-based inference)\n",
    "\n",
    "# Use label heuristics from d_labitems if available (arterial/venous hints)\n",
    "labitems_map = (\n",
    "    labitems.set_index(\"itemid\")[[\"label\", \"fluid\"]]\n",
    "    .fillna(\"\")\n",
    "    .to_dict(\"index\")\n",
    ")\n",
    "\n",
    "def _infer_source_text(text: str) -> str | None:\n",
    "    if re.search(r\"(arterial|abg|a[- ]?line|\\bart\\b)\", text):\n",
    "        return \"arterial\"\n",
    "    if re.search(r\"(venous|vbg|central|mixed|\\bven\\b)\", text):\n",
    "        return \"venous\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def infer_source_item(itemid):\n",
    "    meta = labitems_map.get(itemid, {})\n",
    "    text = f\"{meta.get('label','')} {meta.get('fluid','')}\".lower()\n",
    "    return _infer_source_text(text)\n",
    "\n",
    "# Infer source per specimen_id using ANY item label within the specimen\n",
    "spec_source = None\n",
    "if \"specimen_id\" in labs.columns and labs[\"specimen_id\"].notna().any():\n",
    "    spec_items = labs.loc[labs[\"specimen_id\"].notna(), [\"ed_stay_id\", \"specimen_id\", \"itemid\"]].drop_duplicates()\n",
    "    spec_items[\"source_hint\"] = spec_items[\"itemid\"].map(infer_source_item)\n",
    "\n",
    "    def _resolve_source(s):\n",
    "        s = s.dropna()\n",
    "        if (s == \"arterial\").any():\n",
    "            return \"arterial\"\n",
    "        if (s == \"venous\").any():\n",
    "            return \"venous\"\n",
    "        return \"other\"\n",
    "\n",
    "    spec_source = (\n",
    "        spec_items.groupby([\"ed_stay_id\", \"specimen_id\"], as_index=False)[\"source_hint\"]\n",
    "        .apply(_resolve_source)\n",
    "        .rename(columns={\"source_hint\": \"source\"})\n",
    "    )\n",
    "\n",
    "if spec_source is not None:\n",
    "    panel = panel.merge(spec_source, on=[\"ed_stay_id\", \"specimen_id\"], how=\"left\")\n",
    "\n",
    "# Fallback: assign source based on pco2 item label\n",
    "if pco2_ids:\n",
    "    pco2_itemid = (\n",
    "        labs.loc[labs[\"itemid\"].isin(pco2_ids), [\"ed_stay_id\", \"specimen_id\", \"itemid\"]]\n",
    "        .groupby([\"ed_stay_id\", \"specimen_id\"], as_index=False)[\"itemid\"].first()\n",
    "    )\n",
    "    pco2_itemid[\"source_pco2\"] = pco2_itemid[\"itemid\"].map(infer_source_item)\n",
    "    panel = panel.merge(pco2_itemid[[\"ed_stay_id\", \"specimen_id\", \"source_pco2\"]], on=[\"ed_stay_id\", \"specimen_id\"], how=\"left\")\n",
    "\n",
    "    if \"source\" in panel.columns:\n",
    "        panel[\"source\"] = panel[\"source\"].where(panel[\"source\"] != \"other\", pd.NA)\n",
    "        panel[\"source\"] = panel[\"source\"].fillna(panel[\"source_pco2\"])\n",
    "    else:\n",
    "        panel[\"source\"] = panel[\"source_pco2\"]\n",
    "    panel = panel.drop(columns=[\"source_pco2\"])\n",
    "else:\n",
    "    if \"source\" not in panel.columns:\n",
    "        panel[\"source\"] = \"other\"\n",
    "\n",
    "panel[\"source\"] = panel[\"source\"].fillna(\"other\")\n",
    "\n",
    "# flags\n",
    "panel[\"flag_abg_hypercapnia\"] = ((panel[\"source\"]==\"arterial\") & (panel[\"pco2\"]>=45)).astype(int)\n",
    "panel[\"flag_vbg_hypercapnia\"] = ((panel[\"source\"]==\"venous\") & (panel[\"pco2\"]>=50)).astype(int)\n",
    "panel[\"flag_other_hypercapnia\"] = ((panel[\"source\"]==\"other\") & (panel[\"pco2\"]>=50)).astype(int)\n",
    "panel[\"flag_any_gas_hypercapnia\"] = ((panel[\"flag_abg_hypercapnia\"]==1) | (panel[\"flag_vbg_hypercapnia\"]==1) | (panel[\"flag_other_hypercapnia\"]==1)).astype(int)\n",
    "\n",
    "# per-stay unknown source rate\n",
    "if len(panel) > 0:\n",
    "    unk_rate = (\n",
    "        panel.assign(_unk=(panel[\"source\"].fillna(\"other\") == \"other\"))\n",
    "             .groupby(\"ed_stay_id\", as_index=False)[\"_unk\"].mean()\n",
    "             .rename(columns={\"_unk\": \"gas_source_other_rate\"})\n",
    "    )\n",
    "else:\n",
    "    unk_rate = panel[[\"ed_stay_id\"]].drop_duplicates()\n",
    "    unk_rate[\"gas_source_other_rate\"] = 1.0\n",
    "\n",
    "# collapse to ED stay flags\n",
    "flags = panel.groupby(\"ed_stay_id\", as_index=False).agg(\n",
    "    flag_abg_hypercapnia=(\"flag_abg_hypercapnia\",\"max\"),\n",
    "    flag_vbg_hypercapnia=(\"flag_vbg_hypercapnia\",\"max\"),\n",
    "    flag_other_hypercapnia=(\"flag_other_hypercapnia\",\"max\"),\n",
    "    flag_any_gas_hypercapnia=(\"flag_any_gas_hypercapnia\",\"max\"),\n",
    ")\n",
    "\n",
    "ed_df = ed_df.merge(flags, on=\"ed_stay_id\", how=\"left\")\n",
    "ed_df = ed_df.merge(unk_rate, on=\"ed_stay_id\", how=\"left\")\n",
    "ed_df[\"gas_source_other_rate\"] = ed_df[\"gas_source_other_rate\"].fillna(1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b49df4",
   "metadata": {},
   "source": [
    "## Phase 5C — ICU POC blood gases (chartevents, optional)\n",
    "\n",
    "**Rationale:** Capture ICU point-of-care gases if central lab labevents miss early measurements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1799ff9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:29:57.542261Z",
     "iopub.status.busy": "2026-02-05T23:29:57.542149Z",
     "iopub.status.idle": "2026-02-05T23:29:59.579787Z",
     "shell.execute_reply": "2026-02-05T23:29:59.579354Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.\n",
    "\n",
    "# Discover ICU POC itemids from d_items\n",
    "\n",
    "SQL[\"ditems_sql\"] = f\"\"\"\n",
    "SELECT itemid, label, category\n",
    "FROM `{PHYS}.{ICU}.d_items`\n",
    "\"\"\"\n",
    "\n",
    "ditems = run_sql_bq(sql(\"ditems_sql\"))\n",
    "\n",
    "icu_patterns = {\n",
    "    \"pco2\": re.compile(r\"\\bp\\s*co2\\b|pco2|pco₂\", re.I),\n",
    "    \"ph\": re.compile(r\"\\bph\\b\", re.I),\n",
    "    \"hco3\": re.compile(r\"hco3|bicarbonate\", re.I),\n",
    "    \"lactate\": re.compile(r\"lactate\", re.I),\n",
    "    \"specimen\": re.compile(r\"specimen|source|type\", re.I),\n",
    "}\n",
    "\n",
    "icu_cat = re.compile(r\"blood\\s*gas|blood gas|resp|arterial|venous\", re.I)\n",
    "\n",
    "icu_matches = {}\n",
    "for name, pat in icu_patterns.items():\n",
    "    dfm = ditems[ditems[\"label\"].str.contains(pat, na=False)]\n",
    "    dfm = dfm[dfm[\"category\"].str.contains(icu_cat, na=False)]\n",
    "    icu_matches[name] = dfm[[\"itemid\",\"label\",\"category\"]]\n",
    "# Exclude non-blood pCO2 measurements from ICU candidates\n",
    "if \"pco2\" in icu_matches and len(icu_matches[\"pco2\"]) > 0:\n",
    "    icu_matches[\"pco2\"] = icu_matches[\"pco2\"][~icu_matches[\"pco2\"][\"label\"].str.contains(r\"co2\\s*(prod|production|elimin|elimination)|\\bvco2\\b\", case=False, na=False)]\n",
    "\n",
    "icu_itemids = sorted({int(i) for dfm in icu_matches.values() for i in dfm[\"itemid\"].tolist()})\n",
    "print(\"ICU POC candidate itemids:\", len(icu_itemids))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3c01a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:29:59.581848Z",
     "iopub.status.busy": "2026-02-05T23:29:59.581640Z",
     "iopub.status.idle": "2026-02-05T23:30:04.102182Z",
     "shell.execute_reply": "2026-02-05T23:30:04.101342Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.\n",
    "\n",
    "# Extract ICU chartevents within ED 0–24h window for cohort ICU stays\n",
    "\n",
    "if len(icu_itemids) == 0:\n",
    "    icu_poc_long = pd.DataFrame()\n",
    "    print(\"No ICU POC itemids found.\")\n",
    "else:\n",
    "    # ensure icu stay ids available\n",
    "    if 'icu' not in globals():\n",
    "        raise NameError(\"ICU stays table 'icu' not found; run ICU phase first.\")\n",
    "\n",
    "    icu_stays = icu[[\"icu_stay_id\",\"hadm_id\",\"intime\"]].copy()\n",
    "    icu_stays = icu_stays.dropna(subset=[\"icu_stay_id\"])\n",
    "\n",
    "    icu_poc_sql = f\"\"\"\n",
    "    WITH icu_stays AS (\n",
    "      SELECT stay_id AS icu_stay_id, hadm_id\n",
    "      FROM `{PHYS}.{ICU}.icustays`\n",
    "      WHERE stay_id IN UNNEST(@icu_stay_ids)\n",
    "    ),\n",
    "    eds AS (\n",
    "      SELECT stay_id AS ed_stay_id, hadm_id, intime AS ed_intime\n",
    "      FROM `{PHYS}.{ED}.edstays`\n",
    "      WHERE hadm_id IN (SELECT hadm_id FROM icu_stays)\n",
    "    )\n",
    "    SELECT\n",
    "      s.icu_stay_id,\n",
    "      e.ed_stay_id,\n",
    "      e.hadm_id,\n",
    "      ce.charttime,\n",
    "      ce.itemid,\n",
    "      ce.valuenum,\n",
    "      ce.valueuom\n",
    "    FROM `{PHYS}.{ICU}.chartevents` ce\n",
    "    JOIN icu_stays s ON s.icu_stay_id = ce.stay_id\n",
    "    JOIN eds e ON e.hadm_id = s.hadm_id\n",
    "    WHERE ce.itemid IN UNNEST(@itemids)\n",
    "      AND ce.charttime BETWEEN e.ed_intime AND TIMESTAMP_ADD(e.ed_intime, INTERVAL 24 HOUR)\n",
    "    \"\"\"\n",
    "\n",
    "    icu_poc_long = run_sql_bq(icu_poc_sql, {\"icu_stay_ids\": icu_stays[\"icu_stay_id\"].astype(int).tolist(), \"itemids\": icu_itemids})\n",
    "    print(\"ICU POC long rows:\", len(icu_poc_long))\n",
    "\n",
    "# Build panels by 5-minute bins per ICU stay\n",
    "if len(icu_poc_long) > 0:\n",
    "    icu_poc_long[\"time_bin\"] = icu_poc_long[\"charttime\"].dt.floor(\"5min\")\n",
    "\n",
    "    def pick_from_ids(df, ids, name):\n",
    "        tmp = df.loc[df[\"itemid\"].isin(ids), [\"icu_stay_id\",\"time_bin\",\"valuenum\"]]\n",
    "        tmp = tmp.rename(columns={\"valuenum\": name})\n",
    "        return tmp.groupby([\"icu_stay_id\",\"time_bin\"], as_index=False).first()\n",
    "\n",
    "    pco2_ids = set(icu_matches.get(\"pco2\", pd.DataFrame()).get(\"itemid\", []).tolist())\n",
    "    ph_ids = set(icu_matches.get(\"ph\", pd.DataFrame()).get(\"itemid\", []).tolist())\n",
    "    hco3_ids = set(icu_matches.get(\"hco3\", pd.DataFrame()).get(\"itemid\", []).tolist())\n",
    "    lact_ids = set(icu_matches.get(\"lactate\", pd.DataFrame()).get(\"itemid\", []).tolist())\n",
    "\n",
    "    panel_poc = (\n",
    "        icu_poc_long.groupby([\"icu_stay_id\",\"time_bin\"], as_index=False)\n",
    "        .agg(panel_time=(\"charttime\",\"min\"))\n",
    "    )\n",
    "    if pco2_ids:\n",
    "        panel_poc = panel_poc.merge(pick_from_ids(icu_poc_long, pco2_ids, \"pco2\"), on=[\"icu_stay_id\",\"time_bin\"], how=\"left\")\n",
    "    if ph_ids:\n",
    "        panel_poc = panel_poc.merge(pick_from_ids(icu_poc_long, ph_ids, \"ph\"), on=[\"icu_stay_id\",\"time_bin\"], how=\"left\")\n",
    "    if hco3_ids:\n",
    "        panel_poc = panel_poc.merge(pick_from_ids(icu_poc_long, hco3_ids, \"hco3\"), on=[\"icu_stay_id\",\"time_bin\"], how=\"left\")\n",
    "    if lact_ids:\n",
    "        panel_poc = panel_poc.merge(pick_from_ids(icu_poc_long, lact_ids, \"lactate\"), on=[\"icu_stay_id\",\"time_bin\"], how=\"left\")\n",
    "\n",
    "    # map to ED stay via hadm_id\n",
    "    panel_poc = panel_poc.merge(icu[[\"icu_stay_id\",\"hadm_id\"]], on=\"icu_stay_id\", how=\"left\")\n",
    "    panel_poc = panel_poc.merge(ed_df[[\"ed_stay_id\",\"hadm_id\",\"ed_intime\"]], on=\"hadm_id\", how=\"left\")\n",
    "    panel_poc[\"dt_hours\"] = (panel_poc[\"panel_time\"] - panel_poc[\"ed_intime\"]).dt.total_seconds() / 3600.0\n",
    "else:\n",
    "    panel_poc = pd.DataFrame()\n",
    "\n",
    "# Ensure expected panel_poc columns exist even if analyte is absent\n",
    "for col in [\"pco2\", \"ph\", \"hco3\", \"lactate\"]:\n",
    "    if col not in panel_poc.columns:\n",
    "        panel_poc[col] = pd.NA\n",
    "\n",
    "poc_flags = pd.DataFrame(columns=[\"ed_stay_id\", \"flag_any_gas_hypercapnia_poc\"])\n",
    "if len(panel_poc) > 0:\n",
    "    p24_poc = panel_poc.loc[panel_poc[\"dt_hours\"].between(0,24, inclusive=\"both\")].copy()\n",
    "    p24_poc.loc[:, \"flag_any_gas_hypercapnia_poc\"] = (p24_poc[\"pco2\"] >= 50).astype(\"Int64\")\n",
    "\n",
    "    poc_flags = p24_poc.groupby(\"ed_stay_id\", as_index=False).agg(\n",
    "        flag_any_gas_hypercapnia_poc=(\"flag_any_gas_hypercapnia_poc\",\"max\")\n",
    "    )\n",
    "\n",
    "# Avoid duplicate column on re-run\n",
    "if \"flag_any_gas_hypercapnia_poc\" in ed_df.columns:\n",
    "    ed_df = ed_df.drop(columns=[\"flag_any_gas_hypercapnia_poc\"])\n",
    "\n",
    "if len(poc_flags) > 0:\n",
    "    ed_df = ed_df.merge(poc_flags, on=\"ed_stay_id\", how=\"left\")\n",
    "\n",
    "# incremental yield\n",
    "base = ed_df.get(\"flag_any_gas_hypercapnia\", pd.Series(0, index=ed_df.index)).fillna(0).astype(int)\n",
    "poc = ed_df.get(\"flag_any_gas_hypercapnia_poc\", pd.Series(0, index=ed_df.index)).fillna(0).astype(int)\n",
    "inc = ((base == 0) & (poc == 1)).sum()\n",
    "print(\"ICU POC incremental hypercapnia cases (ED stays):\", int(inc))\n",
    "\n",
    "# optional export\n",
    "if len(panel_poc) > 0:\n",
    "    from datetime import datetime\n",
    "\n",
    "    prior_runs_dir = DATA_DIR / \"prior runs\"\n",
    "    prior_runs_dir.mkdir(parents=True, exist_ok=True)\n",
    "    poc_path = prior_runs_dir / f\"gas_panels_poc_{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet\"\n",
    "    panel_poc.to_parquet(poc_path, index=False)\n",
    "    print(\"Wrote:\", poc_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3c0913",
   "metadata": {},
   "source": [
    "## Phase 6 — BMI/anthropometrics (OMR)\n",
    "\n",
    "**Rationale:** Add BMI/height/weight closest to ED presentation when available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07cecc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:30:04.108097Z",
     "iopub.status.busy": "2026-02-05T23:30:04.107923Z",
     "iopub.status.idle": "2026-02-05T23:30:06.414947Z",
     "shell.execute_reply": "2026-02-05T23:30:06.414622Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.\n",
    "\n",
    "# OMR BMI/height/weight\n",
    "\n",
    "try:\n",
    "    omr_sql = f\"\"\"\n",
    "    SELECT subject_id, chartdate, result_name, result_value\n",
    "    FROM `{PHYS}.{HOSP}.omr`\n",
    "    WHERE LOWER(result_name) IN ('bmi','height','weight')\n",
    "    \"\"\"\n",
    "    omr = run_sql_bq(omr_sql)\n",
    "    print(\"OMR rows:\", len(omr))\n",
    "\n",
    "    if len(omr) == 0:\n",
    "        print(\"OMR query returned 0 rows; BMI/height/weight will remain missing.\")\n",
    "    else:\n",
    "        omr[\"result_name\"] = omr[\"result_name\"].str.lower()\n",
    "        omr[\"result_value_num\"] = (\n",
    "            omr[\"result_value\"].astype(str)\n",
    "            .str.extract(r\"(-?\\d+(?:\\.\\d+)?)\", expand=False)\n",
    "        )\n",
    "        omr[\"result_value_num\"] = pd.to_numeric(omr[\"result_value_num\"], errors=\"coerce\")\n",
    "\n",
    "        omr_pivot = (\n",
    "            omr.pivot_table(\n",
    "                index=[\"subject_id\",\"chartdate\"],\n",
    "                columns=\"result_name\",\n",
    "                values=\"result_value_num\",\n",
    "                aggfunc=\"first\",\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        # attach closest pre-ED (within 365 days)\n",
    "        ed_dates = ed_df[[\"ed_stay_id\",\"subject_id\",\"ed_intime\"]].copy()\n",
    "        ed_dates[\"ed_date\"] = ed_dates[\"ed_intime\"].dt.date\n",
    "        omr_pivot[\"chartdate\"] = pd.to_datetime(omr_pivot[\"chartdate\"]).dt.date\n",
    "\n",
    "        merged = ed_dates.merge(omr_pivot, on=\"subject_id\", how=\"left\")\n",
    "        merged[\"days_before\"] = (pd.to_datetime(merged[\"ed_date\"]) - pd.to_datetime(merged[\"chartdate\"])).dt.days\n",
    "        merged = merged.loc[(merged[\"days_before\"] >= 0) & (merged[\"days_before\"] <= 365)]\n",
    "\n",
    "        closest = (\n",
    "            merged.sort_values([\"ed_stay_id\",\"days_before\"]).groupby(\"ed_stay_id\", as_index=False).first()\n",
    "            .rename(columns={\n",
    "                \"bmi\":\"bmi_closest_pre_ed\",\n",
    "                \"height\":\"height_closest_pre_ed\",\n",
    "                \"weight\":\"weight_closest_pre_ed\",\n",
    "            })\n",
    "        )\n",
    "\n",
    "        ed_df = ed_df.merge(\n",
    "            closest[[\"ed_stay_id\",\"bmi_closest_pre_ed\",\"height_closest_pre_ed\",\"weight_closest_pre_ed\"]],\n",
    "            on=\"ed_stay_id\", how=\"left\"\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(\"OMR not available or failed:\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5247cb",
   "metadata": {},
   "source": [
    "## Phase 7 — ICD comorbidity flags\n",
    "\n",
    "**Rationale:** Derive comorbidity indicators from index admission ICD codes for stratified analyses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f462dbb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:30:06.416696Z",
     "iopub.status.busy": "2026-02-05T23:30:06.416595Z",
     "iopub.status.idle": "2026-02-05T23:30:15.461594Z",
     "shell.execute_reply": "2026-02-05T23:30:15.461032Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Compute comorbidity flags from hospital + ED ICD codes with chunked hadm queries.\n",
    "\n",
    "# ICD code pulls for comorbidity flags (hospital + ED; combined OR)\n",
    "# NOTE: Use prefix filters to reduce CPU and avoid regex-heavy scans.\n",
    "\n",
    "FLAGS = [\n",
    "    \"flag_copd\", \"flag_osa_ohs\", \"flag_chf\", \"flag_neuromuscular\",\n",
    "    \"flag_opioid_substance\", \"flag_pneumonia\",\n",
    "]\n",
    "\n",
    "SQL[\"icd_flags_sql\"] = f\"\"\"\n",
    "WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x),\n",
    "icd AS (\n",
    "  SELECT\n",
    "    hadm_id,\n",
    "    UPPER(REPLACE(icd_code, \".\", \"\")) AS code_norm\n",
    "  FROM `{PHYS}.{HOSP}.diagnoses_icd`\n",
    "  WHERE hadm_id IN (SELECT hadm_id FROM hadms)\n",
    "    AND (\n",
    "      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"J43\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"J44\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"G473\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"E662\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"I50\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"G12\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"G70\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"G71\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"F11\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"T40\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"F13\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"J12\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"J13\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"J14\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"J15\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"J16\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"J17\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"J18\")\n",
    "    )\n",
    ")\n",
    "SELECT\n",
    "  hadm_id,\n",
    "  MAX(IF(STARTS_WITH(code_norm, \"J43\") OR STARTS_WITH(code_norm, \"J44\"), 1, 0)) AS flag_copd,\n",
    "  MAX(IF(STARTS_WITH(code_norm, \"G473\") OR STARTS_WITH(code_norm, \"E662\"), 1, 0)) AS flag_osa_ohs,\n",
    "  MAX(IF(STARTS_WITH(code_norm, \"I50\"), 1, 0)) AS flag_chf,\n",
    "  MAX(IF(STARTS_WITH(code_norm, \"G12\") OR STARTS_WITH(code_norm, \"G70\") OR STARTS_WITH(code_norm, \"G71\"), 1, 0)) AS flag_neuromuscular,\n",
    "  MAX(IF(STARTS_WITH(code_norm, \"F11\") OR STARTS_WITH(code_norm, \"T40\") OR STARTS_WITH(code_norm, \"F13\"), 1, 0)) AS flag_opioid_substance,\n",
    "  MAX(IF(STARTS_WITH(code_norm, \"J12\") OR STARTS_WITH(code_norm, \"J13\") OR STARTS_WITH(code_norm, \"J14\") OR STARTS_WITH(code_norm, \"J15\") OR STARTS_WITH(code_norm, \"J16\") OR STARTS_WITH(code_norm, \"J17\") OR STARTS_WITH(code_norm, \"J18\"), 1, 0)) AS flag_pneumonia\n",
    "FROM icd\n",
    "GROUP BY hadm_id\n",
    "\"\"\"\n",
    "\n",
    "SQL[\"ed_icd_flags_sql\"] = f\"\"\"\n",
    "WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x),\n",
    "ed_dx AS (\n",
    "  SELECT\n",
    "    s.hadm_id,\n",
    "    UPPER(REPLACE(d.icd_code, \".\", \"\")) AS code_norm\n",
    "  FROM `{PHYS}.{ED}.diagnosis` d\n",
    "  JOIN `{PHYS}.{ED}.edstays` s\n",
    "    ON s.stay_id = d.stay_id\n",
    "  WHERE s.hadm_id IN (SELECT hadm_id FROM hadms)\n",
    "    AND (\n",
    "      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"J43\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"J44\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"G473\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"E662\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"I50\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"G12\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"G70\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"G71\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"F11\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"T40\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"F13\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"J12\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"J13\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"J14\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"J15\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"J16\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"J17\") OR\n",
    "      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"J18\")\n",
    "    )\n",
    ")\n",
    "SELECT\n",
    "  hadm_id,\n",
    "  MAX(IF(STARTS_WITH(code_norm, \"J43\") OR STARTS_WITH(code_norm, \"J44\"), 1, 0)) AS flag_copd,\n",
    "  MAX(IF(STARTS_WITH(code_norm, \"G473\") OR STARTS_WITH(code_norm, \"E662\"), 1, 0)) AS flag_osa_ohs,\n",
    "  MAX(IF(STARTS_WITH(code_norm, \"I50\"), 1, 0)) AS flag_chf,\n",
    "  MAX(IF(STARTS_WITH(code_norm, \"G12\") OR STARTS_WITH(code_norm, \"G70\") OR STARTS_WITH(code_norm, \"G71\"), 1, 0)) AS flag_neuromuscular,\n",
    "  MAX(IF(STARTS_WITH(code_norm, \"F11\") OR STARTS_WITH(code_norm, \"T40\") OR STARTS_WITH(code_norm, \"F13\"), 1, 0)) AS flag_opioid_substance,\n",
    "  MAX(IF(STARTS_WITH(code_norm, \"J12\") OR STARTS_WITH(code_norm, \"J13\") OR STARTS_WITH(code_norm, \"J14\") OR STARTS_WITH(code_norm, \"J15\") OR STARTS_WITH(code_norm, \"J16\") OR STARTS_WITH(code_norm, \"J17\") OR STARTS_WITH(code_norm, \"J18\"), 1, 0)) AS flag_pneumonia\n",
    "FROM ed_dx\n",
    "GROUP BY hadm_id\n",
    "\"\"\"\n",
    "\n",
    "def _run_flag_query_chunked(sql_name: str, label: str) -> pd.DataFrame:\n",
    "    parts = []\n",
    "    hadm_chunk_size = 10000\n",
    "    for i in range(0, len(hadm_list), hadm_chunk_size):\n",
    "        hadm_chunk = hadm_list[i:i + hadm_chunk_size]\n",
    "        part = run_sql_bq(sql(sql_name), {\"hadms\": hadm_chunk})\n",
    "        if len(part) > 0:\n",
    "            parts.append(part)\n",
    "        print(f\"{label} chunk {i // hadm_chunk_size + 1}: rows={len(part)}\")\n",
    "\n",
    "    if parts:\n",
    "        out = pd.concat(parts, ignore_index=True)\n",
    "        out = out.groupby(\"hadm_id\", as_index=False)[FLAGS].max()\n",
    "        return out\n",
    "\n",
    "    return pd.DataFrame(columns=[\"hadm_id\"] + FLAGS)\n",
    "\n",
    "flag_hosp = _run_flag_query_chunked(\"icd_flags_sql\", \"Hosp ICD flags\")\n",
    "flag_ed = _run_flag_query_chunked(\"ed_icd_flags_sql\", \"ED ICD flags\")\n",
    "\n",
    "flag_hosp = flag_hosp.rename(columns={k: f\"{k}_hosp\" for k in FLAGS})\n",
    "flag_ed = flag_ed.rename(columns={k: f\"{k}_ed\" for k in FLAGS})\n",
    "\n",
    "flag_df = flag_hosp.merge(flag_ed, on=\"hadm_id\", how=\"outer\")\n",
    "for k in FLAGS:\n",
    "    hosp_col = f\"{k}_hosp\"\n",
    "    ed_col = f\"{k}_ed\"\n",
    "    if hosp_col not in flag_df.columns:\n",
    "        flag_df[hosp_col] = 0\n",
    "    if ed_col not in flag_df.columns:\n",
    "        flag_df[ed_col] = 0\n",
    "    flag_df[k] = ((flag_df[hosp_col].fillna(0).astype(int) == 1) | (flag_df[ed_col].fillna(0).astype(int) == 1)).astype(int)\n",
    "\n",
    "# Merge into admission-level and ED-stay-level frames (override if present)\n",
    "for _df_name in [\"df\", \"ed_df\"]:\n",
    "    if _df_name in globals():\n",
    "        _df = globals()[_df_name]\n",
    "        drop_cols = [c for c in flag_df.columns if c != \"hadm_id\" and c in _df.columns]\n",
    "        if drop_cols:\n",
    "            _df = _df.drop(columns=drop_cols)\n",
    "        _df = _df.merge(flag_df, on=\"hadm_id\", how=\"left\")\n",
    "        globals()[_df_name] = _df\n",
    "\n",
    "# Prevalence (combined flags)\n",
    "for k in FLAGS:\n",
    "    if k in ed_df.columns:\n",
    "        print(k, int(ed_df[k].fillna(0).sum()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43905020",
   "metadata": {},
   "source": [
    "## Phase 8 — Timing phenotypes and derived bands\n",
    "\n",
    "**Rationale:** Compute time-anchored hypercapnia/acidemia phenotypes for ED presentation vs later course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdc738d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:30:15.465047Z",
     "iopub.status.busy": "2026-02-05T23:30:15.464908Z",
     "iopub.status.idle": "2026-02-05T23:30:15.476013Z",
     "shell.execute_reply": "2026-02-05T23:30:15.475775Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Create time-anchored phenotypes and severity bands relative to first ED presentation.\n",
    "\n",
    "# Timing phenotypes\n",
    "\n",
    "anchor = \"ed_intime_first\" if \"ed_intime_first\" in ed_df.columns else \"ed_intime\"\n",
    "\n",
    "ed_df[\"dt_first_qualifying_gas_hours\"] = (ed_df[\"first_gas_time\"] - ed_df[anchor]).dt.total_seconds() / 3600.0\n",
    "ed_df[\"presenting_hypercapnia\"] = (ed_df[\"dt_first_qualifying_gas_hours\"] <= 6).astype(\"Int64\")\n",
    "ed_df[\"late_hypercapnia\"] = (ed_df[\"dt_first_qualifying_gas_hours\"] > 6).astype(\"Int64\")\n",
    "\n",
    "# NIV/IMV timing relative to first ED presentation\n",
    "if \"first_imv_time\" in ed_df.columns:\n",
    "    ed_df[\"dt_first_imv_hours\"] = (ed_df[\"first_imv_time\"] - ed_df[anchor]).dt.total_seconds() / 3600.0\n",
    "if \"first_niv_time\" in ed_df.columns:\n",
    "    ed_df[\"dt_first_niv_hours\"] = (ed_df[\"first_niv_time\"] - ed_df[anchor]).dt.total_seconds() / 3600.0\n",
    "\n",
    "# Bands\n",
    "bins_ph = [-1, 7.20, 7.30, 7.35, 99]\n",
    "labels_ph = [\"<7.20\",\"7.20–7.29\",\"7.30–7.34\",\"≥7.35\"]\n",
    "ed_df[\"ph_band\"] = pd.cut(ed_df[\"first_ph\"], bins=bins_ph, labels=labels_ph)\n",
    "\n",
    "bins_hco3 = [-1, 24, 30, 999]\n",
    "labels_hco3 = [\"<24\",\"24–29\",\"≥30\"]\n",
    "ed_df[\"hco3_band\"] = pd.cut(ed_df[\"first_hco3\"], bins=bins_hco3, labels=labels_hco3)\n",
    "\n",
    "bins_lac = [-1, 2, 4, 999]\n",
    "labels_lac = [\"<2\",\"2–4\",\">4\"]\n",
    "ed_df[\"lactate_band\"] = pd.cut(ed_df[\"first_lactate\"], bins=bins_lac, labels=labels_lac)\n",
    "\n",
    "\n",
    "\n",
    "# Align ED-stay gas source flags to hadm-level thresholds and ICU POC signal.\n",
    "# This keeps source-specific flags internally consistent in the final ED-stay export.\n",
    "if \"df\" in globals() and \"hadm_id\" in ed_df.columns and \"hadm_id\" in df.columns:\n",
    "    hadm_thr_cols = [c for c in [\"abg_hypercap_threshold\", \"vbg_hypercap_threshold\", \"other_hypercap_threshold\"] if c in df.columns]\n",
    "    if hadm_thr_cols:\n",
    "        hadm_thr = df[[\"hadm_id\"] + hadm_thr_cols].drop_duplicates(\"hadm_id\")\n",
    "        hadm_thr_map = hadm_thr.set_index(\"hadm_id\")\n",
    "\n",
    "        for col in hadm_thr_cols:\n",
    "            mapped = pd.to_numeric(ed_df[\"hadm_id\"].map(hadm_thr_map[col]), errors=\"coerce\")\n",
    "            existing = pd.to_numeric(ed_df[col], errors=\"coerce\") if col in ed_df.columns else pd.Series(np.nan, index=ed_df.index)\n",
    "            ed_df[col] = existing.fillna(mapped).fillna(0).astype(\"Int64\")\n",
    "\n",
    "# Source-specific flags must include any matching hadm-level threshold.\n",
    "flag_specs = [\n",
    "    (\"flag_abg_hypercapnia\", \"abg_hypercap_threshold\"),\n",
    "    (\"flag_vbg_hypercapnia\", \"vbg_hypercap_threshold\"),\n",
    "    (\"flag_other_hypercapnia\", \"other_hypercap_threshold\"),\n",
    "]\n",
    "for flag_col, thr_col in flag_specs:\n",
    "    base = pd.to_numeric(ed_df.get(flag_col, pd.Series(0, index=ed_df.index)), errors=\"coerce\").fillna(0).astype(int)\n",
    "    thr = pd.to_numeric(ed_df.get(thr_col, pd.Series(0, index=ed_df.index)), errors=\"coerce\").fillna(0).astype(int)\n",
    "    ed_df[flag_col] = ((base == 1) | (thr == 1)).astype(\"Int64\")\n",
    "\n",
    "any_from_sources = (\n",
    "    (pd.to_numeric(ed_df.get(\"flag_abg_hypercapnia\", 0), errors=\"coerce\").fillna(0).astype(int) == 1)\n",
    "    | (pd.to_numeric(ed_df.get(\"flag_vbg_hypercapnia\", 0), errors=\"coerce\").fillna(0).astype(int) == 1)\n",
    "    | (pd.to_numeric(ed_df.get(\"flag_other_hypercapnia\", 0), errors=\"coerce\").fillna(0).astype(int) == 1)\n",
    ")\n",
    "\n",
    "poc_any = pd.to_numeric(ed_df.get(\"flag_any_gas_hypercapnia_poc\", pd.Series(0, index=ed_df.index)), errors=\"coerce\").fillna(0).astype(int) == 1\n",
    "ed_df[\"flag_any_gas_hypercapnia\"] = (any_from_sources | poc_any).astype(\"Int64\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d695d3",
   "metadata": {},
   "source": [
    "## QA / audits\n",
    "\n",
    "**Rationale:** Validate joins, missingness, lab completeness, and produce reproducibility artifacts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfa0fe8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:30:15.477341Z",
     "iopub.status.busy": "2026-02-05T23:30:15.477258Z",
     "iopub.status.idle": "2026-02-05T23:30:15.494362Z",
     "shell.execute_reply": "2026-02-05T23:30:15.494150Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Create reusable data-quality and merge guardrail helpers to prevent silent join errors.\n",
    "\n",
    "# --- QA checks (lightweight, deterministic)\n",
    "STRICT_QA = False\n",
    "\n",
    "# 1) Key uniqueness\n",
    "if 'ed_df' in globals() and 'ed_stay_id' in ed_df.columns:\n",
    "    try:\n",
    "        assert_unique(ed_df, 'ed_stay_id', 'ed_df')\n",
    "        print('QA: ed_stay_id unique OK')\n",
    "    except Exception as e:\n",
    "        print('QA FAIL:', e)\n",
    "        if STRICT_QA:\n",
    "            raise\n",
    "\n",
    "# 2) Inclusion sanity: any_hypercap_icd==0 implies gas criteria met\n",
    "if 'ed_df' in globals() and 'any_hypercap_icd' in ed_df.columns:\n",
    "    gas_flag = ed_df.get('flag_any_gas_hypercapnia', pd.Series(0, index=ed_df.index)).fillna(0).astype(int)\n",
    "    icd_flag = ed_df['any_hypercap_icd'].fillna(0).astype(int)\n",
    "    viol = (icd_flag == 0) & (gas_flag == 0)\n",
    "    n_viol = int(viol.sum())\n",
    "    print('QA: ICD==0 & Gas==0 count:', n_viol)\n",
    "    if STRICT_QA and n_viol > 0:\n",
    "        raise AssertionError('Found rows without ICD and without gas criteria.')\n",
    "\n",
    "# 3) Temporal sanity: first_gas_time >= ed_intime (allow small negative drift)\n",
    "if 'ed_df' in globals() and 'first_gas_time' in ed_df.columns and 'ed_intime' in ed_df.columns:\n",
    "    dt = (pd.to_datetime(ed_df['first_gas_time']) - pd.to_datetime(ed_df['ed_intime'])).dt.total_seconds() / 3600\n",
    "    n_neg = int((dt < -1).sum())  # allow 1h clock drift\n",
    "    print('QA: first_gas_time < ed_intime by >1h:', n_neg)\n",
    "    if STRICT_QA and n_neg > 0:\n",
    "        raise AssertionError('first_gas_time before ed_intime by >1h')\n",
    "\n",
    "# 4) Range checks (report only)\n",
    "if 'ed_df' in globals():\n",
    "    ranges = {\n",
    "        'first_ph': (6.8, 7.8),\n",
    "        'first_pco2': (10, 200),\n",
    "        'first_lactate': (0, 30),\n",
    "        'creatinine': (0, 20),\n",
    "    }\n",
    "    rc = check_ranges(ed_df, ranges)\n",
    "    if not rc.empty:\n",
    "        print('QA range check (n out-of-range):')\n",
    "        print(rc.to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef44922",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:30:15.495872Z",
     "iopub.status.busy": "2026-02-05T23:30:15.495783Z",
     "iopub.status.idle": "2026-02-05T23:30:15.535095Z",
     "shell.execute_reply": "2026-02-05T23:30:15.534858Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Report missingness for key variables to support transparent data-quality reporting.\n",
    "\n",
    "import json\n",
    "\n",
    "# T1 — uniqueness and join explosion guard\n",
    "if ed_df[\"ed_stay_id\"].nunique() != len(ed_df):\n",
    "    raise ValueError(\"ed_stay_id not unique after merges\")\n",
    "\n",
    "# T2 — missingness summary for new fields\n",
    "new_fields = [c for c in TARGET_FIELDS if c in ed_df.columns]\n",
    "miss = pd.DataFrame({\n",
    "    \"field\": new_fields,\n",
    "    \"missing_n\": [int(ed_df[c].isna().sum()) for c in new_fields],\n",
    "    \"missing_pct\": [float(ed_df[c].isna().mean()) for c in new_fields],\n",
    "})\n",
    "print(miss.sort_values(\"missing_pct\", ascending=False).head(30))\n",
    "\n",
    "# T3 — lab capture completeness\n",
    "pct_any_6h = float(p06[\"ed_stay_id\"].nunique() / max(ed_df[\"ed_stay_id\"].nunique(),1))\n",
    "pct_any_24h = float(p24[\"ed_stay_id\"].nunique() / max(ed_df[\"ed_stay_id\"].nunique(),1))\n",
    "print(\"% any gas panel 0–6h:\", round(pct_any_6h*100,1))\n",
    "print(\"% any gas panel 0–24h:\", round(pct_any_24h*100,1))\n",
    "\n",
    "# Use per-stay source-other rate if available\n",
    "if \"gas_source_other_rate\" in ed_df.columns:\n",
    "    gas_source_other_rate = float(ed_df[\"gas_source_other_rate\"].mean())\n",
    "else:\n",
    "    gas_source_other_rate = 1.0\n",
    "print(\"% source other (panel-level):\", round(gas_source_other_rate*100,1))\n",
    "\n",
    "hadm_rows = int(len(df)) if \"df\" in globals() else None\n",
    "hadm_cc_rows = None\n",
    "if \"df\" in globals() and \"ed_triage_cc\" in df.columns:\n",
    "    _hadm_cc_mask = df[\"ed_triage_cc\"].notna() & (df[\"ed_triage_cc\"].astype(str).str.strip() != \"\")\n",
    "    hadm_cc_rows = int(_hadm_cc_mask.sum())\n",
    "\n",
    "# T4 — QA summary artifact\n",
    "qa_summary = {\n",
    "    \"ed_spine_rows\": int(len(ed_df)),\n",
    "    \"ed_rows\": int(len(ed_df)),\n",
    "    \"ed_unique\": int(ed_df[\"ed_stay_id\"].nunique()),\n",
    "    \"hadm_rows\": hadm_rows,\n",
    "    \"hadm_cc_rows\": hadm_cc_rows,\n",
    "    \"icu_link_rate\": float(ed_df[\"icu_intime_first\"].notna().mean()) if \"icu_intime_first\" in ed_df.columns else None,\n",
    "    \"pct_any_gas_0_6h\": pct_any_6h,\n",
    "    \"pct_any_gas_0_24h\": pct_any_24h,\n",
    "    \"gas_source_other_rate\": gas_source_other_rate,\n",
    "    \"source_unknown_rate\": gas_source_other_rate,\n",
    "    \"run_metadata\": RUN_METADATA if \"RUN_METADATA\" in globals() else None,\n",
    "}\n",
    "qa_path = WORK_DIR / \"qa_summary.json\"\n",
    "qa_path.write_text(json.dumps(qa_summary, indent=2))\n",
    "print(\"Wrote:\", qa_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4ea668",
   "metadata": {},
   "source": [
    "## Outputs (ED-stay cohort + long tables)\n",
    "\n",
    "**Rationale:** Persist ED-stay analytic datasets and supporting long tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9816b512",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:30:15.536679Z",
     "iopub.status.busy": "2026-02-05T23:30:15.536582Z",
     "iopub.status.idle": "2026-02-05T23:30:16.217007Z",
     "shell.execute_reply": "2026-02-05T23:30:16.216408Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Write finalized cohort outputs and derivative tables for downstream analysis workflows.\n",
    "\n",
    "# Save outputs (dated parquet artifacts archived under prior runs)\n",
    "from datetime import datetime\n",
    "\n",
    "prior_runs_dir = DATA_DIR / \"prior runs\"\n",
    "prior_runs_dir.mkdir(parents=True, exist_ok=True)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "cohort_path = prior_runs_dir / f\"cohort_ed_stay_{timestamp}.parquet\"\n",
    "ed_vitals_path = prior_runs_dir / f\"ed_vitals_long_{timestamp}.parquet\"\n",
    "labs_path = prior_runs_dir / f\"labs_long_{timestamp}.parquet\"\n",
    "gas_panels_path = prior_runs_dir / f\"gas_panels_{timestamp}.parquet\"\n",
    "\n",
    "ed_df.to_parquet(cohort_path, index=False)\n",
    "ed_vitals_long.to_parquet(ed_vitals_path, index=False)\n",
    "labs_long.to_parquet(labs_path, index=False)\n",
    "panel.to_parquet(gas_panels_path, index=False)\n",
    "\n",
    "print(\"Wrote:\", cohort_path)\n",
    "print(\"Wrote:\", ed_vitals_path)\n",
    "print(\"Wrote:\", labs_path)\n",
    "print(\"Wrote:\", gas_panels_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8609ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:30:16.220427Z",
     "iopub.status.busy": "2026-02-05T23:30:16.220297Z",
     "iopub.status.idle": "2026-02-05T23:30:30.630793Z",
     "shell.execute_reply": "2026-02-05T23:30:30.630260Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Report missingness for key variables to support transparent data-quality reporting.\n",
    "\n",
    "# Also export to Excel (tabular dataset) - optional archive output\n",
    "from datetime import datetime\n",
    "\n",
    "WRITE_ARCHIVE_XLSX_EXPORTS = os.getenv(\"WRITE_ARCHIVE_XLSX_EXPORTS\", \"0\") == \"1\"\n",
    "if WRITE_ARCHIVE_XLSX_EXPORTS:\n",
    "    prior_runs_dir = DATA_DIR / \"prior runs\"\n",
    "    prior_runs_dir.mkdir(parents=True, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "    xlsx_path = prior_runs_dir / f\"mimic_hypercap_EXT_EDstay_bq_gas_{timestamp}.xlsx\"\n",
    "\n",
    "    with pd.ExcelWriter(xlsx_path, engine=\"openpyxl\") as xw:\n",
    "        ed_df.to_excel(xw, sheet_name=\"cohort_ed_stay\", index=False)\n",
    "        # Optional: include QA tables if present\n",
    "        try:\n",
    "            miss.to_excel(xw, sheet_name=\"missingness\", index=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    print(\"Saved:\", xlsx_path)\n",
    "else:\n",
    "    print(\"Skipping ED-stay archive workbook export (set WRITE_ARCHIVE_XLSX_EXPORTS=1 to enable).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776739a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:30:30.634641Z",
     "iopub.status.busy": "2026-02-05T23:30:30.634486Z",
     "iopub.status.idle": "2026-02-05T23:31:28.989408Z",
     "shell.execute_reply": "2026-02-05T23:31:28.988401Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Capture ED presentation context (chief complaint, acuity, and early vitals) at the ED-stay level.\n",
    "\n",
    "# Final Excel outputs requested: all encounters + ED chief-complaint only\n",
    "from datetime import datetime\n",
    "\n",
    "prior_runs_dir = DATA_DIR / \"prior runs\"\n",
    "prior_runs_dir.mkdir(parents=True, exist_ok=True)\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# 1) All encounters meeting inclusion criteria (admission-level), with ED linkage flags\n",
    "ed_link = (\n",
    "    ed_df.groupby(\"hadm_id\", as_index=False)\n",
    "         .agg(ed_stay_id_first=(\"ed_stay_id\", \"first\"), n_ed_stays=(\"ed_stay_id\", \"nunique\"))\n",
    ")\n",
    "all_encounters = df.merge(ed_link, on=\"hadm_id\", how=\"left\")\n",
    "all_encounters[\"has_ed_encounter\"] = all_encounters[\"n_ed_stays\"].fillna(0).astype(int).gt(0).astype(int)\n",
    "all_encounters[\"encounter_source\"] = all_encounters[\"has_ed_encounter\"].map({1: \"ED-linked\", 0: \"Inpatient-only\"})\n",
    "\n",
    "if WRITE_ARCHIVE_XLSX_EXPORTS:\n",
    "    all_path = prior_runs_dir / f\"mimic_hypercap_EXT_all_encounters_bq_{timestamp}.xlsx\"\n",
    "    with pd.ExcelWriter(all_path, engine=\"openpyxl\") as xw:\n",
    "        all_encounters.to_excel(xw, sheet_name=\"all_encounters\", index=False)\n",
    "    print(\"Saved:\", all_path)\n",
    "\n",
    "# 2) ED chief-complaint-only (ED-stay level)\n",
    "if \"ed_triage_cc\" not in ed_df.columns:\n",
    "    raise KeyError(\"Column 'ed_triage_cc' not found in ed_df. Ensure ED triage merge ran.\")\n",
    "\n",
    "mask_cc = ed_df[\"ed_triage_cc\"].notna() & (ed_df[\"ed_triage_cc\"].astype(str).str.strip() != \"\")\n",
    "ed_cc_only = ed_df.loc[mask_cc].copy()\n",
    "\n",
    "if WRITE_ARCHIVE_XLSX_EXPORTS:\n",
    "    cc_path = prior_runs_dir / f\"mimic_hypercap_EXT_EDcc_only_edstay_bq_{timestamp}.xlsx\"\n",
    "    with pd.ExcelWriter(cc_path, engine=\"openpyxl\") as xw:\n",
    "        ed_cc_only.to_excel(xw, sheet_name=\"ed_cc_only\", index=False)\n",
    "    print(\"Saved:\", cc_path)\n",
    "else:\n",
    "    print(\"Skipping all-encounters/ED-CC archive exports (set WRITE_ARCHIVE_XLSX_EXPORTS=1 to enable).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbe3ba0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:31:28.993411Z",
     "iopub.status.busy": "2026-02-05T23:31:28.993276Z",
     "iopub.status.idle": "2026-02-05T23:32:25.591931Z",
     "shell.execute_reply": "2026-02-05T23:32:25.590841Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Capture ED presentation context (chief complaint, acuity, and early vitals) at the ED-stay level.\n",
    "\n",
    "# Final CC output (cohort-only; excludes NLP-derived columns)\n",
    "from datetime import datetime\n",
    "\n",
    "# Ensure cc-only frames exist (build from df if upstream optional export cells were skipped)\n",
    "if \"df_cc\" not in globals():\n",
    "    if \"df\" not in globals():\n",
    "        raise NameError(\"df not found. Run cohort assembly cells first.\")\n",
    "    if \"ed_triage_cc\" not in df.columns:\n",
    "        raise KeyError(\"ed_triage_cc missing in df; cannot build df_cc.\")\n",
    "    _mask_df_cc = df[\"ed_triage_cc\"].notna() & (df[\"ed_triage_cc\"].astype(str).str.strip() != \"\")\n",
    "    df_cc = df.loc[_mask_df_cc].copy()\n",
    "\n",
    "if \"ed_cc_only\" not in globals():\n",
    "    if \"ed_df\" not in globals():\n",
    "        raise NameError(\"ed_df not found. Run the ED-stay build cells first.\")\n",
    "    if \"ed_triage_cc\" not in ed_df.columns:\n",
    "        raise KeyError(\"ed_triage_cc missing in ed_df; cannot build ed_cc_only.\")\n",
    "    _mask_cc = ed_df[\"ed_triage_cc\"].notna() & (ed_df[\"ed_triage_cc\"].astype(str).str.strip() != \"\")\n",
    "    ed_cc_only = ed_df.loc[_mask_cc].copy()\n",
    "\n",
    "# Add ED-stay columns (dedup to earliest ED stay per hadm_id)\n",
    "ed_tmp = ed_cc_only.copy()\n",
    "if \"ed_intime\" in ed_tmp.columns:\n",
    "    ed_tmp = ed_tmp.sort_values([\"hadm_id\", \"ed_intime\", \"ed_stay_id\"], na_position=\"last\")\n",
    "else:\n",
    "    ed_tmp = ed_tmp.sort_values([\"hadm_id\", \"ed_stay_id\"], na_position=\"last\")\n",
    "ed_first = ed_tmp.drop_duplicates(\"hadm_id\", keep=\"first\")\n",
    "\n",
    "add_ed_cols = [c for c in ed_first.columns if c not in df_cc.columns]\n",
    "\n",
    "final_cc = df_cc.merge(ed_first[[\"hadm_id\"] + add_ed_cols], on=\"hadm_id\", how=\"left\")\n",
    "\n",
    "prior_runs_dir = DATA_DIR / \"prior runs\"\n",
    "prior_runs_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "dated_out_path = prior_runs_dir / f\"{out_date} MIMICIV all with CC.xlsx\"\n",
    "canonical_out_path = DATA_DIR / \"MIMICIV all with CC.xlsx\"\n",
    "\n",
    "# Write via temporary files then atomically replace targets to avoid zero-byte outputs on interruption.\n",
    "def _atomic_write_excel(df_obj, target_path: Path) -> None:\n",
    "    tmp_path = target_path.with_suffix(target_path.suffix + \".tmp\")\n",
    "    df_obj.to_excel(tmp_path, index=False)\n",
    "    os.replace(tmp_path, target_path)\n",
    "\n",
    "_atomic_write_excel(final_cc, dated_out_path)\n",
    "_atomic_write_excel(final_cc, canonical_out_path)\n",
    "\n",
    "print(\"Saved:\", dated_out_path)\n",
    "print(\"Saved:\", canonical_out_path)\n",
    "print(\"Base rows:\", len(df_cc), \"| Added from ED-stay:\", len(add_ed_cols))\n",
    "print(\"Total columns:\", len(final_cc.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fd8556",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T23:32:25.601138Z",
     "iopub.status.busy": "2026-02-05T23:32:25.600940Z",
     "iopub.status.idle": "2026-02-05T23:32:26.223459Z",
     "shell.execute_reply": "2026-02-05T23:32:26.223240Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: Extract and standardize first ABG/VBG physiology fields for baseline characterization.\n",
    "\n",
    "# Data dictionary (OSF-style) for final cohort output\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Choose target dataset for dictionary\n",
    "if \"final_cc\" in globals():\n",
    "    dd_target = final_cc\n",
    "elif \"df_cc\" in globals():\n",
    "    dd_target = df_cc\n",
    "elif \"df\" in globals():\n",
    "    dd_target = df\n",
    "else:\n",
    "    raise NameError(\"No cohort dataframe found for data dictionary (expected final_cc/df_cc/df).\")\n",
    "\n",
    "# Helper functions\n",
    "\n",
    "def _readable(name: str) -> str:\n",
    "    return name.replace(\"_\", \" \").strip().title()\n",
    "\n",
    "\n",
    "def _infer_units(col: str) -> str:\n",
    "    if col.endswith(\"_hours\"):\n",
    "        return \"hours\"\n",
    "    if col.endswith(\"_time\") or col.endswith(\"_intime\") or col.endswith(\"_outtime\") or col.endswith(\"_date\"):\n",
    "        return \"datetime\"\n",
    "    if col.startswith(\"flag_\") or col.endswith(\"_flag\") or col.endswith(\"_before_imv\"):\n",
    "        return \"binary (0/1)\"\n",
    "    if col.endswith(\"_paco2\") or col.endswith(\"_pco2\"):\n",
    "        return \"mmHg\"\n",
    "    if col.endswith(\"_ph\"):\n",
    "        return \"pH\"\n",
    "    if col.endswith(\"_lactate\"):\n",
    "        return \"mmol/L\"\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def _allowed_values(series):\n",
    "    s = series.dropna()\n",
    "    if s.empty:\n",
    "        return \"\"\n",
    "    if s.dtype.kind in \"biu\":\n",
    "        vals = sorted(map(int, s.unique()))\n",
    "        if len(vals) <= 15:\n",
    "            return \", \".join(map(str, vals))\n",
    "        return f\"{min(vals)}–{max(vals)}\"\n",
    "    if s.dtype.kind in \"f\":\n",
    "        return f\"{np.nanmin(s):.3g}–{np.nanmax(s):.3g}\"\n",
    "    # object/categorical\n",
    "    vals = sorted(map(str, s.unique()))\n",
    "    return \", \".join(vals[:15]) + (\" …\" if len(vals) > 15 else \"\")\n",
    "\n",
    "\n",
    "META = {\n",
    "    # IDs\n",
    "    \"subject_id\": {\n",
    "        \"label\": \"Subject identifier\",\n",
    "        \"definition\": \"Identifier for a unique patient in MIMIC-IV.\",\n",
    "        \"source\": \"mimiciv_hosp.patients\",\n",
    "        \"derivation\": \"as recorded\",\n",
    "    },\n",
    "    \"hadm_id\": {\n",
    "        \"label\": \"Hospital admission identifier\",\n",
    "        \"definition\": \"Identifier for a hospital admission.\",\n",
    "        \"source\": \"mimiciv_hosp.admissions\",\n",
    "        \"derivation\": \"as recorded\",\n",
    "    },\n",
    "    \"ed_stay_id\": {\n",
    "        \"label\": \"ED stay identifier\",\n",
    "        \"definition\": \"Identifier for an ED stay.\",\n",
    "        \"source\": \"mimiciv_ed.edstays\",\n",
    "        \"derivation\": \"as recorded\",\n",
    "    },\n",
    "    \"ed_intime\": {\n",
    "        \"label\": \"ED arrival time\",\n",
    "        \"definition\": \"ED arrival time for this ED stay.\",\n",
    "        \"source\": \"mimiciv_ed.edstays\",\n",
    "        \"derivation\": \"as recorded\",\n",
    "        \"units\": \"datetime\",\n",
    "    },\n",
    "    \"ed_outtime\": {\n",
    "        \"label\": \"ED departure time\",\n",
    "        \"definition\": \"ED departure time for this ED stay.\",\n",
    "        \"source\": \"mimiciv_ed.edstays\",\n",
    "        \"derivation\": \"as recorded\",\n",
    "        \"units\": \"datetime\",\n",
    "    },\n",
    "    \"ed_intime_first\": {\n",
    "        \"label\": \"First ED arrival time (admission)\",\n",
    "        \"definition\": \"Earliest ED arrival time among ED stays linked to the admission.\",\n",
    "        \"source\": \"mimiciv_ed.edstays\",\n",
    "        \"derivation\": \"min(edstays.intime) per hadm_id\",\n",
    "        \"units\": \"datetime\",\n",
    "    },\n",
    "    # Demographics / triage\n",
    "    \"ed_gender\": {\n",
    "        \"label\": \"ED gender\",\n",
    "        \"definition\": \"Gender as recorded in ED stay table.\",\n",
    "        \"source\": \"mimiciv_ed.edstays\",\n",
    "        \"derivation\": \"as recorded\",\n",
    "    },\n",
    "    \"ed_race\": {\n",
    "        \"label\": \"ED race\",\n",
    "        \"definition\": \"Race as recorded in ED stay table.\",\n",
    "        \"source\": \"mimiciv_ed.edstays\",\n",
    "        \"derivation\": \"as recorded\",\n",
    "    },\n",
    "    \"hosp_race\": {\n",
    "        \"label\": \"Hospital race\",\n",
    "        \"definition\": \"Race as recorded in admissions table.\",\n",
    "        \"source\": \"mimiciv_hosp.admissions\",\n",
    "        \"derivation\": \"as recorded\",\n",
    "    },\n",
    "    \"ed_triage_cc\": {\n",
    "        \"label\": \"ED chief complaint\",\n",
    "        \"definition\": \"Chief complaint recorded at ED triage.\",\n",
    "        \"source\": \"mimiciv_ed.triage\",\n",
    "        \"derivation\": \"as recorded\",\n",
    "    },\n",
    "    \"ed_triage_acuity\": {\n",
    "        \"label\": \"ED triage acuity\",\n",
    "        \"definition\": \"Triage acuity level recorded at ED presentation.\",\n",
    "        \"source\": \"mimiciv_ed.triage\",\n",
    "        \"derivation\": \"as recorded\",\n",
    "    },\n",
    "    # Outcomes\n",
    "    \"hospital_expire_flag\": {\n",
    "        \"label\": \"Hospital expire flag\",\n",
    "        \"definition\": \"Hospital mortality indicator from admissions table.\",\n",
    "        \"source\": \"mimiciv_hosp.admissions\",\n",
    "        \"derivation\": \"as recorded\",\n",
    "        \"units\": \"binary (0/1)\",\n",
    "    },\n",
    "    \"in_hospital_death\": {\n",
    "        \"label\": \"In-hospital death\",\n",
    "        \"definition\": \"Indicator for in-hospital death during admission.\",\n",
    "        \"source\": \"mimiciv_hosp.admissions\",\n",
    "        \"derivation\": \"(hospital_expire_flag == 1) OR (deathtime not null)\",\n",
    "        \"units\": \"binary (0/1)\",\n",
    "    },\n",
    "    # Ventilation\n",
    "    \"imv_flag\": {\n",
    "        \"label\": \"IMV flag\",\n",
    "        \"definition\": \"Indicator of invasive mechanical ventilation during admission.\",\n",
    "        \"source\": \"ICD procedures + ICU chartevents\",\n",
    "        \"derivation\": \"max(IMV ICD flag, IMV chart flag)\",\n",
    "        \"units\": \"binary (0/1)\",\n",
    "    },\n",
    "    \"niv_flag\": {\n",
    "        \"label\": \"NIV flag\",\n",
    "        \"definition\": \"Indicator of noninvasive ventilation during admission.\",\n",
    "        \"source\": \"ICD procedures + ICU chartevents\",\n",
    "        \"derivation\": \"max(NIV ICD flag, NIV chart flag)\",\n",
    "        \"units\": \"binary (0/1)\",\n",
    "    },\n",
    "    \"first_imv_time\": {\n",
    "        \"label\": \"First IMV time\",\n",
    "        \"definition\": \"Earliest charted time of IMV in ICU chartevents (if present).\",\n",
    "        \"source\": \"mimiciv_icu.chartevents\",\n",
    "        \"derivation\": \"min charttime among IMV charted events\",\n",
    "        \"units\": \"datetime\",\n",
    "    },\n",
    "    \"first_niv_time\": {\n",
    "        \"label\": \"First NIV time\",\n",
    "        \"definition\": \"Earliest charted time of NIV in ICU chartevents (if present).\",\n",
    "        \"source\": \"mimiciv_icu.chartevents\",\n",
    "        \"derivation\": \"min charttime among NIV charted events\",\n",
    "        \"units\": \"datetime\",\n",
    "    },\n",
    "    \"dt_first_imv_hours\": {\n",
    "        \"label\": \"Hours to first IMV\",\n",
    "        \"definition\": \"Hours from first ED arrival to first IMV time.\",\n",
    "        \"source\": \"Derived\",\n",
    "        \"derivation\": \"(first_imv_time - ed_intime_first) in hours\",\n",
    "        \"units\": \"hours\",\n",
    "    },\n",
    "    \"dt_first_niv_hours\": {\n",
    "        \"label\": \"Hours to first NIV\",\n",
    "        \"definition\": \"Hours from first ED arrival to first NIV time.\",\n",
    "        \"source\": \"Derived\",\n",
    "        \"derivation\": \"(first_niv_time - ed_intime_first) in hours\",\n",
    "        \"units\": \"hours\",\n",
    "    },\n",
    "    \"abg_before_imv\": {\n",
    "        \"label\": \"ABG before IMV\",\n",
    "        \"definition\": \"Indicator that first ABG time precedes first IMV time.\",\n",
    "        \"source\": \"Derived\",\n",
    "        \"derivation\": \"first_abg_time < first_imv_time\",\n",
    "        \"units\": \"binary (0/1)\",\n",
    "    },\n",
    "    \"vbg_before_imv\": {\n",
    "        \"label\": \"VBG before IMV\",\n",
    "        \"definition\": \"Indicator that first VBG time precedes first IMV time.\",\n",
    "        \"source\": \"Derived\",\n",
    "        \"derivation\": \"first_vbg_time < first_imv_time\",\n",
    "        \"units\": \"binary (0/1)\",\n",
    "    },\n",
    "    # Gas/ABG/VBG\n",
    "    \"abg_hypercap_threshold\": {\n",
    "        \"label\": \"ABG hypercapnia threshold met\",\n",
    "        \"definition\": \"Indicator that any arterial pCO2 ≥ 45 mmHg in hosp/ICU pCO2 extraction.\",\n",
    "        \"source\": \"mimiciv_hosp.labevents + mimiciv_icu.chartevents\",\n",
    "        \"derivation\": \"max(arterial pCO2 ≥ 45) per hadm_id\",\n",
    "        \"units\": \"binary (0/1)\",\n",
    "    },\n",
    "    \"vbg_hypercap_threshold\": {\n",
    "        \"label\": \"VBG hypercapnia threshold met\",\n",
    "        \"definition\": \"Indicator that any venous pCO2 ≥ 50 mmHg in hosp/ICU pCO2 extraction.\",\n",
    "        \"source\": \"mimiciv_hosp.labevents + mimiciv_icu.chartevents\",\n",
    "        \"derivation\": \"max(venous pCO2 ≥ 50) per hadm_id\",\n",
    "        \"units\": \"binary (0/1)\",\n",
    "    },\n",
    "    \"other_hypercap_threshold\": {\n",
    "        \"label\": \"Other-source hypercapnia threshold met\",\n",
    "        \"definition\": \"Indicator that any other/unspecified-source pCO2 ≥ 50 mmHg in hosp/ICU pCO2 extraction.\",\n",
    "        \"source\": \"mimiciv_hosp.labevents + mimiciv_icu.chartevents\",\n",
    "        \"derivation\": \"max(other-source pCO2 ≥ 50) per hadm_id\",\n",
    "        \"units\": \"binary (0/1)\",\n",
    "    },\n",
    "    \"first_gas_time\": {\n",
    "        \"label\": \"First gas panel time\",\n",
    "        \"definition\": \"Earliest gas panel time in the ED 0–24h window.\",\n",
    "        \"source\": \"mimiciv_hosp.labevents (gas panels)\",\n",
    "        \"derivation\": \"min panel_time within ED 0–24h window\",\n",
    "        \"units\": \"datetime\",\n",
    "    },\n",
    "    \"dt_first_qualifying_gas_hours\": {\n",
    "        \"label\": \"Hours to first gas panel\",\n",
    "        \"definition\": \"Hours from first ED arrival to first gas panel time.\",\n",
    "        \"source\": \"Derived\",\n",
    "        \"derivation\": \"(first_gas_time - ed_intime_first) in hours\",\n",
    "        \"units\": \"hours\",\n",
    "    },\n",
    "    \"first_other_time\": {\n",
    "        \"label\": \"First other-source gas time\",\n",
    "        \"definition\": \"Earliest time of a pCO2 measurement with other/unspecified source classification.\",\n",
    "        \"source\": \"mimiciv_hosp.labevents + mimiciv_icu.chartevents\",\n",
    "        \"derivation\": \"min(other-source pCO2 charttime) across LAB and POC\",\n",
    "        \"units\": \"datetime\",\n",
    "    },\n",
    "    \"presenting_hypercapnia\": {\n",
    "        \"label\": \"Presenting (≤6h) gas timing\",\n",
    "        \"definition\": \"Indicator that first gas panel occurred within 6h of first ED arrival.\",\n",
    "        \"source\": \"Derived\",\n",
    "        \"derivation\": \"dt_first_qualifying_gas_hours ≤ 6\",\n",
    "        \"units\": \"binary (0/1)\",\n",
    "    },\n",
    "    \"late_hypercapnia\": {\n",
    "        \"label\": \"Late (>6h) gas timing\",\n",
    "        \"definition\": \"Indicator that first gas panel occurred >6h after first ED arrival.\",\n",
    "        \"source\": \"Derived\",\n",
    "        \"derivation\": \"dt_first_qualifying_gas_hours > 6\",\n",
    "        \"units\": \"binary (0/1)\",\n",
    "    },\n",
    "    \"gas_source_other_rate\": {\n",
    "        \"label\": \"Gas source other/unknown rate\",\n",
    "        \"definition\": \"Proportion of gas panels classified as other/unspecified source per ED stay.\",\n",
    "        \"source\": \"Derived from gas panel source inference\",\n",
    "        \"derivation\": \"mean(source == 'other') per ed_stay_id\",\n",
    "        \"units\": \"proportion\",\n",
    "    },\n",
    "    # Comorbidities\n",
    "    \"flag_copd\": {\n",
    "        \"label\": \"COPD/emphysema flag\",\n",
    "        \"definition\": \"Indicator for COPD/emphysema ICD codes in ED or hospital diagnoses.\",\n",
    "        \"source\": \"mimiciv_hosp.diagnoses_icd + mimiciv_ed.diagnosis\",\n",
    "        \"derivation\": \"ICD prefixes J43/J44 (ED OR hospital)\",\n",
    "        \"units\": \"binary (0/1)\",\n",
    "    },\n",
    "    \"flag_osa_ohs\": {\n",
    "        \"label\": \"OSA/OHS flag\",\n",
    "        \"definition\": \"Indicator for OSA/OHS ICD codes in ED or hospital diagnoses.\",\n",
    "        \"source\": \"mimiciv_hosp.diagnoses_icd + mimiciv_ed.diagnosis\",\n",
    "        \"derivation\": \"ICD prefixes G473/E662 (ED OR hospital)\",\n",
    "        \"units\": \"binary (0/1)\",\n",
    "    },\n",
    "    \"flag_chf\": {\n",
    "        \"label\": \"CHF flag\",\n",
    "        \"definition\": \"Indicator for CHF ICD codes in ED or hospital diagnoses.\",\n",
    "        \"source\": \"mimiciv_hosp.diagnoses_icd + mimiciv_ed.diagnosis\",\n",
    "        \"derivation\": \"ICD prefix I50 (ED OR hospital)\",\n",
    "        \"units\": \"binary (0/1)\",\n",
    "    },\n",
    "    \"flag_neuromuscular\": {\n",
    "        \"label\": \"Neuromuscular flag\",\n",
    "        \"definition\": \"Indicator for neuromuscular ICD codes in ED or hospital diagnoses.\",\n",
    "        \"source\": \"mimiciv_hosp.diagnoses_icd + mimiciv_ed.diagnosis\",\n",
    "        \"derivation\": \"ICD prefixes G12/G70/G71 (ED OR hospital)\",\n",
    "        \"units\": \"binary (0/1)\",\n",
    "    },\n",
    "    \"flag_opioid_substance\": {\n",
    "        \"label\": \"Opioid/substance flag\",\n",
    "        \"definition\": \"Indicator for opioid/substance ICD codes in ED or hospital diagnoses.\",\n",
    "        \"source\": \"mimiciv_hosp.diagnoses_icd + mimiciv_ed.diagnosis\",\n",
    "        \"derivation\": \"ICD prefixes F11/T40/F13 (ED OR hospital)\",\n",
    "        \"units\": \"binary (0/1)\",\n",
    "    },\n",
    "    \"flag_pneumonia\": {\n",
    "        \"label\": \"Pneumonia flag\",\n",
    "        \"definition\": \"Indicator for pneumonia ICD codes in ED or hospital diagnoses.\",\n",
    "        \"source\": \"mimiciv_hosp.diagnoses_icd + mimiciv_ed.diagnosis\",\n",
    "        \"derivation\": \"ICD prefixes J12–J18 (ED OR hospital)\",\n",
    "        \"units\": \"binary (0/1)\",\n",
    "    },\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for col in dd_target.columns:\n",
    "    info = META.get(col, {})\n",
    "    series = dd_target[col]\n",
    "    rows.append({\n",
    "        \"variable\": col,\n",
    "        \"label\": info.get(\"label\", _readable(col)),\n",
    "        \"units\": info.get(\"units\", _infer_units(col)),\n",
    "        \"allowed_values\": info.get(\"allowed_values\", _allowed_values(series)),\n",
    "        \"definition\": info.get(\"definition\", \"UNCONFIRMED\"),\n",
    "        \"synonyms\": info.get(\"synonyms\", \"\"),\n",
    "        \"description\": info.get(\"description\", \"\"),\n",
    "        \"source\": info.get(\"source\", \"UNCONFIRMED\"),\n",
    "        \"derivation\": info.get(\"derivation\", \"UNCONFIRMED\"),\n",
    "        \"dtype\": str(series.dtype),\n",
    "        \"example_value\": series.dropna().iloc[0] if series.dropna().shape[0] else None,\n",
    "    })\n",
    "\n",
    "data_dict = pd.DataFrame(rows)\n",
    "\n",
    "out_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "prior_runs_dir = DATA_DIR / \"prior runs\"\n",
    "prior_runs_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_xlsx = prior_runs_dir / f\"{out_date} MIMICIV all with CC_data_dictionary.xlsx\"\n",
    "out_csv = prior_runs_dir / f\"{out_date} MIMICIV all with CC_data_dictionary.csv\"\n",
    "\n",
    "data_dict.to_excel(out_xlsx, index=False)\n",
    "data_dict.to_csv(out_csv, index=False)\n",
    "\n",
    "print(\"Saved data dictionary:\", out_xlsx)\n",
    "print(\"Saved data dictionary:\", out_csv)\n",
    "print(\"Rows:\", len(data_dict))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hypercap-CC-NLP (3.11.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
