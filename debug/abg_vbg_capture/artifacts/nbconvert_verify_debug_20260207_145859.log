[NbConvertApp] Searching ['/Users/blocke/.jupyter', '/Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/.venv/etc/jupyter', '/usr/local/etc/jupyter', '/etc/jupyter'] for config files
[NbConvertApp] Looking for jupyter_config in /etc/jupyter
[NbConvertApp] Looking for jupyter_config in /usr/local/etc/jupyter
[NbConvertApp] Looking for jupyter_config in /Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/.venv/etc/jupyter
[NbConvertApp] Looking for jupyter_config in /Users/blocke/.jupyter
[NbConvertApp] Looking for jupyter_nbconvert_config in /etc/jupyter
[NbConvertApp] Looking for jupyter_nbconvert_config in /usr/local/etc/jupyter
[NbConvertApp] Looking for jupyter_nbconvert_config in /Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/.venv/etc/jupyter
[NbConvertApp] Looking for jupyter_nbconvert_config in /Users/blocke/.jupyter
[NbConvertApp] Looping through config variables with prefix "JUPYTER_NBCONVERT"
[NbConvertApp] Converting notebook MIMICIV_hypercap_EXT_cohort.ipynb to notebook
[NbConvertApp] Notebook name is 'MIMICIV_hypercap_EXT_cohort'
[NbConvertApp] Applying preprocessor: ClearOutputPreprocessor
[NbConvertApp] Applying preprocessor: ExecutePreprocessor
[NbConvertApp] Instantiating kernel 'Python (hypercap-cc-nlp)' with kernel provisioner: local-provisioner
[NbConvertApp] Starting kernel: ['/Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/.venv/bin/python', '-Xfrozen_modules=off', '-m', 'ipykernel_launcher', '-f', '/private/var/folders/vf/n84t7b8171lf64smq4ddvw1m0000gn/T/tmp7_sov3qc.json']
[NbConvertApp] Connecting to: tcp://127.0.0.1:57831
[NbConvertApp] connecting iopub channel to tcp://127.0.0.1:57828
[NbConvertApp] Connecting to: tcp://127.0.0.1:57828
[NbConvertApp] connecting shell channel to tcp://127.0.0.1:57827
[NbConvertApp] Connecting to: tcp://127.0.0.1:57827
[NbConvertApp] connecting stdin channel to tcp://127.0.0.1:57829
[NbConvertApp] Connecting to: tcp://127.0.0.1:57829
[NbConvertApp] connecting heartbeat channel to tcp://127.0.0.1:57830
[NbConvertApp] connecting control channel to tcp://127.0.0.1:57831
[NbConvertApp] Connecting to: tcp://127.0.0.1:57831
[NbConvertApp] Skipping non-executing cell 0
[NbConvertApp] Skipping non-executing cell 1
[NbConvertApp] Skipping non-executing cell 2
[NbConvertApp] Skipping non-executing cell 3
[NbConvertApp] Executing cell:
# Purpose: Build ABG/VBG hypercapnia threshold flags from lab and ICU POC pCO2 measurements.

import sys
print(sys.executable)

# Central SQL registry (define all query templates here)
SQL = {}

def sql(name: str) -> str:
    if name not in SQL:
        raise KeyError(f"SQL template not found: {name}")
    return SQL[name]

# SQL templates (populated below in-place to keep notebook linear)
# Names: admit_sql, co2_thresholds_sql, cohort_icd_sql, counts_sql, demo_sql, ditems_sql, ed_counts_sql, ed_first_vitals_sql, ed_spine_sql, ed_to_icu_sql, ed_triage_sql, ed_vitals_sql, icd_sql, icu_meta_sql, icu_sql, labitems_sql, labs_sql, vent_chart_sql, vent_sql

[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Purpose: Build ABG/VBG hypercapnia threshold flags from lab and ICU POC pCO2 measurements.\n\nimport sys\nprint(sys.executable)\n\n# Central SQL registry (define all query templates here)\nSQL = {}\n\ndef sql(name: str) -> str:\n    if name not in SQL:\n        raise KeyError(f"SQL template not found: {name}")\n    return SQL[name]\n\n# SQL templates (populated below in-place to keep notebook linear)\n# Names: admit_sql, co2_thresholds_sql, cohort_icd_sql, counts_sql, demo_sql, ditems_sql, ed_counts_sql, ed_first_vitals_sql, ed_spine_sql, ed_to_icu_sql, ed_triage_sql, ed_vitals_sql, icd_sql, icu_meta_sql, icu_sql, labitems_sql, labs_sql, vent_chart_sql, vent_sql\n', 'execution_count': 1}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '/Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/.venv/bin/python\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 5
[NbConvertApp] Skipping non-executing cell 6
[NbConvertApp] Skipping non-executing cell 7
[NbConvertApp] Skipping non-executing cell 8
[NbConvertApp] Skipping non-executing cell 9
[NbConvertApp] Executing cell:
# Purpose: Set up project paths, environment variables, and BigQuery client connections for reproducible execution.

# --- Imports & environment
import os, re, json, math, textwrap
from pathlib import Path

import numpy as np
import pandas as pd

from google.cloud import bigquery
from google.oauth2 import service_account
from dotenv import load_dotenv

load_dotenv()

WORK_DIR = Path(os.getenv("WORK_DIR", Path.cwd())).expanduser().resolve()
DATA_DIR = WORK_DIR / "MIMIC tabular data"
DATA_DIR.mkdir(parents=True, exist_ok=True)

# ---- Backend selection (we use BigQuery)
BACKEND = os.getenv("MIMIC_BACKEND", "bigquery").strip().lower()
assert BACKEND == "bigquery", "This notebook is BigQuery-specific."

WORK_PROJECT = os.getenv("WORK_PROJECT", "").strip()  # your billing project
PHYS = os.getenv("BQ_PHYSIONET_PROJECT", "physionet-data").strip()  # hosting project (read-only)

# Dataset preferences: resolved to accessible datasets in the next setup cell.
HOSP = os.getenv("BQ_DATASET_HOSP", "mimiciv_3_1_hosp").strip()
ICU  = os.getenv("BQ_DATASET_ICU",  "mimiciv_3_1_icu").strip()
ED   = os.getenv("BQ_DATASET_ED",   "").strip()

# BigQuery client
client = bigquery.Client(project=WORK_PROJECT)

print("Project:", WORK_PROJECT)
print("PhysioNet host:", PHYS)
print("HOSP (pref):", HOSP, "ICU (pref):", ICU, "ED (pref):", ED)
print("WORK_DIR:", WORK_DIR)

[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Purpose: Set up project paths, environment variables, and BigQuery client connections for reproducible execution.\n\n# --- Imports & environment\nimport os, re, json, math, textwrap\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nfrom google.cloud import bigquery\nfrom google.oauth2 import service_account\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nWORK_DIR = Path(os.getenv("WORK_DIR", Path.cwd())).expanduser().resolve()\nDATA_DIR = WORK_DIR / "MIMIC tabular data"\nDATA_DIR.mkdir(parents=True, exist_ok=True)\n\n# ---- Backend selection (we use BigQuery)\nBACKEND = os.getenv("MIMIC_BACKEND", "bigquery").strip().lower()\nassert BACKEND == "bigquery", "This notebook is BigQuery-specific."\n\nWORK_PROJECT = os.getenv("WORK_PROJECT", "").strip()  # your billing project\nPHYS = os.getenv("BQ_PHYSIONET_PROJECT", "physionet-data").strip()  # hosting project (read-only)\n\n# Dataset preferences: resolved to accessible datasets in the next setup cell.\nHOSP = os.getenv("BQ_DATASET_HOSP", "mimiciv_3_1_hosp").strip()\nICU  = os.getenv("BQ_DATASET_ICU",  "mimiciv_3_1_icu").strip()\nED   = os.getenv("BQ_DATASET_ED",   "").strip()\n\n# BigQuery client\nclient = bigquery.Client(project=WORK_PROJECT)\n\nprint("Project:", WORK_PROJECT)\nprint("PhysioNet host:", PHYS)\nprint("HOSP (pref):", HOSP, "ICU (pref):", ICU, "ED (pref):", ED)\nprint("WORK_DIR:", WORK_DIR)\n', 'execution_count': 2}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Project: mimic-hypercapnia\nPhysioNet host: physionet-data\nHOSP (pref): mimiciv_3_1_hosp ICU (pref): mimiciv_3_1_icu ED (pref): mimiciv_ed\nWORK_DIR: /Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
# Purpose: Define reusable BigQuery helpers and resolve dataset names across known naming variants.

from datetime import datetime, timezone

# Fail fast on long-running queries instead of hanging indefinitely in nbconvert.
BQ_QUERY_TIMEOUT_SECS = int(os.getenv("BQ_QUERY_TIMEOUT_SECS", "1800"))

# --- Helper: run SQL with optional named parameters
def run_sql_bq(sql: str, params: dict | None = None) -> pd.DataFrame:
    job_config = bigquery.QueryJobConfig()
    job_config.labels = {"pipeline": "hypercapcc", "notebook": "cohort"}
    if params:
        bq_params = []
        for k, v in params.items():
            if isinstance(v, (list, tuple, np.ndarray, pd.Series)):
                # BigQuery ARRAY<INT64> if all ints; else ARRAY<STRING>
                v_list = list(v)
                if all(isinstance(x, (int, np.integer)) for x in v_list):
                    bq_params.append(bigquery.ArrayQueryParameter(k, "INT64", list(map(int, v_list))))
                else:
                    bq_params.append(bigquery.ArrayQueryParameter(k, "STRING", list(map(str, v_list))))
            else:
                # scalar
                if isinstance(v, (int, np.integer)):
                    bq_params.append(bigquery.ScalarQueryParameter(k, "INT64", int(v)))
                elif isinstance(v, float):
                    bq_params.append(bigquery.ScalarQueryParameter(k, "FLOAT64", float(v)))
                else:
                    bq_params.append(bigquery.ScalarQueryParameter(k, "STRING", str(v)))
        job_config.query_parameters = bq_params

    job = client.query(sql, job_config=job_config)
    try:
        result = job.result(timeout=BQ_QUERY_TIMEOUT_SECS)
    except Exception as exc:
        try:
            job.cancel()
        except Exception:
            pass
        raise RuntimeError(
            f"BigQuery query failed or timed out after {BQ_QUERY_TIMEOUT_SECS}s (job_id={job.job_id})."
        ) from exc

    try:
        return result.to_dataframe(create_bqstorage_client=True)
    except TypeError:
        return result.to_dataframe()

# --- Helper: test if a fully-qualified table exists and is accessible
def table_exists(fqtn: str) -> bool:
    try:
        _ = run_sql_bq(f"SELECT 1 FROM `{fqtn}` LIMIT 1")
        return True
    except Exception:
        return False

def resolve_dataset(preferred: str, candidates: list[str], probe_table: str, label: str) -> tuple[str, str]:
    # Keep preferred first, then try known aliases.
    ordered = []
    if preferred:
        ordered.append(preferred)
    for cand in candidates:
        if cand not in ordered:
            ordered.append(cand)

    for dataset in ordered:
        fqtn = f"{PHYS}.{dataset}.{probe_table}"
        if table_exists(fqtn):
            return dataset, fqtn

    raise RuntimeError(
        f"No accessible {label} dataset found for probe table '{probe_table}'. Tried: {ordered}"
    )

# Resolve HOSP/ICU/ED names so notebook runs across v3.1 naming variants.
HOSP, HOSP_PROBE = resolve_dataset(
    preferred=HOSP,
    candidates=["mimiciv_3_1_hosp", "mimiciv_v3_1_hosp", "mimiciv_hosp"],
    probe_table="admissions",
    label="HOSP",
)
ICU, ICU_PROBE = resolve_dataset(
    preferred=ICU,
    candidates=["mimiciv_3_1_icu", "mimiciv_v3_1_icu", "mimiciv_icu"],
    probe_table="icustays",
    label="ICU",
)
if not ED:
    ED = "mimiciv_ed"
ED, ED_PROBE = resolve_dataset(
    preferred=ED,
    candidates=["mimiciv_ed", "mimiciv_3_1_ed", "mimiciv_v3_1_ed"],
    probe_table="edstays",
    label="ED",
)

RUN_METADATA = {
    "run_utc": datetime.now(timezone.utc).isoformat(),
    "work_project": WORK_PROJECT,
    "physionet_project": PHYS,
    "datasets": {"hosp": HOSP, "icu": ICU, "ed": ED},
    "dataset_probes": {"hosp": HOSP_PROBE, "icu": ICU_PROBE, "ed": ED_PROBE},
    "notebook": "MIMICIV_hypercap_EXT_cohort.ipynb",
}

print("Resolved datasets:", RUN_METADATA["datasets"])
print("Dataset probes:", RUN_METADATA["dataset_probes"])

[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Purpose: Define reusable BigQuery helpers and resolve dataset names across known naming variants.\n\nfrom datetime import datetime, timezone\n\n# Fail fast on long-running queries instead of hanging indefinitely in nbconvert.\nBQ_QUERY_TIMEOUT_SECS = int(os.getenv("BQ_QUERY_TIMEOUT_SECS", "1800"))\n\n# --- Helper: run SQL with optional named parameters\ndef run_sql_bq(sql: str, params: dict | None = None) -> pd.DataFrame:\n    job_config = bigquery.QueryJobConfig()\n    job_config.labels = {"pipeline": "hypercapcc", "notebook": "cohort"}\n    if params:\n        bq_params = []\n        for k, v in params.items():\n            if isinstance(v, (list, tuple, np.ndarray, pd.Series)):\n                # BigQuery ARRAY<INT64> if all ints; else ARRAY<STRING>\n                v_list = list(v)\n                if all(isinstance(x, (int, np.integer)) for x in v_list):\n                    bq_params.append(bigquery.ArrayQueryParameter(k, "INT64", list(map(int, v_list))))\n                else:\n                    bq_params.append(bigquery.ArrayQueryParameter(k, "STRING", list(map(str, v_list))))\n            else:\n                # scalar\n                if isinstance(v, (int, np.integer)):\n                    bq_params.append(bigquery.ScalarQueryParameter(k, "INT64", int(v)))\n                elif isinstance(v, float):\n                    bq_params.append(bigquery.ScalarQueryParameter(k, "FLOAT64", float(v)))\n                else:\n                    bq_params.append(bigquery.ScalarQueryParameter(k, "STRING", str(v)))\n        job_config.query_parameters = bq_params\n\n    job = client.query(sql, job_config=job_config)\n    try:\n        result = job.result(timeout=BQ_QUERY_TIMEOUT_SECS)\n    except Exception as exc:\n        try:\n            job.cancel()\n        except Exception:\n            pass\n        raise RuntimeError(\n            f"BigQuery query failed or timed out after {BQ_QUERY_TIMEOUT_SECS}s (job_id={job.job_id})."\n        ) from exc\n\n    try:\n        return result.to_dataframe(create_bqstorage_client=True)\n    except TypeError:\n        return result.to_dataframe()\n\n# --- Helper: test if a fully-qualified table exists and is accessible\ndef table_exists(fqtn: str) -> bool:\n    try:\n        _ = run_sql_bq(f"SELECT 1 FROM `{fqtn}` LIMIT 1")\n        return True\n    except Exception:\n        return False\n\ndef resolve_dataset(preferred: str, candidates: list[str], probe_table: str, label: str) -> tuple[str, str]:\n    # Keep preferred first, then try known aliases.\n    ordered = []\n    if preferred:\n        ordered.append(preferred)\n    for cand in candidates:\n        if cand not in ordered:\n            ordered.append(cand)\n\n    for dataset in ordered:\n        fqtn = f"{PHYS}.{dataset}.{probe_table}"\n        if table_exists(fqtn):\n            return dataset, fqtn\n\n    raise RuntimeError(\n        f"No accessible {label} dataset found for probe table \'{probe_table}\'. Tried: {ordered}"\n    )\n\n# Resolve HOSP/ICU/ED names so notebook runs across v3.1 naming variants.\nHOSP, HOSP_PROBE = resolve_dataset(\n    preferred=HOSP,\n    candidates=["mimiciv_3_1_hosp", "mimiciv_v3_1_hosp", "mimiciv_hosp"],\n    probe_table="admissions",\n    label="HOSP",\n)\nICU, ICU_PROBE = resolve_dataset(\n    preferred=ICU,\n    candidates=["mimiciv_3_1_icu", "mimiciv_v3_1_icu", "mimiciv_icu"],\n    probe_table="icustays",\n    label="ICU",\n)\nif not ED:\n    ED = "mimiciv_ed"\nED, ED_PROBE = resolve_dataset(\n    preferred=ED,\n    candidates=["mimiciv_ed", "mimiciv_3_1_ed", "mimiciv_v3_1_ed"],\n    probe_table="edstays",\n    label="ED",\n)\n\nRUN_METADATA = {\n    "run_utc": datetime.now(timezone.utc).isoformat(),\n    "work_project": WORK_PROJECT,\n    "physionet_project": PHYS,\n    "datasets": {"hosp": HOSP, "icu": ICU, "ed": ED},\n    "dataset_probes": {"hosp": HOSP_PROBE, "icu": ICU_PROBE, "ed": ED_PROBE},\n    "notebook": "MIMICIV_hypercap_EXT_cohort.ipynb",\n}\n\nprint("Resolved datasets:", RUN_METADATA["datasets"])\nprint("Dataset probes:", RUN_METADATA["dataset_probes"])\n', 'execution_count': 3}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': "Resolved datasets: {'hosp': 'mimiciv_3_1_hosp', 'icu': 'mimiciv_3_1_icu', 'ed': 'mimiciv_ed'}\nDataset probes: {'hosp': 'physionet-data.mimiciv_3_1_hosp.admissions', 'icu': 'physionet-data.mimiciv_3_1_icu.icustays', 'ed': 'physionet-data.mimiciv_ed.edstays'}\n"}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
# Purpose: Create reusable data-quality and merge guardrail helpers to prevent silent join errors.

# --- Helper utilities for reproducibility and safe joins
def require_cols(df: pd.DataFrame, cols: list[str], name: str) -> None:
    missing = [c for c in cols if c not in df.columns]
    if missing:
        raise KeyError(f"{name} missing columns: {missing}")

def assert_unique(df: pd.DataFrame, key: str, name: str) -> None:
    if df[key].duplicated().any():
        n = int(df[key].duplicated().sum())
        raise ValueError(f"{name} has {n} duplicate {key} values")

def safe_merge(left: pd.DataFrame, right: pd.DataFrame, on: list[str] | str, how: str, name: str) -> pd.DataFrame:
    # guard against accidental duplicate columns
    overlap = set(left.columns) & set(right.columns)
    if isinstance(on, str):
        on_cols = {on}
    else:
        on_cols = set(on)
    overlap = overlap - on_cols
    if overlap:
        raise ValueError(f"{name} merge would duplicate columns: {sorted(overlap)}")
    return left.merge(right, on=on, how=how)

def check_ranges(df: pd.DataFrame, ranges: dict[str, tuple[float, float]]) -> pd.DataFrame:
    rows = []
    for col, (lo, hi) in ranges.items():
        if col not in df.columns:
            continue
        bad = df[col].notna() & ((df[col] < lo) | (df[col] > hi))
        rows.append({"col": col, "n_bad": int(bad.sum())})
    return pd.DataFrame(rows)


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Purpose: Create reusable data-quality and merge guardrail helpers to prevent silent join errors.\n\n# --- Helper utilities for reproducibility and safe joins\ndef require_cols(df: pd.DataFrame, cols: list[str], name: str) -> None:\n    missing = [c for c in cols if c not in df.columns]\n    if missing:\n        raise KeyError(f"{name} missing columns: {missing}")\n\ndef assert_unique(df: pd.DataFrame, key: str, name: str) -> None:\n    if df[key].duplicated().any():\n        n = int(df[key].duplicated().sum())\n        raise ValueError(f"{name} has {n} duplicate {key} values")\n\ndef safe_merge(left: pd.DataFrame, right: pd.DataFrame, on: list[str] | str, how: str, name: str) -> pd.DataFrame:\n    # guard against accidental duplicate columns\n    overlap = set(left.columns) & set(right.columns)\n    if isinstance(on, str):\n        on_cols = {on}\n    else:\n        on_cols = set(on)\n    overlap = overlap - on_cols\n    if overlap:\n        raise ValueError(f"{name} merge would duplicate columns: {sorted(overlap)}")\n    return left.merge(right, on=on, how=how)\n\ndef check_ranges(df: pd.DataFrame, ranges: dict[str, tuple[float, float]]) -> pd.DataFrame:\n    rows = []\n    for col, (lo, hi) in ranges.items():\n        if col not in df.columns:\n            continue\n        bad = df[col].notna() & ((df[col] < lo) | (df[col] > hi))\n        rows.append({"col": col, "n_bad": int(bad.sum())})\n    return pd.DataFrame(rows)\n\n', 'execution_count': 4}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 13
[NbConvertApp] Skipping non-executing cell 14
[NbConvertApp] Executing cell:
# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.

# Target ICD codes (dotless, uppercase)
ICD10_CODES = ['J9602','J9612','J9622','J9692','E662']
ICD9_CODES  = ['27803']

SQL["cohort_icd_sql"] = f"""
-- ICD-based cohort flags per admission
WITH target_codes AS (
  SELECT 'J9602' AS code, 10 AS ver UNION ALL
  SELECT 'J9612', 10 UNION ALL
  SELECT 'J9622', 10 UNION ALL
  SELECT 'J9692', 10 UNION ALL
  SELECT 'E662',  10 UNION ALL
  SELECT '27803', 9
),

-- Hospital ICDs restricted to target codes
hosp_dx AS (
  SELECT
    d.subject_id,
    d.hadm_id,
    UPPER(REPLACE(d.icd_code, '.', '')) AS code_norm,
    d.icd_version
  FROM `{PHYS}.{HOSP}.diagnoses_icd` d
  JOIN target_codes t
    ON t.ver = d.icd_version AND t.code = UPPER(REPLACE(d.icd_code, '.', ''))
  WHERE d.hadm_id IS NOT NULL
),

-- Hospital flags per admission
hosp_flags AS (
  SELECT
    subject_id, hadm_id,
    MAX(IF(icd_version=10 AND code_norm='J9602',1,0)) AS ICD10_J9602,
    MAX(IF(icd_version=10 AND code_norm='J9612',1,0)) AS ICD10_J9612,
    MAX(IF(icd_version=10 AND code_norm='J9622',1,0)) AS ICD10_J9622,
    MAX(IF(icd_version=10 AND code_norm='J9692',1,0)) AS ICD10_J9692,
    MAX(IF(icd_version=10 AND code_norm='E662', 1,0)) AS ICD10_E662,
    MAX(IF(icd_version=9  AND code_norm='27803',1,0)) AS ICD9_27803
  FROM hosp_dx
  GROUP BY subject_id, hadm_id
),

-- ED ICDs restricted to target codes (map to hadm via edstays)
ed_dx AS (
  SELECT
    s.subject_id,
    s.hadm_id,
    s.stay_id,
    s.intime AS ed_intime,
    UPPER(REPLACE(d.icd_code, '.', '')) AS code_norm,
    d.icd_version
  FROM `{PHYS}.{ED}.diagnosis` d
  JOIN `{PHYS}.{ED}.edstays` s
    ON s.subject_id = d.subject_id AND s.stay_id = d.stay_id
  JOIN target_codes t
    ON t.ver = d.icd_version AND t.code = UPPER(REPLACE(d.icd_code, '.', ''))
  WHERE s.hadm_id IS NOT NULL
),

-- ED flags per ED stay (so we can both: OR flags across stays and also pick earliest stay_id)
ed_flags_by_stay AS (
  SELECT
    subject_id, hadm_id, stay_id, MIN(ed_intime) AS ed_intime,
    MAX(IF(icd_version=10 AND code_norm='J9602',1,0)) AS ICD10_J9602,
    MAX(IF(icd_version=10 AND code_norm='J9612',1,0)) AS ICD10_J9612,
    MAX(IF(icd_version=10 AND code_norm='J9622',1,0)) AS ICD10_J9622,
    MAX(IF(icd_version=10 AND code_norm='J9692',1,0)) AS ICD10_J9692,
    MAX(IF(icd_version=10 AND code_norm='E662', 1,0)) AS ICD10_E662,
    MAX(IF(icd_version=9  AND code_norm='27803',1,0)) AS ICD9_27803
  FROM ed_dx
  GROUP BY subject_id, hadm_id, stay_id
),

-- OR the ED flags across all ED stays mapped to the same hadm
ed_flags_or AS (
  SELECT
    subject_id, hadm_id,
    MAX(ICD10_J9602) AS ICD10_J9602,
    MAX(ICD10_J9612) AS ICD10_J9612,
    MAX(ICD10_J9622) AS ICD10_J9622,
    MAX(ICD10_J9692) AS ICD10_J9692,
    MAX(ICD10_E662 ) AS ICD10_E662,
    MAX(ICD9_27803) AS ICD9_27803
  FROM ed_flags_by_stay
  GROUP BY subject_id, hadm_id
),

-- Earliest ED stay_id per hadm (NO UNNEST of aggregates; use [OFFSET(0)])
ed_earliest AS (
  SELECT
    subject_id,
    hadm_id,
    (ARRAY_AGG(STRUCT(stay_id, ed_intime) ORDER BY ed_intime LIMIT 1))[OFFSET(0)].stay_id AS stay_id
  FROM ed_flags_by_stay
  GROUP BY subject_id, hadm_id
),

-- Bring flags and earliest stay_id together
ed_by_hadm AS (
  SELECT
    f.subject_id,
    f.hadm_id,
    e.stay_id,
    f.ICD10_J9602,
    f.ICD10_J9612,
    f.ICD10_J9622,
    f.ICD10_J9692,
    f.ICD10_E662,
    f.ICD9_27803
  FROM ed_flags_or f
  LEFT JOIN ed_earliest e
    USING (subject_id, hadm_id)
),

-- Combine ED and hospital flags at the admission level
combined AS (
  SELECT
    COALESCE(h.subject_id, e.subject_id) AS subject_id,
    COALESCE(h.hadm_id,     e.hadm_id)   AS hadm_id,
    GREATEST(IFNULL(h.ICD10_J9602,0), IFNULL(e.ICD10_J9602,0)) AS ICD10_J9602,
    GREATEST(IFNULL(h.ICD10_J9612,0), IFNULL(e.ICD10_J9612,0)) AS ICD10_J9612,
    GREATEST(IFNULL(h.ICD10_J9622,0), IFNULL(e.ICD10_J9622,0)) AS ICD10_J9622,
    GREATEST(IFNULL(h.ICD10_J9692,0), IFNULL(e.ICD10_J9692,0)) AS ICD10_J9692,
    GREATEST(IFNULL(h.ICD10_E662 ,0), IFNULL(e.ICD10_E662 ,0)) AS ICD10_E662,
    GREATEST(IFNULL(h.ICD9_27803,0), IFNULL(e.ICD9_27803,0)) AS ICD9_27803,
    IF((IFNULL(h.ICD10_J9602,0)+IFNULL(h.ICD10_J9612,0)+IFNULL(h.ICD10_J9622,0)+IFNULL(h.ICD10_J9692,0)+IFNULL(h.ICD10_E662,0)+IFNULL(h.ICD9_27803,0)) > 0, 1, 0) AS any_hypercap_icd_hosp,
    IF((IFNULL(e.ICD10_J9602,0)+IFNULL(e.ICD10_J9612,0)+IFNULL(e.ICD10_J9622,0)+IFNULL(e.ICD10_J9692,0)+IFNULL(e.ICD10_E662,0)+IFNULL(e.ICD9_27803,0)) > 0, 1, 0) AS any_hypercap_icd_ed
  FROM hosp_flags h
  FULL OUTER JOIN ed_by_hadm e
    ON h.hadm_id = e.hadm_id
)

SELECT
  subject_id, hadm_id,
  ICD10_J9602, ICD10_J9612, ICD10_J9622, ICD10_J9692, ICD10_E662, ICD9_27803,
  IF((ICD10_J9602+ICD10_J9612+ICD10_J9622+ICD10_J9692+ICD10_E662+ICD9_27803) > 0, 1, 0) AS any_hypercap_icd,
  any_hypercap_icd_hosp,
  any_hypercap_icd_ed,
  CASE
    WHEN any_hypercap_icd_hosp=1 AND any_hypercap_icd_ed=1 THEN 'ED+HOSP'
    WHEN any_hypercap_icd_ed=1 THEN 'ED'
    WHEN any_hypercap_icd_hosp=1 THEN 'HOSP'
    ELSE 'NONE'
  END AS icd_source
FROM combined
"""

cohort_icd = run_sql_bq(sql("cohort_icd_sql"))
print("ICD cohort admissions:", len(cohort_icd))
cohort_icd.head(3)


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.\n\n# Target ICD codes (dotless, uppercase)\nICD10_CODES = [\'J9602\',\'J9612\',\'J9622\',\'J9692\',\'E662\']\nICD9_CODES  = [\'27803\']\n\nSQL["cohort_icd_sql"] = f"""\n-- ICD-based cohort flags per admission\nWITH target_codes AS (\n  SELECT \'J9602\' AS code, 10 AS ver UNION ALL\n  SELECT \'J9612\', 10 UNION ALL\n  SELECT \'J9622\', 10 UNION ALL\n  SELECT \'J9692\', 10 UNION ALL\n  SELECT \'E662\',  10 UNION ALL\n  SELECT \'27803\', 9\n),\n\n-- Hospital ICDs restricted to target codes\nhosp_dx AS (\n  SELECT\n    d.subject_id,\n    d.hadm_id,\n    UPPER(REPLACE(d.icd_code, \'.\', \'\')) AS code_norm,\n    d.icd_version\n  FROM `{PHYS}.{HOSP}.diagnoses_icd` d\n  JOIN target_codes t\n    ON t.ver = d.icd_version AND t.code = UPPER(REPLACE(d.icd_code, \'.\', \'\'))\n  WHERE d.hadm_id IS NOT NULL\n),\n\n-- Hospital flags per admission\nhosp_flags AS (\n  SELECT\n    subject_id, hadm_id,\n    MAX(IF(icd_version=10 AND code_norm=\'J9602\',1,0)) AS ICD10_J9602,\n    MAX(IF(icd_version=10 AND code_norm=\'J9612\',1,0)) AS ICD10_J9612,\n    MAX(IF(icd_version=10 AND code_norm=\'J9622\',1,0)) AS ICD10_J9622,\n    MAX(IF(icd_version=10 AND code_norm=\'J9692\',1,0)) AS ICD10_J9692,\n    MAX(IF(icd_version=10 AND code_norm=\'E662\', 1,0)) AS ICD10_E662,\n    MAX(IF(icd_version=9  AND code_norm=\'27803\',1,0)) AS ICD9_27803\n  FROM hosp_dx\n  GROUP BY subject_id, hadm_id\n),\n\n-- ED ICDs restricted to target codes (map to hadm via edstays)\ned_dx AS (\n  SELECT\n    s.subject_id,\n    s.hadm_id,\n    s.stay_id,\n    s.intime AS ed_intime,\n    UPPER(REPLACE(d.icd_code, \'.\', \'\')) AS code_norm,\n    d.icd_version\n  FROM `{PHYS}.{ED}.diagnosis` d\n  JOIN `{PHYS}.{ED}.edstays` s\n    ON s.subject_id = d.subject_id AND s.stay_id = d.stay_id\n  JOIN target_codes t\n    ON t.ver = d.icd_version AND t.code = UPPER(REPLACE(d.icd_code, \'.\', \'\'))\n  WHERE s.hadm_id IS NOT NULL\n),\n\n-- ED flags per ED stay (so we can both: OR flags across stays and also pick earliest stay_id)\ned_flags_by_stay AS (\n  SELECT\n    subject_id, hadm_id, stay_id, MIN(ed_intime) AS ed_intime,\n    MAX(IF(icd_version=10 AND code_norm=\'J9602\',1,0)) AS ICD10_J9602,\n    MAX(IF(icd_version=10 AND code_norm=\'J9612\',1,0)) AS ICD10_J9612,\n    MAX(IF(icd_version=10 AND code_norm=\'J9622\',1,0)) AS ICD10_J9622,\n    MAX(IF(icd_version=10 AND code_norm=\'J9692\',1,0)) AS ICD10_J9692,\n    MAX(IF(icd_version=10 AND code_norm=\'E662\', 1,0)) AS ICD10_E662,\n    MAX(IF(icd_version=9  AND code_norm=\'27803\',1,0)) AS ICD9_27803\n  FROM ed_dx\n  GROUP BY subject_id, hadm_id, stay_id\n),\n\n-- OR the ED flags across all ED stays mapped to the same hadm\ned_flags_or AS (\n  SELECT\n    subject_id, hadm_id,\n    MAX(ICD10_J9602) AS ICD10_J9602,\n    MAX(ICD10_J9612) AS ICD10_J9612,\n    MAX(ICD10_J9622) AS ICD10_J9622,\n    MAX(ICD10_J9692) AS ICD10_J9692,\n    MAX(ICD10_E662 ) AS ICD10_E662,\n    MAX(ICD9_27803) AS ICD9_27803\n  FROM ed_flags_by_stay\n  GROUP BY subject_id, hadm_id\n),\n\n-- Earliest ED stay_id per hadm (NO UNNEST of aggregates; use [OFFSET(0)])\ned_earliest AS (\n  SELECT\n    subject_id,\n    hadm_id,\n    (ARRAY_AGG(STRUCT(stay_id, ed_intime) ORDER BY ed_intime LIMIT 1))[OFFSET(0)].stay_id AS stay_id\n  FROM ed_flags_by_stay\n  GROUP BY subject_id, hadm_id\n),\n\n-- Bring flags and earliest stay_id together\ned_by_hadm AS (\n  SELECT\n    f.subject_id,\n    f.hadm_id,\n    e.stay_id,\n    f.ICD10_J9602,\n    f.ICD10_J9612,\n    f.ICD10_J9622,\n    f.ICD10_J9692,\n    f.ICD10_E662,\n    f.ICD9_27803\n  FROM ed_flags_or f\n  LEFT JOIN ed_earliest e\n    USING (subject_id, hadm_id)\n),\n\n-- Combine ED and hospital flags at the admission level\ncombined AS (\n  SELECT\n    COALESCE(h.subject_id, e.subject_id) AS subject_id,\n    COALESCE(h.hadm_id,     e.hadm_id)   AS hadm_id,\n    GREATEST(IFNULL(h.ICD10_J9602,0), IFNULL(e.ICD10_J9602,0)) AS ICD10_J9602,\n    GREATEST(IFNULL(h.ICD10_J9612,0), IFNULL(e.ICD10_J9612,0)) AS ICD10_J9612,\n    GREATEST(IFNULL(h.ICD10_J9622,0), IFNULL(e.ICD10_J9622,0)) AS ICD10_J9622,\n    GREATEST(IFNULL(h.ICD10_J9692,0), IFNULL(e.ICD10_J9692,0)) AS ICD10_J9692,\n    GREATEST(IFNULL(h.ICD10_E662 ,0), IFNULL(e.ICD10_E662 ,0)) AS ICD10_E662,\n    GREATEST(IFNULL(h.ICD9_27803,0), IFNULL(e.ICD9_27803,0)) AS ICD9_27803,\n    IF((IFNULL(h.ICD10_J9602,0)+IFNULL(h.ICD10_J9612,0)+IFNULL(h.ICD10_J9622,0)+IFNULL(h.ICD10_J9692,0)+IFNULL(h.ICD10_E662,0)+IFNULL(h.ICD9_27803,0)) > 0, 1, 0) AS any_hypercap_icd_hosp,\n    IF((IFNULL(e.ICD10_J9602,0)+IFNULL(e.ICD10_J9612,0)+IFNULL(e.ICD10_J9622,0)+IFNULL(e.ICD10_J9692,0)+IFNULL(e.ICD10_E662,0)+IFNULL(e.ICD9_27803,0)) > 0, 1, 0) AS any_hypercap_icd_ed\n  FROM hosp_flags h\n  FULL OUTER JOIN ed_by_hadm e\n    ON h.hadm_id = e.hadm_id\n)\n\nSELECT\n  subject_id, hadm_id,\n  ICD10_J9602, ICD10_J9612, ICD10_J9622, ICD10_J9692, ICD10_E662, ICD9_27803,\n  IF((ICD10_J9602+ICD10_J9612+ICD10_J9622+ICD10_J9692+ICD10_E662+ICD9_27803) > 0, 1, 0) AS any_hypercap_icd,\n  any_hypercap_icd_hosp,\n  any_hypercap_icd_ed,\n  CASE\n    WHEN any_hypercap_icd_hosp=1 AND any_hypercap_icd_ed=1 THEN \'ED+HOSP\'\n    WHEN any_hypercap_icd_ed=1 THEN \'ED\'\n    WHEN any_hypercap_icd_hosp=1 THEN \'HOSP\'\n    ELSE \'NONE\'\n  END AS icd_source\nFROM combined\n"""\n\ncohort_icd = run_sql_bq(sql("cohort_icd_sql"))\nprint("ICD cohort admissions:", len(cohort_icd))\ncohort_icd.head(3)\n\n', 'execution_count': 5}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ICD cohort admissions: 4237\n'}
[NbConvertApp] msg_type: execute_result
[NbConvertApp] content: {'data': {'text/plain': '   subject_id   hadm_id  ICD10_J9602  ICD10_J9612  ICD10_J9622  ICD10_J9692  \\\n0    10485425  21207827            0            0            1            0   \n1    16826447  21830147            0            1            0            0   \n2    11482582  22869017            0            0            0            0   \n\n   ICD10_E662  ICD9_27803  any_hypercap_icd  any_hypercap_icd_hosp  \\\n0           1           0                 1                      1   \n1           1           0                 1                      1   \n2           0           1                 1                      1   \n\n   any_hypercap_icd_ed icd_source  \n0                    0       HOSP  \n1                    0       HOSP  \n2                    0       HOSP  ', 'text/html': '<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>subject_id</th>\n      <th>hadm_id</th>\n      <th>ICD10_J9602</th>\n      <th>ICD10_J9612</th>\n      <th>ICD10_J9622</th>\n      <th>ICD10_J9692</th>\n      <th>ICD10_E662</th>\n      <th>ICD9_27803</th>\n      <th>any_hypercap_icd</th>\n      <th>any_hypercap_icd_hosp</th>\n      <th>any_hypercap_icd_ed</th>\n      <th>icd_source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10485425</td>\n      <td>21207827</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>HOSP</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>16826447</td>\n      <td>21830147</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>HOSP</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>11482582</td>\n      <td>22869017</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>HOSP</td>\n    </tr>\n  </tbody>\n</table>\n</div>'}, 'metadata': {}, 'execution_count': 5}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 16
[NbConvertApp] Skipping non-executing cell 17
[NbConvertApp] Executing cell:
# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.

SQL["co2_thresholds_sql"] = f"""
/* ---- LAB (HOSP) pCO2 across entire dataset ---- */
WITH hosp_cand AS (
  SELECT
    le.hadm_id, le.charttime, le.specimen_id,
    COALESCE(
      CAST(le.valuenum AS FLOAT64),
      SAFE_CAST(REGEXP_EXTRACT(LOWER(le.value), r'(-?\d+(?:\.\d+)?)') AS FLOAT64)
    ) AS val,
    LOWER(REPLACE(COALESCE(le.valueuom,''),' ','')) AS uom_nospace,
    LOWER(di.label) AS lbl,
    LOWER(COALESCE(di.fluid,'')) AS fl
  FROM `{PHYS}.{HOSP}.labevents` le
  JOIN `{PHYS}.{HOSP}.d_labitems` di ON di.itemid = le.itemid
  WHERE (le.valuenum IS NOT NULL OR le.value IS NOT NULL)
    AND (
      LOWER(COALESCE(di.category,'')) LIKE '%blood gas%' OR
      LOWER(di.label) LIKE '%pco2%' OR
      REGEXP_CONTAINS(LOWER(di.label), r'\bpa?\s*co(?:2|₂)\b')
    )
    AND NOT REGEXP_CONTAINS(LOWER(di.label),
        r'(et\s*co2|end[- ]?tidal|t\s*co2|tco2|total\s*co2|hco3|bicar)')
),
hosp_spec AS (
  SELECT le.specimen_id, LOWER(COALESCE(le.value,'')) AS spec_val
  FROM `{PHYS}.{HOSP}.labevents` le
  JOIN `{PHYS}.{HOSP}.d_labitems` di ON di.itemid = le.itemid
  WHERE le.specimen_id IS NOT NULL
    AND REGEXP_CONTAINS(LOWER(di.label), r'(specimen|sample)')
),
hosp_pco2 AS (
  SELECT
    c.hadm_id, c.charttime,
    CASE
      WHEN REGEXP_CONTAINS(s.spec_val, r'arter') OR REGEXP_CONTAINS(s.spec_val, r'\bart\b') THEN 'arterial'
      WHEN REGEXP_CONTAINS(s.spec_val, r'ven|mixed|central') THEN 'venous'
      WHEN REGEXP_CONTAINS(c.fl, r'arter') THEN 'arterial'
      WHEN REGEXP_CONTAINS(c.fl, r'ven') THEN 'venous'
      WHEN c.fl LIKE '%arterial%' OR REGEXP_CONTAINS(c.lbl, r'\b(abg|art|arterial|a[- ]?line)\b') THEN 'arterial'
      WHEN c.fl LIKE '%ven%'      OR REGEXP_CONTAINS(c.lbl, r'\b(vbg|ven|venous|mixed|central)\b') THEN 'venous'
      ELSE 'other'
    END AS site,
    CASE WHEN c.uom_nospace='kpa' THEN c.val*7.50062 ELSE c.val END AS pco2_mmHg
  FROM hosp_cand c
  LEFT JOIN hosp_spec s USING (specimen_id)
  WHERE c.val IS NOT NULL
),
hosp_pco2_std AS (
  SELECT hadm_id, site, charttime, pco2_mmHg
  FROM hosp_pco2
  WHERE site IN ('arterial','venous','other') AND pco2_mmHg BETWEEN 5 AND 200
),

/* ---- ICU (POC) pCO2 across entire dataset ---- */
icu_raw AS (
  SELECT
    ie.hadm_id,
    ce.stay_id,
    ce.charttime,
    LOWER(di.label) AS lbl,
    LOWER(REPLACE(COALESCE(ce.valueuom,''),' ','')) AS uom_nospace,
    LOWER(COALESCE(ce.value,'')) AS valstr,
    COALESCE(
      CAST(ce.valuenum AS FLOAT64),
      SAFE_CAST(REGEXP_EXTRACT(LOWER(ce.value), r'(-?\d+(?:\.\d+)?)') AS FLOAT64)
    ) AS val
  FROM `{PHYS}.{ICU}.chartevents` ce
  JOIN `{PHYS}.{ICU}.d_items`  di ON di.itemid = ce.itemid
  JOIN `{PHYS}.{ICU}.icustays` ie ON ie.stay_id = ce.stay_id
),
icu_cand AS (
  SELECT
    hadm_id, stay_id, charttime, lbl, uom_nospace, valstr, val,
    CASE
      WHEN (
            REGEXP_CONTAINS(lbl, r'(^|[^a-z])p(?:a)?\s*co\s*(?:2|₂)([^a-z]|$)')
            OR uom_nospace IN ('mmhg','kpa')
            OR REGEXP_CONTAINS(valstr, r'\b(mm\s*hg|kpa)\b')
           )
           AND NOT REGEXP_CONTAINS(lbl,
               r'(et\s*co2|end[- ]?tidal|t\s*co2|tco2|total\s*co2|hco3|bicar|v\s*co2|vco2|co2\s*(prod|elimin|production|elimination))')
      THEN 'pco2'
      ELSE NULL
    END AS analyte,
    CASE
      WHEN REGEXP_CONTAINS(lbl, r'\b(abg|art|arterial|a[- ]?line)\b') THEN 'arterial'
      WHEN REGEXP_CONTAINS(lbl, r'\b(vbg|ven|venous|mixed|central)\b') THEN 'venous'
      ELSE 'other'
    END AS site
  FROM icu_raw
  WHERE val IS NOT NULL
    AND (
      REGEXP_CONTAINS(lbl, r'(^|[^a-z])p(?:a)?\s*co\s*(?:2|₂)([^a-z]|$)')
      OR uom_nospace IN ('mmhg','kpa')
      OR REGEXP_CONTAINS(valstr, r'\b(mm\s*hg|kpa)\b')
    )
    AND NOT REGEXP_CONTAINS(lbl,
        r'(et\s*co2|end[- ]?tidal|t\s*co2|tco2|total\s*co2|hco3|bicar|v\s*co2|vco2|co2\s*(prod|elimin|production|elimination))')
),
icu_co2_std AS (
  SELECT
    hadm_id,
    site,
    charttime,
    CASE WHEN uom_nospace='kpa' OR REGEXP_CONTAINS(valstr, r'\bkpa\b') THEN val*7.50062 ELSE val END AS pco2_mmHg
  FROM icu_cand
  WHERE analyte='pco2'
    AND site IN ('arterial','venous','other')
    AND (CASE WHEN uom_nospace='kpa' OR REGEXP_CONTAINS(valstr, r'\bkpa\b') THEN val*7.50062 ELSE val END) BETWEEN 5 AND 200
),

/* ---- Combine and threshold per admission ---- */
all_pco2 AS (
  SELECT * FROM hosp_pco2_std
  UNION ALL
  SELECT * FROM icu_co2_std
),
thresh AS (
  SELECT
    hadm_id,
    MAX(IF(site='arterial' AND pco2_mmHg >= 45.0, 1, 0)) AS abg_hypercap_threshold,
    MAX(IF(site='venous'   AND pco2_mmHg >= 50.0, 1, 0)) AS vbg_hypercap_threshold,
    MAX(IF(site='other'    AND pco2_mmHg >= 50.0, 1, 0)) AS other_hypercap_threshold
  FROM all_pco2
  GROUP BY hadm_id
)
SELECT * FROM thresh
"""

co2_thresh = run_sql_bq(sql("co2_thresholds_sql"))
print("Admissions meeting thresholds:", len(co2_thresh))
co2_thresh.head(3)


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.\n\nSQL["co2_thresholds_sql"] = f"""\n/* ---- LAB (HOSP) pCO2 across entire dataset ---- */\nWITH hosp_cand AS (\n  SELECT\n    le.hadm_id, le.charttime, le.specimen_id,\n    COALESCE(\n      CAST(le.valuenum AS FLOAT64),\n      SAFE_CAST(REGEXP_EXTRACT(LOWER(le.value), r\'(-?\\d+(?:\\.\\d+)?)\') AS FLOAT64)\n    ) AS val,\n    LOWER(REPLACE(COALESCE(le.valueuom,\'\'),\' \',\'\')) AS uom_nospace,\n    LOWER(di.label) AS lbl,\n    LOWER(COALESCE(di.fluid,\'\')) AS fl\n  FROM `{PHYS}.{HOSP}.labevents` le\n  JOIN `{PHYS}.{HOSP}.d_labitems` di ON di.itemid = le.itemid\n  WHERE (le.valuenum IS NOT NULL OR le.value IS NOT NULL)\n    AND (\n      LOWER(COALESCE(di.category,\'\')) LIKE \'%blood gas%\' OR\n      LOWER(di.label) LIKE \'%pco2%\' OR\n      REGEXP_CONTAINS(LOWER(di.label), r\'\\bpa?\\s*co(?:2|₂)\\b\')\n    )\n    AND NOT REGEXP_CONTAINS(LOWER(di.label),\n        r\'(et\\s*co2|end[- ]?tidal|t\\s*co2|tco2|total\\s*co2|hco3|bicar)\')\n),\nhosp_spec AS (\n  SELECT le.specimen_id, LOWER(COALESCE(le.value,\'\')) AS spec_val\n  FROM `{PHYS}.{HOSP}.labevents` le\n  JOIN `{PHYS}.{HOSP}.d_labitems` di ON di.itemid = le.itemid\n  WHERE le.specimen_id IS NOT NULL\n    AND REGEXP_CONTAINS(LOWER(di.label), r\'(specimen|sample)\')\n),\nhosp_pco2 AS (\n  SELECT\n    c.hadm_id, c.charttime,\n    CASE\n      WHEN REGEXP_CONTAINS(s.spec_val, r\'arter\') OR REGEXP_CONTAINS(s.spec_val, r\'\\bart\\b\') THEN \'arterial\'\n      WHEN REGEXP_CONTAINS(s.spec_val, r\'ven|mixed|central\') THEN \'venous\'\n      WHEN REGEXP_CONTAINS(c.fl, r\'arter\') THEN \'arterial\'\n      WHEN REGEXP_CONTAINS(c.fl, r\'ven\') THEN \'venous\'\n      WHEN c.fl LIKE \'%arterial%\' OR REGEXP_CONTAINS(c.lbl, r\'\\b(abg|art|arterial|a[- ]?line)\\b\') THEN \'arterial\'\n      WHEN c.fl LIKE \'%ven%\'      OR REGEXP_CONTAINS(c.lbl, r\'\\b(vbg|ven|venous|mixed|central)\\b\') THEN \'venous\'\n      ELSE \'other\'\n    END AS site,\n    CASE WHEN c.uom_nospace=\'kpa\' THEN c.val*7.50062 ELSE c.val END AS pco2_mmHg\n  FROM hosp_cand c\n  LEFT JOIN hosp_spec s USING (specimen_id)\n  WHERE c.val IS NOT NULL\n),\nhosp_pco2_std AS (\n  SELECT hadm_id, site, charttime, pco2_mmHg\n  FROM hosp_pco2\n  WHERE site IN (\'arterial\',\'venous\',\'other\') AND pco2_mmHg BETWEEN 5 AND 200\n),\n\n/* ---- ICU (POC) pCO2 across entire dataset ---- */\nicu_raw AS (\n  SELECT\n    ie.hadm_id,\n    ce.stay_id,\n    ce.charttime,\n    LOWER(di.label) AS lbl,\n    LOWER(REPLACE(COALESCE(ce.valueuom,\'\'),\' \',\'\')) AS uom_nospace,\n    LOWER(COALESCE(ce.value,\'\')) AS valstr,\n    COALESCE(\n      CAST(ce.valuenum AS FLOAT64),\n      SAFE_CAST(REGEXP_EXTRACT(LOWER(ce.value), r\'(-?\\d+(?:\\.\\d+)?)\') AS FLOAT64)\n    ) AS val\n  FROM `{PHYS}.{ICU}.chartevents` ce\n  JOIN `{PHYS}.{ICU}.d_items`  di ON di.itemid = ce.itemid\n  JOIN `{PHYS}.{ICU}.icustays` ie ON ie.stay_id = ce.stay_id\n),\nicu_cand AS (\n  SELECT\n    hadm_id, stay_id, charttime, lbl, uom_nospace, valstr, val,\n    CASE\n      WHEN (\n            REGEXP_CONTAINS(lbl, r\'(^|[^a-z])p(?:a)?\\s*co\\s*(?:2|₂)([^a-z]|$)\')\n            OR uom_nospace IN (\'mmhg\',\'kpa\')\n            OR REGEXP_CONTAINS(valstr, r\'\\b(mm\\s*hg|kpa)\\b\')\n           )\n           AND NOT REGEXP_CONTAINS(lbl,\n               r\'(et\\s*co2|end[- ]?tidal|t\\s*co2|tco2|total\\s*co2|hco3|bicar|v\\s*co2|vco2|co2\\s*(prod|elimin|production|elimination))\')\n      THEN \'pco2\'\n      ELSE NULL\n    END AS analyte,\n    CASE\n      WHEN REGEXP_CONTAINS(lbl, r\'\\b(abg|art|arterial|a[- ]?line)\\b\') THEN \'arterial\'\n      WHEN REGEXP_CONTAINS(lbl, r\'\\b(vbg|ven|venous|mixed|central)\\b\') THEN \'venous\'\n      ELSE \'other\'\n    END AS site\n  FROM icu_raw\n  WHERE val IS NOT NULL\n    AND (\n      REGEXP_CONTAINS(lbl, r\'(^|[^a-z])p(?:a)?\\s*co\\s*(?:2|₂)([^a-z]|$)\')\n      OR uom_nospace IN (\'mmhg\',\'kpa\')\n      OR REGEXP_CONTAINS(valstr, r\'\\b(mm\\s*hg|kpa)\\b\')\n    )\n    AND NOT REGEXP_CONTAINS(lbl,\n        r\'(et\\s*co2|end[- ]?tidal|t\\s*co2|tco2|total\\s*co2|hco3|bicar|v\\s*co2|vco2|co2\\s*(prod|elimin|production|elimination))\')\n),\nicu_co2_std AS (\n  SELECT\n    hadm_id,\n    site,\n    charttime,\n    CASE WHEN uom_nospace=\'kpa\' OR REGEXP_CONTAINS(valstr, r\'\\bkpa\\b\') THEN val*7.50062 ELSE val END AS pco2_mmHg\n  FROM icu_cand\n  WHERE analyte=\'pco2\'\n    AND site IN (\'arterial\',\'venous\',\'other\')\n    AND (CASE WHEN uom_nospace=\'kpa\' OR REGEXP_CONTAINS(valstr, r\'\\bkpa\\b\') THEN val*7.50062 ELSE val END) BETWEEN 5 AND 200\n),\n\n/* ---- Combine and threshold per admission ---- */\nall_pco2 AS (\n  SELECT * FROM hosp_pco2_std\n  UNION ALL\n  SELECT * FROM icu_co2_std\n),\nthresh AS (\n  SELECT\n    hadm_id,\n    MAX(IF(site=\'arterial\' AND pco2_mmHg >= 45.0, 1, 0)) AS abg_hypercap_threshold,\n    MAX(IF(site=\'venous\'   AND pco2_mmHg >= 50.0, 1, 0)) AS vbg_hypercap_threshold,\n    MAX(IF(site=\'other\'    AND pco2_mmHg >= 50.0, 1, 0)) AS other_hypercap_threshold\n  FROM all_pco2\n  GROUP BY hadm_id\n)\nSELECT * FROM thresh\n"""\n\nco2_thresh = run_sql_bq(sql("co2_thresholds_sql"))\nprint("Admissions meeting thresholds:", len(co2_thresh))\nco2_thresh.head(3)\n\n', 'execution_count': 6}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Admissions meeting thresholds: 124690\n'}
[NbConvertApp] msg_type: execute_result
[NbConvertApp] content: {'data': {'text/plain': '    hadm_id  abg_hypercap_threshold  vbg_hypercap_threshold  \\\n0  21801929                       0                       0   \n1  26912823                       0                       0   \n2  22630089                       0                       0   \n\n   other_hypercap_threshold  \n0                         1  \n1                         1  \n2                         1  ', 'text/html': '<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>hadm_id</th>\n      <th>abg_hypercap_threshold</th>\n      <th>vbg_hypercap_threshold</th>\n      <th>other_hypercap_threshold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>21801929</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>26912823</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>22630089</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>'}, 'metadata': {}, 'execution_count': 6}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 19
[NbConvertApp] Skipping non-executing cell 20
[NbConvertApp] Executing cell:
# Purpose: Combine ICD and gas-threshold ascertainment routes to produce the final hadm inclusion list.

# Outer-join because thresholds can identify hadm_id with no ICD codes and vice versa
cohort_any = cohort_icd.merge(co2_thresh, how="outer", on="hadm_id")

# Fill missing flags with 0 where appropriate
icd_cols = ["ICD10_J9602","ICD10_J9612","ICD10_J9622","ICD10_J9692","ICD10_E662","ICD9_27803","any_hypercap_icd","any_hypercap_icd_hosp","any_hypercap_icd_ed"]
for c in icd_cols:
    if c in cohort_any.columns:
        cohort_any[c] = cohort_any[c].fillna(0).astype(int)

for c in ["abg_hypercap_threshold","vbg_hypercap_threshold","other_hypercap_threshold"]:
    if c in cohort_any.columns:
        cohort_any[c] = cohort_any[c].fillna(0).astype(int)

# Final enrollment flag
cohort_any["pco2_threshold_any"] = ((cohort_any["abg_hypercap_threshold"]==1) | (cohort_any["vbg_hypercap_threshold"]==1) | (cohort_any["other_hypercap_threshold"]==1)).astype(int)
cohort_any["enrolled_any"] = ((cohort_any["any_hypercap_icd"]==1) | (cohort_any["pco2_threshold_any"]==1)).astype(int)

print("ICD-only admissions        :", int((cohort_any["any_hypercap_icd"]==1).sum()))
print("Threshold-only admissions  :", int(((cohort_any["pco2_threshold_any"]==1) & (cohort_any["any_hypercap_icd"]==0)).sum()))
print("Both ICD and threshold     :", int(((cohort_any["pco2_threshold_any"]==1) & (cohort_any["any_hypercap_icd"]==1)).sum()))
print("Total enrolled (union)     :", int((cohort_any["enrolled_any"]==1).sum()))

# New hadm list used for the rest of the notebook
hadm_list = cohort_any.loc[cohort_any["enrolled_any"]==1, "hadm_id"].dropna().astype("int64").tolist()
len(hadm_list)


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Purpose: Combine ICD and gas-threshold ascertainment routes to produce the final hadm inclusion list.\n\n# Outer-join because thresholds can identify hadm_id with no ICD codes and vice versa\ncohort_any = cohort_icd.merge(co2_thresh, how="outer", on="hadm_id")\n\n# Fill missing flags with 0 where appropriate\nicd_cols = ["ICD10_J9602","ICD10_J9612","ICD10_J9622","ICD10_J9692","ICD10_E662","ICD9_27803","any_hypercap_icd","any_hypercap_icd_hosp","any_hypercap_icd_ed"]\nfor c in icd_cols:\n    if c in cohort_any.columns:\n        cohort_any[c] = cohort_any[c].fillna(0).astype(int)\n\nfor c in ["abg_hypercap_threshold","vbg_hypercap_threshold","other_hypercap_threshold"]:\n    if c in cohort_any.columns:\n        cohort_any[c] = cohort_any[c].fillna(0).astype(int)\n\n# Final enrollment flag\ncohort_any["pco2_threshold_any"] = ((cohort_any["abg_hypercap_threshold"]==1) | (cohort_any["vbg_hypercap_threshold"]==1) | (cohort_any["other_hypercap_threshold"]==1)).astype(int)\ncohort_any["enrolled_any"] = ((cohort_any["any_hypercap_icd"]==1) | (cohort_any["pco2_threshold_any"]==1)).astype(int)\n\nprint("ICD-only admissions        :", int((cohort_any["any_hypercap_icd"]==1).sum()))\nprint("Threshold-only admissions  :", int(((cohort_any["pco2_threshold_any"]==1) & (cohort_any["any_hypercap_icd"]==0)).sum()))\nprint("Both ICD and threshold     :", int(((cohort_any["pco2_threshold_any"]==1) & (cohort_any["any_hypercap_icd"]==1)).sum()))\nprint("Total enrolled (union)     :", int((cohort_any["enrolled_any"]==1).sum()))\n\n# New hadm list used for the rest of the notebook\nhadm_list = cohort_any.loc[cohort_any["enrolled_any"]==1, "hadm_id"].dropna().astype("int64").tolist()\nlen(hadm_list)\n\n', 'execution_count': 7}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ICD-only admissions        : 4237\nThreshold-only admissions  : 110943\nBoth ICD and threshold     : 3690\nTotal enrolled (union)     : 115180\n'}
[NbConvertApp] msg_type: execute_result
[NbConvertApp] content: {'data': {'text/plain': '115179'}, 'metadata': {}, 'execution_count': 7}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 22
[NbConvertApp] Skipping non-executing cell 23
[NbConvertApp] Executing cell:
# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.

params = {"hadms": hadm_list}

bg_pairs_sql = rf"""
WITH hadms AS (SELECT hadm_id FROM UNNEST(@hadms) AS hadm_id),

/* ---------------- LAB (HOSP) ---------------- */
hosp_cand AS (
  SELECT
    le.subject_id, le.hadm_id, le.charttime, le.specimen_id,
    CAST(le.valuenum AS FLOAT64) AS val,
    LOWER(COALESCE(le.valueuom,'')) AS uom,
    LOWER(di.label) AS lbl,
    LOWER(COALESCE(di.fluid,'')) AS fl,
    LOWER(COALESCE(di.category,'')) AS cat
  FROM `{PHYS}.{HOSP}.labevents`  le
  JOIN `{PHYS}.{HOSP}.d_labitems` di ON di.itemid = le.itemid
  JOIN hadms h ON h.hadm_id = le.hadm_id
  WHERE le.valuenum IS NOT NULL
    AND (
         LOWER(COALESCE(di.category,'')) LIKE '%blood gas%' OR
         LOWER(di.label) LIKE '%pco2%' OR
         REGEXP_CONTAINS(LOWER(di.label), r'\bph\b') OR
         REGEXP_CONTAINS(LOWER(di.label), r'\bpa?\s*co(?:2|₂)\b')
        )
    AND NOT REGEXP_CONTAINS(LOWER(di.label), r'(et\s*co2|end[- ]?tidal|t\s*co2|tco2|total\s*co2|hco3|bicar)')
),
hosp_spec AS (
  SELECT le.specimen_id, LOWER(COALESCE(le.value,'')) AS spec_val
  FROM `{PHYS}.{HOSP}.labevents` le
  JOIN `{PHYS}.{HOSP}.d_labitems` di ON di.itemid = le.itemid
  WHERE le.specimen_id IS NOT NULL
    AND REGEXP_CONTAINS(LOWER(di.label), r'(specimen|sample)')
),
hosp_class AS (
  SELECT
    c.hadm_id, c.charttime, c.specimen_id, c.val, c.uom, c.lbl, c.fl,
    CASE
      WHEN REGEXP_CONTAINS(c.lbl, r'\b(?:blood\s*)?ph\b') THEN 'ph'
      WHEN (c.lbl LIKE '%pco2%' OR REGEXP_CONTAINS(c.lbl, r'\bpa?\s*co(?:2|₂)\b')) THEN 'pco2'
      ELSE NULL
    END AS analyte,
    CASE
      WHEN REGEXP_CONTAINS(s.spec_val, r'arter') OR REGEXP_CONTAINS(s.spec_val, r'\bart\b') THEN 'arterial'
      WHEN REGEXP_CONTAINS(s.spec_val, r'ven|mixed|central') THEN 'venous'
      WHEN c.fl LIKE '%arterial%' OR REGEXP_CONTAINS(c.lbl, r'\b(abg|art|arterial|a[- ]?line)\b') THEN 'arterial'
      WHEN c.fl LIKE '%ven%'      OR REGEXP_CONTAINS(c.lbl, r'\b(vbg|ven|venous|mixed|central)\b') THEN 'venous'
      ELSE 'other'
    END AS site
  FROM hosp_cand c
  LEFT JOIN hosp_spec s USING (specimen_id)
),
hosp_pairs AS (
  SELECT
    hadm_id, specimen_id,
    MIN(charttime) AS sample_time,
    MAX(IF(analyte='ph',   val, NULL)) AS ph,
    MAX(IF(analyte='pco2', val, NULL)) AS pco2_raw,
    (ARRAY_AGG(IF(analyte='pco2', uom, NULL) IGNORE NULLS LIMIT 1))[OFFSET(0)] AS pco2_uom,
    (ARRAY_AGG(IF(analyte='ph',   uom, NULL) IGNORE NULLS LIMIT 1))[OFFSET(0)] AS ph_uom,
    (ARRAY_AGG(site IGNORE NULLS LIMIT 1))[OFFSET(0)] AS site
  FROM hosp_class
  GROUP BY hadm_id, specimen_id
  HAVING (ph IS NOT NULL OR pco2_raw IS NOT NULL) AND site IN ('arterial','venous','other')
),
hosp_pairs_std AS (
  SELECT
    hadm_id, specimen_id, sample_time, site,
    ph, ph_uom,
    CASE WHEN pco2_uom = 'kpa' THEN pco2_raw * 7.50062 ELSE pco2_raw END AS pco2_mmHg,
    'mmhg' AS pco2_uom_norm
  FROM hosp_pairs
  WHERE (ph IS NULL OR (ph BETWEEN 6.3 AND 7.8))
    AND (pco2_raw IS NULL OR (CASE WHEN pco2_uom='kpa' THEN pco2_raw*7.50062 ELSE pco2_raw END) BETWEEN 5 AND 200)
),
lab_abg AS (
  SELECT hadm_id,
         ph            AS lab_abg_ph,
         ph_uom        AS lab_abg_ph_uom,
         pco2_mmHg     AS lab_abg_paco2,
         'mmhg'        AS lab_abg_paco2_uom,
         sample_time   AS lab_abg_time
  FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY hadm_id ORDER BY sample_time) rn
        FROM hosp_pairs_std WHERE site='arterial') WHERE rn=1
),
lab_vbg AS (
  SELECT hadm_id,
         ph            AS lab_vbg_ph,
         ph_uom        AS lab_vbg_ph_uom,
         pco2_mmHg     AS lab_vbg_paco2,
         'mmhg'        AS lab_vbg_paco2_uom,
         sample_time   AS lab_vbg_time
  FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY hadm_id ORDER BY sample_time) rn
        FROM hosp_pairs_std WHERE site='venous') WHERE rn=1
),
lab_other AS (
  SELECT hadm_id,
         ph            AS lab_other_ph,
         ph_uom        AS lab_other_ph_uom,
         pco2_mmHg     AS lab_other_paco2,
         'mmhg'        AS lab_other_paco2_uom,
         sample_time   AS lab_other_time
  FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY hadm_id ORDER BY sample_time) rn
        FROM hosp_pairs_std WHERE site='other') WHERE rn=1
),

/* ---------------- POC (ICU) ---------------- */
icu_raw AS (
  SELECT
    ie.hadm_id, ce.stay_id, ce.charttime,
    LOWER(di.label) AS lbl,
    LOWER(REPLACE(COALESCE(ce.valueuom,''),' ','')) AS uom_nospace,
    LOWER(COALESCE(ce.value,'')) AS valstr,
    COALESCE(
      CAST(ce.valuenum AS FLOAT64),
      SAFE_CAST(REGEXP_EXTRACT(LOWER(ce.value), r'(-?\d+(?:\.\d+)?)') AS FLOAT64)
    ) AS val
  FROM `{PHYS}.{ICU}.chartevents` ce
  JOIN `{PHYS}.{ICU}.d_items`  di ON di.itemid = ce.itemid
  JOIN `{PHYS}.{ICU}.icustays` ie ON ie.stay_id = ce.stay_id
  JOIN hadms h ON h.hadm_id = ie.hadm_id
),
icu_cand AS (
  SELECT
    hadm_id, stay_id, charttime, lbl, uom_nospace, valstr, val,
    CASE
      WHEN REGEXP_CONTAINS(lbl, r'(^|[^a-z])ph([^a-z]|$)') OR (val BETWEEN 6.3 AND 7.8) THEN 'ph'
      WHEN (
             REGEXP_CONTAINS(lbl, r'(^|[^a-z])p(?:a)?\s*co\s*(?:2|₂)([^a-z]|$)')
             OR uom_nospace IN ('mmhg','kpa')
             OR REGEXP_CONTAINS(valstr, r'\b(mm\s*hg|kpa)\b')
           )
           AND NOT REGEXP_CONTAINS(lbl, r'(et\s*co2|end[- ]?tidal|t\s*co2|tco2|total\s*co2|hco3|bicar|v\s*co2|vco2|co2\s*(prod|elimin|production|elimination))')
      THEN 'pco2'
      ELSE NULL
    END AS analyte,
    CASE
      WHEN REGEXP_CONTAINS(lbl, r'\b(abg|art|arterial|a[- ]?line)\b') THEN 'arterial'
      WHEN REGEXP_CONTAINS(lbl, r'\b(vbg|ven|venous|mixed|central)\b') THEN 'venous'
      ELSE 'other'
    END AS site
  FROM icu_raw
  WHERE val IS NOT NULL
    AND (
      REGEXP_CONTAINS(lbl, r'(^|[^a-z])ph([^a-z]|$)') OR
      REGEXP_CONTAINS(lbl, r'(^|[^a-z])p(?:a)?\s*co\s*(?:2|₂)([^a-z]|$)') OR
      uom_nospace IN ('mmhg','kpa') OR
      REGEXP_CONTAINS(valstr, r'\b(mm\s*hg|kpa)\b')
    )
    AND NOT REGEXP_CONTAINS(lbl, r'(et\s*co2|end[- ]?tidal|t\s*co2|tco2|total\s*co2|hco3|bicar|v\s*co2|vco2|co2\s*(prod|elimin|production|elimination))')
),
icu_ph AS (
  SELECT hadm_id, stay_id, charttime, val AS ph, site AS site_ph
  FROM icu_cand WHERE analyte='ph'
),
icu_co2 AS (
  SELECT hadm_id, stay_id, charttime, val AS pco2_raw, uom_nospace, valstr, site AS site_co2
  FROM icu_cand WHERE analyte='pco2'
),
icu_pair_win AS (
  SELECT
    p.hadm_id, p.stay_id,
    COALESCE(p.site_ph, c.site_co2) AS site,
    p.charttime AS ph_time, c.charttime AS co2_time,
    p.ph,
    CASE
      WHEN c.uom_nospace='kpa' OR REGEXP_CONTAINS(c.valstr, r'\bkpa\b') THEN 'kpa'
      WHEN c.uom_nospace='mmhg' OR REGEXP_CONTAINS(c.valstr, r'mm\s*hg') THEN 'mmhg'
      ELSE c.uom_nospace
    END AS pco2_uom_norm_raw,
    c.pco2_raw,
    ABS(TIMESTAMP_DIFF(c.charttime, p.charttime, SECOND)) AS dt_sec
  FROM icu_ph p
  JOIN icu_co2 c
    ON c.hadm_id = p.hadm_id
   AND c.stay_id = p.stay_id
   AND (COALESCE(p.site_ph, c.site_co2) IN ('arterial','venous','other'))
   AND ABS(TIMESTAMP_DIFF(c.charttime, p.charttime, MINUTE)) <= 10
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY p.hadm_id, p.stay_id, p.charttime
    ORDER BY dt_sec
  ) = 1
),
icu_pairs_std AS (
  SELECT
    hadm_id, stay_id, site,
    LEAST(ph_time, co2_time) AS sample_time,
    ph,
    CAST(NULL AS STRING) AS ph_uom,              -- POC pH is unitless/null
    CASE WHEN pco2_uom_norm_raw='kpa' THEN pco2_raw*7.50062 ELSE pco2_raw END AS pco2_mmHg,
    'mmhg' AS pco2_uom_norm
  FROM icu_pair_win
  WHERE (ph BETWEEN 6.3 AND 7.8 OR ph IS NULL)
    AND (CASE WHEN pco2_uom_norm_raw='kpa' THEN pco2_raw*7.50062 ELSE pco2_raw END) BETWEEN 5 AND 200
),
icu_solo_pco2_std AS (
  SELECT
    hadm_id, stay_id, site_co2 AS site,
    charttime AS sample_time,
    CAST(NULL AS FLOAT64) AS ph,
    CAST(NULL AS STRING)  AS ph_uom,            -- no pH here
    CASE WHEN uom_nospace='kpa' OR REGEXP_CONTAINS(valstr, r'\bkpa\b') THEN pco2_raw*7.50062 ELSE pco2_raw END AS pco2_mmHg,
    'mmhg' AS pco2_uom_norm
  FROM icu_co2
  WHERE site_co2 IN ('arterial','venous','other')
    AND pco2_raw BETWEEN 5 AND 200
),
icu_all AS (
  SELECT * FROM icu_pairs_std
  UNION ALL
  SELECT * FROM icu_solo_pco2_std
),

poc_abg AS (
  SELECT hadm_id,
         ph            AS poc_abg_ph,
         ph_uom        AS poc_abg_ph_uom,
         pco2_mmHg     AS poc_abg_paco2,
         'mmhg'        AS poc_abg_paco2_uom,
         sample_time   AS poc_abg_time
  FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY hadm_id ORDER BY sample_time) rn
        FROM icu_all WHERE site='arterial') WHERE rn=1
),
poc_vbg AS (
  SELECT hadm_id,
         ph            AS poc_vbg_ph,
         ph_uom        AS poc_vbg_ph_uom,
         pco2_mmHg     AS poc_vbg_paco2,
         'mmhg'        AS poc_vbg_paco2_uom,
         sample_time   AS poc_vbg_time
  FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY hadm_id ORDER BY sample_time) rn
        FROM icu_all WHERE site='venous') WHERE rn=1
),
poc_other AS (
  SELECT hadm_id,
         ph            AS poc_other_ph,
         ph_uom        AS poc_other_ph_uom,
         pco2_mmHg     AS poc_other_paco2,
         'mmhg'        AS poc_other_paco2_uom,
         sample_time   AS poc_other_time
  FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY hadm_id ORDER BY sample_time) rn
        FROM icu_all WHERE site='other') WHERE rn=1
)

/* ---------------- Final one row per hadm ---------------- */
SELECT
  h.hadm_id,
  -- LAB-ABG / LAB-VBG / LAB-OTHER
  la.lab_abg_ph, la.lab_abg_ph_uom, la.lab_abg_paco2, la.lab_abg_paco2_uom, la.lab_abg_time,
  lv.lab_vbg_ph, lv.lab_vbg_ph_uom, lv.lab_vbg_paco2, lv.lab_vbg_paco2_uom, lv.lab_vbg_time,
  lo.lab_other_ph, lo.lab_other_ph_uom, lo.lab_other_paco2, lo.lab_other_paco2_uom, lo.lab_other_time,
  -- POC-ABG / POC-VBG / POC-OTHER
  pa.poc_abg_ph, pa.poc_abg_ph_uom, pa.poc_abg_paco2, pa.poc_abg_paco2_uom, pa.poc_abg_time,
  pv.poc_vbg_ph, pv.poc_vbg_ph_uom, pv.poc_vbg_paco2, pv.poc_vbg_paco2_uom, pv.poc_vbg_time,
  po.poc_other_ph, po.poc_other_ph_uom, po.poc_other_paco2, po.poc_other_paco2_uom, po.poc_other_time,
  -- First ABG across LAB+POC
  (SELECT AS STRUCT src, t, ph, pco2
   FROM (SELECT 'LAB' AS src, la.lab_abg_time AS t, la.lab_abg_ph AS ph, la.lab_abg_paco2 AS pco2
         UNION ALL
         SELECT 'POC', pa.poc_abg_time, pa.poc_abg_ph, pa.poc_abg_paco2)
   WHERE t IS NOT NULL
   ORDER BY t LIMIT 1) AS first_abg,
  -- First VBG across LAB+POC
  (SELECT AS STRUCT src, t, ph, pco2
   FROM (SELECT 'LAB' AS src, lv.lab_vbg_time AS t, lv.lab_vbg_ph AS ph, lv.lab_vbg_paco2 AS pco2
         UNION ALL
         SELECT 'POC', pv.poc_vbg_time, pv.poc_vbg_ph, pv.poc_vbg_paco2)
   WHERE t IS NOT NULL
   ORDER BY t LIMIT 1) AS first_vbg,
  -- First OTHER-source pCO2 across LAB+POC
  (SELECT AS STRUCT src, t, ph, pco2
   FROM (SELECT 'LAB' AS src, lo.lab_other_time AS t, lo.lab_other_ph AS ph, lo.lab_other_paco2 AS pco2
         UNION ALL
         SELECT 'POC', po.poc_other_time, po.poc_other_ph, po.poc_other_paco2)
   WHERE t IS NOT NULL
   ORDER BY t LIMIT 1) AS first_other
FROM hadms h
LEFT JOIN lab_abg la USING (hadm_id)
LEFT JOIN lab_vbg lv USING (hadm_id)
LEFT JOIN lab_other lo USING (hadm_id)
LEFT JOIN poc_abg pa USING (hadm_id)
LEFT JOIN poc_vbg pv USING (hadm_id)
LEFT JOIN poc_other po USING (hadm_id)
"""

bg_pairs = run_sql_bq(bg_pairs_sql, params)

# Flatten STRUCTs for first_abg, first_vbg, and first_other
for col in ["first_abg","first_vbg","first_other"]:
    if col in bg_pairs.columns:
        bg_pairs[f"{col}_src"]  = bg_pairs[col].apply(lambda x: x.get("src") if isinstance(x, dict) else None)
        bg_pairs[f"{col}_time"] = bg_pairs[col].apply(lambda x: x.get("t")   if isinstance(x, dict) else None)
        bg_pairs[f"{col}_ph"]   = bg_pairs[col].apply(lambda x: x.get("ph")  if isinstance(x, dict) else None)
        bg_pairs[f"{col}_pco2"] = bg_pairs[col].apply(lambda x: x.get("pco2")if isinstance(x, dict) else None)
        bg_pairs = bg_pairs.drop(columns=[col])

bg_pairs.head(3)

[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.\n\nparams = {"hadms": hadm_list}\n\nbg_pairs_sql = rf"""\nWITH hadms AS (SELECT hadm_id FROM UNNEST(@hadms) AS hadm_id),\n\n/* ---------------- LAB (HOSP) ---------------- */\nhosp_cand AS (\n  SELECT\n    le.subject_id, le.hadm_id, le.charttime, le.specimen_id,\n    CAST(le.valuenum AS FLOAT64) AS val,\n    LOWER(COALESCE(le.valueuom,\'\')) AS uom,\n    LOWER(di.label) AS lbl,\n    LOWER(COALESCE(di.fluid,\'\')) AS fl,\n    LOWER(COALESCE(di.category,\'\')) AS cat\n  FROM `{PHYS}.{HOSP}.labevents`  le\n  JOIN `{PHYS}.{HOSP}.d_labitems` di ON di.itemid = le.itemid\n  JOIN hadms h ON h.hadm_id = le.hadm_id\n  WHERE le.valuenum IS NOT NULL\n    AND (\n         LOWER(COALESCE(di.category,\'\')) LIKE \'%blood gas%\' OR\n         LOWER(di.label) LIKE \'%pco2%\' OR\n         REGEXP_CONTAINS(LOWER(di.label), r\'\\bph\\b\') OR\n         REGEXP_CONTAINS(LOWER(di.label), r\'\\bpa?\\s*co(?:2|₂)\\b\')\n        )\n    AND NOT REGEXP_CONTAINS(LOWER(di.label), r\'(et\\s*co2|end[- ]?tidal|t\\s*co2|tco2|total\\s*co2|hco3|bicar)\')\n),\nhosp_spec AS (\n  SELECT le.specimen_id, LOWER(COALESCE(le.value,\'\')) AS spec_val\n  FROM `{PHYS}.{HOSP}.labevents` le\n  JOIN `{PHYS}.{HOSP}.d_labitems` di ON di.itemid = le.itemid\n  WHERE le.specimen_id IS NOT NULL\n    AND REGEXP_CONTAINS(LOWER(di.label), r\'(specimen|sample)\')\n),\nhosp_class AS (\n  SELECT\n    c.hadm_id, c.charttime, c.specimen_id, c.val, c.uom, c.lbl, c.fl,\n    CASE\n      WHEN REGEXP_CONTAINS(c.lbl, r\'\\b(?:blood\\s*)?ph\\b\') THEN \'ph\'\n      WHEN (c.lbl LIKE \'%pco2%\' OR REGEXP_CONTAINS(c.lbl, r\'\\bpa?\\s*co(?:2|₂)\\b\')) THEN \'pco2\'\n      ELSE NULL\n    END AS analyte,\n    CASE\n      WHEN REGEXP_CONTAINS(s.spec_val, r\'arter\') OR REGEXP_CONTAINS(s.spec_val, r\'\\bart\\b\') THEN \'arterial\'\n      WHEN REGEXP_CONTAINS(s.spec_val, r\'ven|mixed|central\') THEN \'venous\'\n      WHEN c.fl LIKE \'%arterial%\' OR REGEXP_CONTAINS(c.lbl, r\'\\b(abg|art|arterial|a[- ]?line)\\b\') THEN \'arterial\'\n      WHEN c.fl LIKE \'%ven%\'      OR REGEXP_CONTAINS(c.lbl, r\'\\b(vbg|ven|venous|mixed|central)\\b\') THEN \'venous\'\n      ELSE \'other\'\n    END AS site\n  FROM hosp_cand c\n  LEFT JOIN hosp_spec s USING (specimen_id)\n),\nhosp_pairs AS (\n  SELECT\n    hadm_id, specimen_id,\n    MIN(charttime) AS sample_time,\n    MAX(IF(analyte=\'ph\',   val, NULL)) AS ph,\n    MAX(IF(analyte=\'pco2\', val, NULL)) AS pco2_raw,\n    (ARRAY_AGG(IF(analyte=\'pco2\', uom, NULL) IGNORE NULLS LIMIT 1))[OFFSET(0)] AS pco2_uom,\n    (ARRAY_AGG(IF(analyte=\'ph\',   uom, NULL) IGNORE NULLS LIMIT 1))[OFFSET(0)] AS ph_uom,\n    (ARRAY_AGG(site IGNORE NULLS LIMIT 1))[OFFSET(0)] AS site\n  FROM hosp_class\n  GROUP BY hadm_id, specimen_id\n  HAVING (ph IS NOT NULL OR pco2_raw IS NOT NULL) AND site IN (\'arterial\',\'venous\',\'other\')\n),\nhosp_pairs_std AS (\n  SELECT\n    hadm_id, specimen_id, sample_time, site,\n    ph, ph_uom,\n    CASE WHEN pco2_uom = \'kpa\' THEN pco2_raw * 7.50062 ELSE pco2_raw END AS pco2_mmHg,\n    \'mmhg\' AS pco2_uom_norm\n  FROM hosp_pairs\n  WHERE (ph IS NULL OR (ph BETWEEN 6.3 AND 7.8))\n    AND (pco2_raw IS NULL OR (CASE WHEN pco2_uom=\'kpa\' THEN pco2_raw*7.50062 ELSE pco2_raw END) BETWEEN 5 AND 200)\n),\nlab_abg AS (\n  SELECT hadm_id,\n         ph            AS lab_abg_ph,\n         ph_uom        AS lab_abg_ph_uom,\n         pco2_mmHg     AS lab_abg_paco2,\n         \'mmhg\'        AS lab_abg_paco2_uom,\n         sample_time   AS lab_abg_time\n  FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY hadm_id ORDER BY sample_time) rn\n        FROM hosp_pairs_std WHERE site=\'arterial\') WHERE rn=1\n),\nlab_vbg AS (\n  SELECT hadm_id,\n         ph            AS lab_vbg_ph,\n         ph_uom        AS lab_vbg_ph_uom,\n         pco2_mmHg     AS lab_vbg_paco2,\n         \'mmhg\'        AS lab_vbg_paco2_uom,\n         sample_time   AS lab_vbg_time\n  FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY hadm_id ORDER BY sample_time) rn\n        FROM hosp_pairs_std WHERE site=\'venous\') WHERE rn=1\n),\nlab_other AS (\n  SELECT hadm_id,\n         ph            AS lab_other_ph,\n         ph_uom        AS lab_other_ph_uom,\n         pco2_mmHg     AS lab_other_paco2,\n         \'mmhg\'        AS lab_other_paco2_uom,\n         sample_time   AS lab_other_time\n  FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY hadm_id ORDER BY sample_time) rn\n        FROM hosp_pairs_std WHERE site=\'other\') WHERE rn=1\n),\n\n/* ---------------- POC (ICU) ---------------- */\nicu_raw AS (\n  SELECT\n    ie.hadm_id, ce.stay_id, ce.charttime,\n    LOWER(di.label) AS lbl,\n    LOWER(REPLACE(COALESCE(ce.valueuom,\'\'),\' \',\'\')) AS uom_nospace,\n    LOWER(COALESCE(ce.value,\'\')) AS valstr,\n    COALESCE(\n      CAST(ce.valuenum AS FLOAT64),\n      SAFE_CAST(REGEXP_EXTRACT(LOWER(ce.value), r\'(-?\\d+(?:\\.\\d+)?)\') AS FLOAT64)\n    ) AS val\n  FROM `{PHYS}.{ICU}.chartevents` ce\n  JOIN `{PHYS}.{ICU}.d_items`  di ON di.itemid = ce.itemid\n  JOIN `{PHYS}.{ICU}.icustays` ie ON ie.stay_id = ce.stay_id\n  JOIN hadms h ON h.hadm_id = ie.hadm_id\n),\nicu_cand AS (\n  SELECT\n    hadm_id, stay_id, charttime, lbl, uom_nospace, valstr, val,\n    CASE\n      WHEN REGEXP_CONTAINS(lbl, r\'(^|[^a-z])ph([^a-z]|$)\') OR (val BETWEEN 6.3 AND 7.8) THEN \'ph\'\n      WHEN (\n             REGEXP_CONTAINS(lbl, r\'(^|[^a-z])p(?:a)?\\s*co\\s*(?:2|₂)([^a-z]|$)\')\n             OR uom_nospace IN (\'mmhg\',\'kpa\')\n             OR REGEXP_CONTAINS(valstr, r\'\\b(mm\\s*hg|kpa)\\b\')\n           )\n           AND NOT REGEXP_CONTAINS(lbl, r\'(et\\s*co2|end[- ]?tidal|t\\s*co2|tco2|total\\s*co2|hco3|bicar|v\\s*co2|vco2|co2\\s*(prod|elimin|production|elimination))\')\n      THEN \'pco2\'\n      ELSE NULL\n    END AS analyte,\n    CASE\n      WHEN REGEXP_CONTAINS(lbl, r\'\\b(abg|art|arterial|a[- ]?line)\\b\') THEN \'arterial\'\n      WHEN REGEXP_CONTAINS(lbl, r\'\\b(vbg|ven|venous|mixed|central)\\b\') THEN \'venous\'\n      ELSE \'other\'\n    END AS site\n  FROM icu_raw\n  WHERE val IS NOT NULL\n    AND (\n      REGEXP_CONTAINS(lbl, r\'(^|[^a-z])ph([^a-z]|$)\') OR\n      REGEXP_CONTAINS(lbl, r\'(^|[^a-z])p(?:a)?\\s*co\\s*(?:2|₂)([^a-z]|$)\') OR\n      uom_nospace IN (\'mmhg\',\'kpa\') OR\n      REGEXP_CONTAINS(valstr, r\'\\b(mm\\s*hg|kpa)\\b\')\n    )\n    AND NOT REGEXP_CONTAINS(lbl, r\'(et\\s*co2|end[- ]?tidal|t\\s*co2|tco2|total\\s*co2|hco3|bicar|v\\s*co2|vco2|co2\\s*(prod|elimin|production|elimination))\')\n),\nicu_ph AS (\n  SELECT hadm_id, stay_id, charttime, val AS ph, site AS site_ph\n  FROM icu_cand WHERE analyte=\'ph\'\n),\nicu_co2 AS (\n  SELECT hadm_id, stay_id, charttime, val AS pco2_raw, uom_nospace, valstr, site AS site_co2\n  FROM icu_cand WHERE analyte=\'pco2\'\n),\nicu_pair_win AS (\n  SELECT\n    p.hadm_id, p.stay_id,\n    COALESCE(p.site_ph, c.site_co2) AS site,\n    p.charttime AS ph_time, c.charttime AS co2_time,\n    p.ph,\n    CASE\n      WHEN c.uom_nospace=\'kpa\' OR REGEXP_CONTAINS(c.valstr, r\'\\bkpa\\b\') THEN \'kpa\'\n      WHEN c.uom_nospace=\'mmhg\' OR REGEXP_CONTAINS(c.valstr, r\'mm\\s*hg\') THEN \'mmhg\'\n      ELSE c.uom_nospace\n    END AS pco2_uom_norm_raw,\n    c.pco2_raw,\n    ABS(TIMESTAMP_DIFF(c.charttime, p.charttime, SECOND)) AS dt_sec\n  FROM icu_ph p\n  JOIN icu_co2 c\n    ON c.hadm_id = p.hadm_id\n   AND c.stay_id = p.stay_id\n   AND (COALESCE(p.site_ph, c.site_co2) IN (\'arterial\',\'venous\',\'other\'))\n   AND ABS(TIMESTAMP_DIFF(c.charttime, p.charttime, MINUTE)) <= 10\n  QUALIFY ROW_NUMBER() OVER (\n    PARTITION BY p.hadm_id, p.stay_id, p.charttime\n    ORDER BY dt_sec\n  ) = 1\n),\nicu_pairs_std AS (\n  SELECT\n    hadm_id, stay_id, site,\n    LEAST(ph_time, co2_time) AS sample_time,\n    ph,\n    CAST(NULL AS STRING) AS ph_uom,              -- POC pH is unitless/null\n    CASE WHEN pco2_uom_norm_raw=\'kpa\' THEN pco2_raw*7.50062 ELSE pco2_raw END AS pco2_mmHg,\n    \'mmhg\' AS pco2_uom_norm\n  FROM icu_pair_win\n  WHERE (ph BETWEEN 6.3 AND 7.8 OR ph IS NULL)\n    AND (CASE WHEN pco2_uom_norm_raw=\'kpa\' THEN pco2_raw*7.50062 ELSE pco2_raw END) BETWEEN 5 AND 200\n),\nicu_solo_pco2_std AS (\n  SELECT\n    hadm_id, stay_id, site_co2 AS site,\n    charttime AS sample_time,\n    CAST(NULL AS FLOAT64) AS ph,\n    CAST(NULL AS STRING)  AS ph_uom,            -- no pH here\n    CASE WHEN uom_nospace=\'kpa\' OR REGEXP_CONTAINS(valstr, r\'\\bkpa\\b\') THEN pco2_raw*7.50062 ELSE pco2_raw END AS pco2_mmHg,\n    \'mmhg\' AS pco2_uom_norm\n  FROM icu_co2\n  WHERE site_co2 IN (\'arterial\',\'venous\',\'other\')\n    AND pco2_raw BETWEEN 5 AND 200\n),\nicu_all AS (\n  SELECT * FROM icu_pairs_std\n  UNION ALL\n  SELECT * FROM icu_solo_pco2_std\n),\n\npoc_abg AS (\n  SELECT hadm_id,\n         ph            AS poc_abg_ph,\n         ph_uom        AS poc_abg_ph_uom,\n         pco2_mmHg     AS poc_abg_paco2,\n         \'mmhg\'        AS poc_abg_paco2_uom,\n         sample_time   AS poc_abg_time\n  FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY hadm_id ORDER BY sample_time) rn\n        FROM icu_all WHERE site=\'arterial\') WHERE rn=1\n),\npoc_vbg AS (\n  SELECT hadm_id,\n         ph            AS poc_vbg_ph,\n         ph_uom        AS poc_vbg_ph_uom,\n         pco2_mmHg     AS poc_vbg_paco2,\n         \'mmhg\'        AS poc_vbg_paco2_uom,\n         sample_time   AS poc_vbg_time\n  FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY hadm_id ORDER BY sample_time) rn\n        FROM icu_all WHERE site=\'venous\') WHERE rn=1\n),\npoc_other AS (\n  SELECT hadm_id,\n         ph            AS poc_other_ph,\n         ph_uom        AS poc_other_ph_uom,\n         pco2_mmHg     AS poc_other_paco2,\n         \'mmhg\'        AS poc_other_paco2_uom,\n         sample_time   AS poc_other_time\n  FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY hadm_id ORDER BY sample_time) rn\n        FROM icu_all WHERE site=\'other\') WHERE rn=1\n)\n\n/* ---------------- Final one row per hadm ---------------- */\nSELECT\n  h.hadm_id,\n  -- LAB-ABG / LAB-VBG / LAB-OTHER\n  la.lab_abg_ph, la.lab_abg_ph_uom, la.lab_abg_paco2, la.lab_abg_paco2_uom, la.lab_abg_time,\n  lv.lab_vbg_ph, lv.lab_vbg_ph_uom, lv.lab_vbg_paco2, lv.lab_vbg_paco2_uom, lv.lab_vbg_time,\n  lo.lab_other_ph, lo.lab_other_ph_uom, lo.lab_other_paco2, lo.lab_other_paco2_uom, lo.lab_other_time,\n  -- POC-ABG / POC-VBG / POC-OTHER\n  pa.poc_abg_ph, pa.poc_abg_ph_uom, pa.poc_abg_paco2, pa.poc_abg_paco2_uom, pa.poc_abg_time,\n  pv.poc_vbg_ph, pv.poc_vbg_ph_uom, pv.poc_vbg_paco2, pv.poc_vbg_paco2_uom, pv.poc_vbg_time,\n  po.poc_other_ph, po.poc_other_ph_uom, po.poc_other_paco2, po.poc_other_paco2_uom, po.poc_other_time,\n  -- First ABG across LAB+POC\n  (SELECT AS STRUCT src, t, ph, pco2\n   FROM (SELECT \'LAB\' AS src, la.lab_abg_time AS t, la.lab_abg_ph AS ph, la.lab_abg_paco2 AS pco2\n         UNION ALL\n         SELECT \'POC\', pa.poc_abg_time, pa.poc_abg_ph, pa.poc_abg_paco2)\n   WHERE t IS NOT NULL\n   ORDER BY t LIMIT 1) AS first_abg,\n  -- First VBG across LAB+POC\n  (SELECT AS STRUCT src, t, ph, pco2\n   FROM (SELECT \'LAB\' AS src, lv.lab_vbg_time AS t, lv.lab_vbg_ph AS ph, lv.lab_vbg_paco2 AS pco2\n         UNION ALL\n         SELECT \'POC\', pv.poc_vbg_time, pv.poc_vbg_ph, pv.poc_vbg_paco2)\n   WHERE t IS NOT NULL\n   ORDER BY t LIMIT 1) AS first_vbg,\n  -- First OTHER-source pCO2 across LAB+POC\n  (SELECT AS STRUCT src, t, ph, pco2\n   FROM (SELECT \'LAB\' AS src, lo.lab_other_time AS t, lo.lab_other_ph AS ph, lo.lab_other_paco2 AS pco2\n         UNION ALL\n         SELECT \'POC\', po.poc_other_time, po.poc_other_ph, po.poc_other_paco2)\n   WHERE t IS NOT NULL\n   ORDER BY t LIMIT 1) AS first_other\nFROM hadms h\nLEFT JOIN lab_abg la USING (hadm_id)\nLEFT JOIN lab_vbg lv USING (hadm_id)\nLEFT JOIN lab_other lo USING (hadm_id)\nLEFT JOIN poc_abg pa USING (hadm_id)\nLEFT JOIN poc_vbg pv USING (hadm_id)\nLEFT JOIN poc_other po USING (hadm_id)\n"""\n\nbg_pairs = run_sql_bq(bg_pairs_sql, params)\n\n# Flatten STRUCTs for first_abg, first_vbg, and first_other\nfor col in ["first_abg","first_vbg","first_other"]:\n    if col in bg_pairs.columns:\n        bg_pairs[f"{col}_src"]  = bg_pairs[col].apply(lambda x: x.get("src") if isinstance(x, dict) else None)\n        bg_pairs[f"{col}_time"] = bg_pairs[col].apply(lambda x: x.get("t")   if isinstance(x, dict) else None)\n        bg_pairs[f"{col}_ph"]   = bg_pairs[col].apply(lambda x: x.get("ph")  if isinstance(x, dict) else None)\n        bg_pairs[f"{col}_pco2"] = bg_pairs[col].apply(lambda x: x.get("pco2")if isinstance(x, dict) else None)\n        bg_pairs = bg_pairs.drop(columns=[col])\n\nbg_pairs.head(3)\n', 'execution_count': 8}
[NbConvertApp] msg_type: execute_result
[NbConvertApp] content: {'data': {'text/plain': '    hadm_id  lab_abg_ph lab_abg_ph_uom  lab_abg_paco2 lab_abg_paco2_uom  \\\n0  20000094         NaN           None            NaN              None   \n1  20000147        7.41          units           35.0              mmhg   \n2  20000235        7.45          units           43.0              mmhg   \n\n         lab_abg_time  lab_vbg_ph lab_vbg_ph_uom  lab_vbg_paco2  \\\n0                 NaT        7.41          units           47.0   \n1 2121-08-30 17:38:00         NaN           None            NaN   \n2 2139-11-27 11:46:00         NaN           None            NaN   \n\n  lab_vbg_paco2_uom  ... first_abg_ph  first_abg_pco2 first_vbg_src  \\\n0              mmhg  ...          NaN             NaN           LAB   \n1              None  ...         7.41            35.0           POC   \n2              None  ...         7.45            43.0          None   \n\n       first_vbg_time first_vbg_ph first_vbg_pco2  first_other_src  \\\n0 2150-03-02 15:53:00         7.41           47.0              POC   \n1 2121-08-30 22:00:00          NaN           20.0              POC   \n2                 NaT          NaN            NaN              LAB   \n\n     first_other_time  first_other_ph first_other_pco2  \n0 2150-03-02 15:21:00             NaN             41.0  \n1 2121-08-31 15:05:00             NaN             78.0  \n2 2139-11-13 06:35:00             7.0              NaN  \n\n[3 rows x 43 columns]', 'text/html': '<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>hadm_id</th>\n      <th>lab_abg_ph</th>\n      <th>lab_abg_ph_uom</th>\n      <th>lab_abg_paco2</th>\n      <th>lab_abg_paco2_uom</th>\n      <th>lab_abg_time</th>\n      <th>lab_vbg_ph</th>\n      <th>lab_vbg_ph_uom</th>\n      <th>lab_vbg_paco2</th>\n      <th>lab_vbg_paco2_uom</th>\n      <th>...</th>\n      <th>first_abg_ph</th>\n      <th>first_abg_pco2</th>\n      <th>first_vbg_src</th>\n      <th>first_vbg_time</th>\n      <th>first_vbg_ph</th>\n      <th>first_vbg_pco2</th>\n      <th>first_other_src</th>\n      <th>first_other_time</th>\n      <th>first_other_ph</th>\n      <th>first_other_pco2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20000094</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>NaT</td>\n      <td>7.41</td>\n      <td>units</td>\n      <td>47.0</td>\n      <td>mmhg</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>LAB</td>\n      <td>2150-03-02 15:53:00</td>\n      <td>7.41</td>\n      <td>47.0</td>\n      <td>POC</td>\n      <td>2150-03-02 15:21:00</td>\n      <td>NaN</td>\n      <td>41.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20000147</td>\n      <td>7.41</td>\n      <td>units</td>\n      <td>35.0</td>\n      <td>mmhg</td>\n      <td>2121-08-30 17:38:00</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>...</td>\n      <td>7.41</td>\n      <td>35.0</td>\n      <td>POC</td>\n      <td>2121-08-30 22:00:00</td>\n      <td>NaN</td>\n      <td>20.0</td>\n      <td>POC</td>\n      <td>2121-08-31 15:05:00</td>\n      <td>NaN</td>\n      <td>78.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20000235</td>\n      <td>7.45</td>\n      <td>units</td>\n      <td>43.0</td>\n      <td>mmhg</td>\n      <td>2139-11-27 11:46:00</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>...</td>\n      <td>7.45</td>\n      <td>43.0</td>\n      <td>None</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>LAB</td>\n      <td>2139-11-13 06:35:00</td>\n      <td>7.0</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 43 columns</p>\n</div>'}, 'metadata': {}, 'execution_count': 8}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 25
[NbConvertApp] Skipping non-executing cell 26
[NbConvertApp] Executing cell:
# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.


SQL["demo_sql"] = f"""
WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x)
SELECT
  a.hadm_id,
  a.subject_id,
  a.admittime,
  a.dischtime,
  a.deathtime,
  a.admission_type,
  a.admission_location,
  a.discharge_location,
  a.insurance,
  -- LOS (days)
  TIMESTAMP_DIFF(a.dischtime, a.admittime, HOUR) / 24.0 AS hosp_los_days,
  -- in-hospital death
  IF(a.deathtime IS NOT NULL, 1, 0) AS death_in_hosp,
  -- demographics
  p.gender,
  SAFE_CAST(ROUND(p.anchor_age + (EXTRACT(YEAR FROM a.admittime) - p.anchor_year), 1) AS FLOAT64) AS age_at_admit,
  -- 30-day all-cause mortality from admission
  IF(p.dod IS NOT NULL AND DATE_DIFF(DATE(p.dod), DATE(a.admittime), DAY) BETWEEN 0 AND 30, 1, 0) AS death_30d
FROM `{PHYS}.{HOSP}.admissions` a
JOIN hadms h USING (hadm_id)
JOIN `{PHYS}.{HOSP}.patients` p USING (subject_id)
"""
demo = run_sql_bq(sql("demo_sql"), {"hadms": hadm_list})
print("Demo rows:", len(demo))
demo.head(3)


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.\n\n\nSQL["demo_sql"] = f"""\nWITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x)\nSELECT\n  a.hadm_id,\n  a.subject_id,\n  a.admittime,\n  a.dischtime,\n  a.deathtime,\n  a.admission_type,\n  a.admission_location,\n  a.discharge_location,\n  a.insurance,\n  -- LOS (days)\n  TIMESTAMP_DIFF(a.dischtime, a.admittime, HOUR) / 24.0 AS hosp_los_days,\n  -- in-hospital death\n  IF(a.deathtime IS NOT NULL, 1, 0) AS death_in_hosp,\n  -- demographics\n  p.gender,\n  SAFE_CAST(ROUND(p.anchor_age + (EXTRACT(YEAR FROM a.admittime) - p.anchor_year), 1) AS FLOAT64) AS age_at_admit,\n  -- 30-day all-cause mortality from admission\n  IF(p.dod IS NOT NULL AND DATE_DIFF(DATE(p.dod), DATE(a.admittime), DAY) BETWEEN 0 AND 30, 1, 0) AS death_30d\nFROM `{PHYS}.{HOSP}.admissions` a\nJOIN hadms h USING (hadm_id)\nJOIN `{PHYS}.{HOSP}.patients` p USING (subject_id)\n"""\ndemo = run_sql_bq(sql("demo_sql"), {"hadms": hadm_list})\nprint("Demo rows:", len(demo))\ndemo.head(3)\n\n', 'execution_count': 9}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Demo rows: 115179\n'}
[NbConvertApp] msg_type: execute_result
[NbConvertApp] content: {'data': {'text/plain': '    hadm_id  subject_id           admittime           dischtime  \\\n0  26713233    10106244 2147-05-09 10:34:00 2147-05-12 13:43:00   \n1  27961368    15443666 2168-12-30 23:30:00 2169-01-05 16:02:00   \n2  23485217    10584718 2165-02-12 15:41:00 2165-03-06 08:20:00   \n\n            deathtime     admission_type  \\\n0                 NaT       DIRECT EMER.   \n1                 NaT  OBSERVATION ADMIT   \n2 2165-03-06 08:20:00           EW EMER.   \n\n                       admission_location discharge_location insurance  \\\n0                      PHYSICIAN REFERRAL               HOME   Private   \n1                          EMERGENCY ROOM   HOME HEALTH CARE  Medicare   \n2  TRANSFER FROM SKILLED NURSING FACILITY               DIED  Medicare   \n\n   hosp_los_days  death_in_hosp gender  age_at_admit  death_30d  \n0       3.125000              0      F          63.0          0  \n1       5.708333              0      F          76.0          0  \n2      21.708333              1      M          78.0          1  ', 'text/html': '<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>hadm_id</th>\n      <th>subject_id</th>\n      <th>admittime</th>\n      <th>dischtime</th>\n      <th>deathtime</th>\n      <th>admission_type</th>\n      <th>admission_location</th>\n      <th>discharge_location</th>\n      <th>insurance</th>\n      <th>hosp_los_days</th>\n      <th>death_in_hosp</th>\n      <th>gender</th>\n      <th>age_at_admit</th>\n      <th>death_30d</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>26713233</td>\n      <td>10106244</td>\n      <td>2147-05-09 10:34:00</td>\n      <td>2147-05-12 13:43:00</td>\n      <td>NaT</td>\n      <td>DIRECT EMER.</td>\n      <td>PHYSICIAN REFERRAL</td>\n      <td>HOME</td>\n      <td>Private</td>\n      <td>3.125000</td>\n      <td>0</td>\n      <td>F</td>\n      <td>63.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>27961368</td>\n      <td>15443666</td>\n      <td>2168-12-30 23:30:00</td>\n      <td>2169-01-05 16:02:00</td>\n      <td>NaT</td>\n      <td>OBSERVATION ADMIT</td>\n      <td>EMERGENCY ROOM</td>\n      <td>HOME HEALTH CARE</td>\n      <td>Medicare</td>\n      <td>5.708333</td>\n      <td>0</td>\n      <td>F</td>\n      <td>76.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>23485217</td>\n      <td>10584718</td>\n      <td>2165-02-12 15:41:00</td>\n      <td>2165-03-06 08:20:00</td>\n      <td>2165-03-06 08:20:00</td>\n      <td>EW EMER.</td>\n      <td>TRANSFER FROM SKILLED NURSING FACILITY</td>\n      <td>DIED</td>\n      <td>Medicare</td>\n      <td>21.708333</td>\n      <td>1</td>\n      <td>M</td>\n      <td>78.0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>'}, 'metadata': {}, 'execution_count': 9}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
# Purpose: Create reusable data-quality and merge guardrail helpers to prevent silent join errors.

# ==== Drop-in: safe merge utilities (one cell, run once) ====
import pandas as pd
from typing import Iterable, Optional, Literal

def _ensure_Int64(s: pd.Series) -> pd.Series:
    """Coerce to pandas nullable Int64 (preserves NA)."""
    return pd.to_numeric(s, errors="coerce").astype("Int64")

def strip_subject_cols(fr: pd.DataFrame) -> pd.DataFrame:
    """Remove any subject_id-like columns from a frame (e.g., 'subject_id', 'Subject_ID')."""
    return fr.drop(columns=[c for c in fr.columns if c.lower().startswith("subject_id")],
                   errors="ignore")

def safe_merge_on_hadm(
    left: pd.DataFrame,
    right: pd.DataFrame,
    *,
    right_name: str,
    take: Optional[Iterable[str]] = None,
    order_by: Optional[Iterable[str]] = None,
    check_subject: Literal[False, "warn", "raise"] = False,
) -> pd.DataFrame:
    """
    Left-merge 'right' into 'left' on hadm_id, returning a copy of left with right's columns.
    - Dedupes right on hadm_id (optionally using order_by to pick the first row).
    - Optionally restricts right columns via `take`.
    - Optionally audits subject_id agreement before dropping subject_id from right.
    - Always strips subject_id-like columns from the right to prevent *_x/_y suffixes.
    - Raises if any *_x/_y suffixes still appear (indicates overlapping names besides hadm_id).
    """
    if "hadm_id" not in left.columns:
        raise KeyError(f"left frame lacks hadm_id before merging {right_name}")
    if "hadm_id" not in right.columns:
        raise KeyError(f"{right_name} lacks hadm_id")

    L = left.copy()
    R = right.copy()

    # Standardize dtypes of keys
    L["hadm_id"] = _ensure_Int64(L["hadm_id"])
    R["hadm_id"] = _ensure_Int64(R["hadm_id"])
    if "subject_id" in L.columns:
        L["subject_id"] = _ensure_Int64(L["subject_id"])
    if "subject_id" in R.columns:
        R["subject_id"] = _ensure_Int64(R["subject_id"])

    # Dedupe RIGHT by hadm_id (optionally order_by first)
    if order_by:
        R = (R.sort_values(list(order_by))
               .drop_duplicates(subset=["hadm_id"], keep="first"))
    else:
        R = R.drop_duplicates(subset=["hadm_id"], keep="first")

    # Optional subject_id consistency audit (before stripping)
    if check_subject and ("subject_id" in L.columns) and ("subject_id" in R.columns):
        # Join only on hadm_id where both sides have subject_id
        tmp = (L[["hadm_id", "subject_id"]]
                 .merge(R[["hadm_id", "subject_id"]],
                        on="hadm_id", how="inner", suffixes=("_L","_R")))
        mism = (tmp["subject_id_L"].notna() & tmp["subject_id_R"].notna() &
                (tmp["subject_id_L"] != tmp["subject_id_R"]))
        n_mism = int(mism.sum())
        if n_mism > 0:
            sample_ids = tmp.loc[mism, "hadm_id"].head(10).tolist()
            msg = (f"[{right_name}] subject_id mismatch on {n_mism} hadm_id(s). "
                   f"Examples: {sample_ids}")
            if check_subject == "raise":
                raise ValueError(msg)
            else:
                print("WARNING:", msg)

    # Limit right columns (avoid accidental overlaps)
    if take is not None:
        keep = ["hadm_id"] + [c for c in take if c != "hadm_id"]
        R = R[[c for c in keep if c in R.columns]]

    # Always strip subject_id-like columns from right to prevent *_x/_y
    R = strip_subject_cols(R)

    # Final merge
    out = L.merge(R, on="hadm_id", how="left", suffixes=("", ""))

    # Guard: no suffixes should be present
    bad = [c for c in out.columns if c.endswith("_x") or c.endswith("_y")]
    if bad:
        raise RuntimeError(
            f"Merge with {right_name} produced suffixed columns {bad}. "
            "You likely have overlapping column names other than hadm_id."
        )
    return out

print("Safe merge helpers loaded.")

[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Purpose: Create reusable data-quality and merge guardrail helpers to prevent silent join errors.\n\n# ==== Drop-in: safe merge utilities (one cell, run once) ====\nimport pandas as pd\nfrom typing import Iterable, Optional, Literal\n\ndef _ensure_Int64(s: pd.Series) -> pd.Series:\n    """Coerce to pandas nullable Int64 (preserves NA)."""\n    return pd.to_numeric(s, errors="coerce").astype("Int64")\n\ndef strip_subject_cols(fr: pd.DataFrame) -> pd.DataFrame:\n    """Remove any subject_id-like columns from a frame (e.g., \'subject_id\', \'Subject_ID\')."""\n    return fr.drop(columns=[c for c in fr.columns if c.lower().startswith("subject_id")],\n                   errors="ignore")\n\ndef safe_merge_on_hadm(\n    left: pd.DataFrame,\n    right: pd.DataFrame,\n    *,\n    right_name: str,\n    take: Optional[Iterable[str]] = None,\n    order_by: Optional[Iterable[str]] = None,\n    check_subject: Literal[False, "warn", "raise"] = False,\n) -> pd.DataFrame:\n    """\n    Left-merge \'right\' into \'left\' on hadm_id, returning a copy of left with right\'s columns.\n    - Dedupes right on hadm_id (optionally using order_by to pick the first row).\n    - Optionally restricts right columns via `take`.\n    - Optionally audits subject_id agreement before dropping subject_id from right.\n    - Always strips subject_id-like columns from the right to prevent *_x/_y suffixes.\n    - Raises if any *_x/_y suffixes still appear (indicates overlapping names besides hadm_id).\n    """\n    if "hadm_id" not in left.columns:\n        raise KeyError(f"left frame lacks hadm_id before merging {right_name}")\n    if "hadm_id" not in right.columns:\n        raise KeyError(f"{right_name} lacks hadm_id")\n\n    L = left.copy()\n    R = right.copy()\n\n    # Standardize dtypes of keys\n    L["hadm_id"] = _ensure_Int64(L["hadm_id"])\n    R["hadm_id"] = _ensure_Int64(R["hadm_id"])\n    if "subject_id" in L.columns:\n        L["subject_id"] = _ensure_Int64(L["subject_id"])\n    if "subject_id" in R.columns:\n        R["subject_id"] = _ensure_Int64(R["subject_id"])\n\n    # Dedupe RIGHT by hadm_id (optionally order_by first)\n    if order_by:\n        R = (R.sort_values(list(order_by))\n               .drop_duplicates(subset=["hadm_id"], keep="first"))\n    else:\n        R = R.drop_duplicates(subset=["hadm_id"], keep="first")\n\n    # Optional subject_id consistency audit (before stripping)\n    if check_subject and ("subject_id" in L.columns) and ("subject_id" in R.columns):\n        # Join only on hadm_id where both sides have subject_id\n        tmp = (L[["hadm_id", "subject_id"]]\n                 .merge(R[["hadm_id", "subject_id"]],\n                        on="hadm_id", how="inner", suffixes=("_L","_R")))\n        mism = (tmp["subject_id_L"].notna() & tmp["subject_id_R"].notna() &\n                (tmp["subject_id_L"] != tmp["subject_id_R"]))\n        n_mism = int(mism.sum())\n        if n_mism > 0:\n            sample_ids = tmp.loc[mism, "hadm_id"].head(10).tolist()\n            msg = (f"[{right_name}] subject_id mismatch on {n_mism} hadm_id(s). "\n                   f"Examples: {sample_ids}")\n            if check_subject == "raise":\n                raise ValueError(msg)\n            else:\n                print("WARNING:", msg)\n\n    # Limit right columns (avoid accidental overlaps)\n    if take is not None:\n        keep = ["hadm_id"] + [c for c in take if c != "hadm_id"]\n        R = R[[c for c in keep if c in R.columns]]\n\n    # Always strip subject_id-like columns from right to prevent *_x/_y\n    R = strip_subject_cols(R)\n\n    # Final merge\n    out = L.merge(R, on="hadm_id", how="left", suffixes=("", ""))\n\n    # Guard: no suffixes should be present\n    bad = [c for c in out.columns if c.endswith("_x") or c.endswith("_y")]\n    if bad:\n        raise RuntimeError(\n            f"Merge with {right_name} produced suffixed columns {bad}. "\n            "You likely have overlapping column names other than hadm_id."\n        )\n    return out\n\nprint("Safe merge helpers loaded.")\n', 'execution_count': 10}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Safe merge helpers loaded.\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 29
[NbConvertApp] Skipping non-executing cell 30
[NbConvertApp] Executing cell:
# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.

race_eth_sql = rf"""
WITH hadms AS (
  SELECT x AS hadm_id
  FROM UNNEST(@hadms) AS x
),

-- Hospital admission "race" text
hosp AS (
  SELECT a.hadm_id, LOWER(TRIM(a.race)) AS race_hosp_raw
  FROM `{PHYS}.{HOSP}.admissions` a
  JOIN hadms hm USING (hadm_id)
),

-- Earliest ED stay leading to the admission; take its "race" text if present
ed_first AS (
  SELECT
    e.hadm_id,
    (ARRAY_AGG(STRUCT(e.intime AS intime, LOWER(TRIM(e.race)) AS race_ed_raw)
               ORDER BY e.intime ASC LIMIT 1))[OFFSET(0)] AS pick
  FROM `{PHYS}.{ED}.edstays` e
  JOIN hadms hm USING (hadm_id)
  GROUP BY e.hadm_id
),
ed AS (
  SELECT hadm_id, pick.race_ed_raw
  FROM ed_first
),

-- Combine ED + Hospital for maximum coverage
comb AS (
  SELECT
    hm.hadm_id,
    ho.race_hosp_raw,
    ed.race_ed_raw,
    TRIM(REGEXP_REPLACE(CONCAT(COALESCE(ho.race_hosp_raw,''), ' ', COALESCE(ed.race_ed_raw,'')), r'\s+', ' ')) AS race_text_any
  FROM hadms hm
  LEFT JOIN hosp ho USING (hadm_id)
  LEFT JOIN ed   ed USING (hadm_id)
),

-- Tokenization to OMB families + Hispanic ethnicity
tok AS (
  SELECT
    hadm_id, race_hosp_raw, race_ed_raw, race_text_any,

    -- Ethnicity (Hispanic)
    REGEXP_CONTAINS(race_text_any, r'\b(hispanic|latinx|latino|latina)\b') AS is_hisp,

    -- Race families (use boundaries to reduce false positives)
    REGEXP_CONTAINS(race_text_any, r'american\s+indian|\balaska\b') AS is_aian,
    REGEXP_CONTAINS(race_text_any, r'\basian\b') AS is_asian,
    REGEXP_CONTAINS(race_text_any, r'\b(black|african\s+american)\b') AS is_black,
    REGEXP_CONTAINS(race_text_any, r'hawaiian|pacific\s+islander') AS is_nhopi,
    REGEXP_CONTAINS(race_text_any, r'\bwhite\b|caucasian') AS is_white,

    -- Unknown/other indicators
    REGEXP_CONTAINS(race_text_any, r'unknown|other|declined|unable|not\s+reported|missing|null') AS is_unknown_any,

    -- Multi-race hints
    REGEXP_CONTAINS(race_text_any, r'(two|2)\s+or\s+more|multi|biracial|multiracial') AS is_multi_hint
  FROM comb
),

-- Decide ethnicity per NIH
ethn AS (
  SELECT
    hadm_id, race_hosp_raw, race_ed_raw, race_text_any,
    CASE
      WHEN is_hisp THEN 'Hispanic or Latino'
      WHEN (race_text_any IS NULL OR race_text_any = '' OR is_unknown_any) THEN 'Unknown or Not Reported'
      ELSE 'Not Hispanic or Latino'
    END AS nih_ethnicity,
    (CAST(is_aian AS INT64) + CAST(is_asian AS INT64) + CAST(is_black AS INT64)
     + CAST(is_nhopi AS INT64) + CAST(is_white AS INT64)) AS race_hits,
    is_aian, is_asian, is_black, is_nhopi, is_white, is_multi_hint, is_unknown_any
  FROM tok
),

-- Decide race per NIH/OMB (1997)
race_assign AS (
  SELECT
    hadm_id, race_hosp_raw, race_ed_raw, race_text_any, nih_ethnicity,
    CASE
      WHEN race_hits >= 2 OR is_multi_hint THEN 'More than one race'
      WHEN is_aian THEN 'American Indian or Alaska Native'
      WHEN is_asian THEN 'Asian'
      WHEN is_black THEN 'Black or African American'
      WHEN is_nhopi THEN 'Native Hawaiian or Other Pacific Islander'
      WHEN is_white THEN 'White'
      WHEN is_unknown_any OR race_text_any IS NULL OR race_text_any = '' THEN 'Unknown or Not Reported'
      ELSE 'Unknown or Not Reported'
    END AS nih_race
  FROM ethn
)

SELECT hadm_id, race_hosp_raw, race_ed_raw, nih_race, nih_ethnicity
FROM race_assign
"""
race_eth = run_sql_bq(race_eth_sql, {"hadms": hadm_list})
print("Race/Eth rows:", len(race_eth))
race_eth.head(3)


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.\n\nrace_eth_sql = rf"""\nWITH hadms AS (\n  SELECT x AS hadm_id\n  FROM UNNEST(@hadms) AS x\n),\n\n-- Hospital admission "race" text\nhosp AS (\n  SELECT a.hadm_id, LOWER(TRIM(a.race)) AS race_hosp_raw\n  FROM `{PHYS}.{HOSP}.admissions` a\n  JOIN hadms hm USING (hadm_id)\n),\n\n-- Earliest ED stay leading to the admission; take its "race" text if present\ned_first AS (\n  SELECT\n    e.hadm_id,\n    (ARRAY_AGG(STRUCT(e.intime AS intime, LOWER(TRIM(e.race)) AS race_ed_raw)\n               ORDER BY e.intime ASC LIMIT 1))[OFFSET(0)] AS pick\n  FROM `{PHYS}.{ED}.edstays` e\n  JOIN hadms hm USING (hadm_id)\n  GROUP BY e.hadm_id\n),\ned AS (\n  SELECT hadm_id, pick.race_ed_raw\n  FROM ed_first\n),\n\n-- Combine ED + Hospital for maximum coverage\ncomb AS (\n  SELECT\n    hm.hadm_id,\n    ho.race_hosp_raw,\n    ed.race_ed_raw,\n    TRIM(REGEXP_REPLACE(CONCAT(COALESCE(ho.race_hosp_raw,\'\'), \' \', COALESCE(ed.race_ed_raw,\'\')), r\'\\s+\', \' \')) AS race_text_any\n  FROM hadms hm\n  LEFT JOIN hosp ho USING (hadm_id)\n  LEFT JOIN ed   ed USING (hadm_id)\n),\n\n-- Tokenization to OMB families + Hispanic ethnicity\ntok AS (\n  SELECT\n    hadm_id, race_hosp_raw, race_ed_raw, race_text_any,\n\n    -- Ethnicity (Hispanic)\n    REGEXP_CONTAINS(race_text_any, r\'\\b(hispanic|latinx|latino|latina)\\b\') AS is_hisp,\n\n    -- Race families (use boundaries to reduce false positives)\n    REGEXP_CONTAINS(race_text_any, r\'american\\s+indian|\\balaska\\b\') AS is_aian,\n    REGEXP_CONTAINS(race_text_any, r\'\\basian\\b\') AS is_asian,\n    REGEXP_CONTAINS(race_text_any, r\'\\b(black|african\\s+american)\\b\') AS is_black,\n    REGEXP_CONTAINS(race_text_any, r\'hawaiian|pacific\\s+islander\') AS is_nhopi,\n    REGEXP_CONTAINS(race_text_any, r\'\\bwhite\\b|caucasian\') AS is_white,\n\n    -- Unknown/other indicators\n    REGEXP_CONTAINS(race_text_any, r\'unknown|other|declined|unable|not\\s+reported|missing|null\') AS is_unknown_any,\n\n    -- Multi-race hints\n    REGEXP_CONTAINS(race_text_any, r\'(two|2)\\s+or\\s+more|multi|biracial|multiracial\') AS is_multi_hint\n  FROM comb\n),\n\n-- Decide ethnicity per NIH\nethn AS (\n  SELECT\n    hadm_id, race_hosp_raw, race_ed_raw, race_text_any,\n    CASE\n      WHEN is_hisp THEN \'Hispanic or Latino\'\n      WHEN (race_text_any IS NULL OR race_text_any = \'\' OR is_unknown_any) THEN \'Unknown or Not Reported\'\n      ELSE \'Not Hispanic or Latino\'\n    END AS nih_ethnicity,\n    (CAST(is_aian AS INT64) + CAST(is_asian AS INT64) + CAST(is_black AS INT64)\n     + CAST(is_nhopi AS INT64) + CAST(is_white AS INT64)) AS race_hits,\n    is_aian, is_asian, is_black, is_nhopi, is_white, is_multi_hint, is_unknown_any\n  FROM tok\n),\n\n-- Decide race per NIH/OMB (1997)\nrace_assign AS (\n  SELECT\n    hadm_id, race_hosp_raw, race_ed_raw, race_text_any, nih_ethnicity,\n    CASE\n      WHEN race_hits >= 2 OR is_multi_hint THEN \'More than one race\'\n      WHEN is_aian THEN \'American Indian or Alaska Native\'\n      WHEN is_asian THEN \'Asian\'\n      WHEN is_black THEN \'Black or African American\'\n      WHEN is_nhopi THEN \'Native Hawaiian or Other Pacific Islander\'\n      WHEN is_white THEN \'White\'\n      WHEN is_unknown_any OR race_text_any IS NULL OR race_text_any = \'\' THEN \'Unknown or Not Reported\'\n      ELSE \'Unknown or Not Reported\'\n    END AS nih_race\n  FROM ethn\n)\n\nSELECT hadm_id, race_hosp_raw, race_ed_raw, nih_race, nih_ethnicity\nFROM race_assign\n"""\nrace_eth = run_sql_bq(race_eth_sql, {"hadms": hadm_list})\nprint("Race/Eth rows:", len(race_eth))\nrace_eth.head(3)\n\n', 'execution_count': 11}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Race/Eth rows: 115179\n'}
[NbConvertApp] msg_type: execute_result
[NbConvertApp] content: {'data': {'text/plain': '    hadm_id           race_hosp_raw race_ed_raw nih_race  \\\n0  20000094                   white        None    White   \n1  20000147  white - other european        None    White   \n2  20000235                   white        None    White   \n\n             nih_ethnicity  \n0   Not Hispanic or Latino  \n1  Unknown or Not Reported  \n2   Not Hispanic or Latino  ', 'text/html': '<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>hadm_id</th>\n      <th>race_hosp_raw</th>\n      <th>race_ed_raw</th>\n      <th>nih_race</th>\n      <th>nih_ethnicity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20000094</td>\n      <td>white</td>\n      <td>None</td>\n      <td>White</td>\n      <td>Not Hispanic or Latino</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20000147</td>\n      <td>white - other european</td>\n      <td>None</td>\n      <td>White</td>\n      <td>Unknown or Not Reported</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20000235</td>\n      <td>white</td>\n      <td>None</td>\n      <td>White</td>\n      <td>Not Hispanic or Latino</td>\n    </tr>\n  </tbody>\n</table>\n</div>'}, 'metadata': {}, 'execution_count': 11}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 32
[NbConvertApp] Skipping non-executing cell 33
[NbConvertApp] Executing cell:
# Purpose: Pull ED triage/first-vitals by loading ED tables once and filtering to cohort hadm_ids in pandas.

hadms_for_ed = set(
    pd.Series(hadm_list)
    .dropna()
    .astype("int64")
    .tolist()
)

# 1) ED stay map (full table), then restrict to cohort hadm_ids locally.
SQL["edmap_all_sql"] = f"""
SELECT stay_id, hadm_id, intime
FROM `{PHYS}.{ED}.edstays`
WHERE hadm_id IS NOT NULL
"""
edmap_all = run_sql_bq(sql("edmap_all_sql"))
edmap_all["hadm_id"] = pd.to_numeric(edmap_all["hadm_id"], errors="coerce").astype("Int64")
edmap_all["stay_id"] = pd.to_numeric(edmap_all["stay_id"], errors="coerce").astype("Int64")
edmap_all["intime"] = pd.to_datetime(edmap_all["intime"], errors="coerce")

edmap = edmap_all[edmap_all["hadm_id"].isin(hadms_for_ed)].copy()
edmap = edmap.drop_duplicates(subset=["stay_id", "hadm_id"])
print("ED map rows (cohort):", len(edmap))

if edmap.empty:
    ed_triage = pd.DataFrame(columns=[
        "hadm_id", "ed_triage_temp", "ed_triage_hr", "ed_triage_rr",
        "ed_triage_o2sat", "ed_triage_sbp", "ed_triage_dbp", "ed_triage_pain",
        "ed_triage_acuity", "ed_triage_cc",
    ])
    ed_first = pd.DataFrame(columns=[
        "hadm_id", "ed_first_vitals_time", "ed_first_temp", "ed_first_hr",
        "ed_first_rr", "ed_first_o2sat", "ed_first_sbp", "ed_first_dbp",
        "ed_first_rhythm", "ed_first_pain",
    ])
else:
    # 2) ED triage (full table), then reduce to earliest ED stay per hadm.
    SQL["ed_triage_all_sql"] = f"""
    SELECT
      stay_id,
      temperature    AS ed_triage_temp,
      heartrate      AS ed_triage_hr,
      resprate       AS ed_triage_rr,
      o2sat          AS ed_triage_o2sat,
      sbp            AS ed_triage_sbp,
      dbp            AS ed_triage_dbp,
      pain           AS ed_triage_pain,
      acuity         AS ed_triage_acuity,
      chiefcomplaint AS ed_triage_cc
    FROM `{PHYS}.{ED}.triage`
    """
    tri_all = run_sql_bq(sql("ed_triage_all_sql"))
    tri_all["stay_id"] = pd.to_numeric(tri_all["stay_id"], errors="coerce").astype("Int64")

    tri_merged = (
        edmap[["stay_id", "hadm_id", "intime"]]
        .merge(tri_all, on="stay_id", how="left")
    )
    ed_triage = (
        tri_merged
        .sort_values(["hadm_id", "intime"], na_position="last")
        .drop_duplicates(subset=["hadm_id"], keep="first")
        .drop(columns=["stay_id", "intime"])
        .reset_index(drop=True)
    )
    print("ED triage rows:", len(ed_triage))

    # 3) First vitals per stay across full ED vitals table, then reduce to earliest vitals-time per hadm.
    SQL["ed_first_vitals_all_sql"] = f"""
    WITH vs_ranked AS (
      SELECT
        v.stay_id,
        v.charttime,
        v.temperature,
        v.heartrate,
        v.resprate,
        v.o2sat,
        v.sbp,
        v.dbp,
        v.rhythm,
        v.pain,
        ROW_NUMBER() OVER (PARTITION BY v.stay_id ORDER BY v.charttime) AS rn
      FROM `{PHYS}.{ED}.vitalsign` v
    )
    SELECT
      stay_id,
      charttime    AS ed_first_vitals_time,
      temperature  AS ed_first_temp,
      heartrate    AS ed_first_hr,
      resprate     AS ed_first_rr,
      o2sat        AS ed_first_o2sat,
      sbp          AS ed_first_sbp,
      dbp          AS ed_first_dbp,
      rhythm       AS ed_first_rhythm,
      pain         AS ed_first_pain
    FROM vs_ranked
    WHERE rn = 1
    """
    first_stay_all = run_sql_bq(sql("ed_first_vitals_all_sql"))
    first_stay_all["stay_id"] = pd.to_numeric(first_stay_all["stay_id"], errors="coerce").astype("Int64")
    first_stay_all["ed_first_vitals_time"] = pd.to_datetime(first_stay_all["ed_first_vitals_time"], errors="coerce")

    first_merged = (
        edmap[["stay_id", "hadm_id"]]
        .merge(first_stay_all, on="stay_id", how="left")
    )
    ed_first = (
        first_merged
        .sort_values(["hadm_id", "ed_first_vitals_time"], na_position="last")
        .drop_duplicates(subset=["hadm_id"], keep="first")
        .drop(columns=["stay_id"])
        .reset_index(drop=True)
    )
    print("ED first vitals rows:", len(ed_first))

[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Purpose: Pull ED triage/first-vitals by loading ED tables once and filtering to cohort hadm_ids in pandas.\n\nhadms_for_ed = set(\n    pd.Series(hadm_list)\n    .dropna()\n    .astype("int64")\n    .tolist()\n)\n\n# 1) ED stay map (full table), then restrict to cohort hadm_ids locally.\nSQL["edmap_all_sql"] = f"""\nSELECT stay_id, hadm_id, intime\nFROM `{PHYS}.{ED}.edstays`\nWHERE hadm_id IS NOT NULL\n"""\nedmap_all = run_sql_bq(sql("edmap_all_sql"))\nedmap_all["hadm_id"] = pd.to_numeric(edmap_all["hadm_id"], errors="coerce").astype("Int64")\nedmap_all["stay_id"] = pd.to_numeric(edmap_all["stay_id"], errors="coerce").astype("Int64")\nedmap_all["intime"] = pd.to_datetime(edmap_all["intime"], errors="coerce")\n\nedmap = edmap_all[edmap_all["hadm_id"].isin(hadms_for_ed)].copy()\nedmap = edmap.drop_duplicates(subset=["stay_id", "hadm_id"])\nprint("ED map rows (cohort):", len(edmap))\n\nif edmap.empty:\n    ed_triage = pd.DataFrame(columns=[\n        "hadm_id", "ed_triage_temp", "ed_triage_hr", "ed_triage_rr",\n        "ed_triage_o2sat", "ed_triage_sbp", "ed_triage_dbp", "ed_triage_pain",\n        "ed_triage_acuity", "ed_triage_cc",\n    ])\n    ed_first = pd.DataFrame(columns=[\n        "hadm_id", "ed_first_vitals_time", "ed_first_temp", "ed_first_hr",\n        "ed_first_rr", "ed_first_o2sat", "ed_first_sbp", "ed_first_dbp",\n        "ed_first_rhythm", "ed_first_pain",\n    ])\nelse:\n    # 2) ED triage (full table), then reduce to earliest ED stay per hadm.\n    SQL["ed_triage_all_sql"] = f"""\n    SELECT\n      stay_id,\n      temperature    AS ed_triage_temp,\n      heartrate      AS ed_triage_hr,\n      resprate       AS ed_triage_rr,\n      o2sat          AS ed_triage_o2sat,\n      sbp            AS ed_triage_sbp,\n      dbp            AS ed_triage_dbp,\n      pain           AS ed_triage_pain,\n      acuity         AS ed_triage_acuity,\n      chiefcomplaint AS ed_triage_cc\n    FROM `{PHYS}.{ED}.triage`\n    """\n    tri_all = run_sql_bq(sql("ed_triage_all_sql"))\n    tri_all["stay_id"] = pd.to_numeric(tri_all["stay_id"], errors="coerce").astype("Int64")\n\n    tri_merged = (\n        edmap[["stay_id", "hadm_id", "intime"]]\n        .merge(tri_all, on="stay_id", how="left")\n    )\n    ed_triage = (\n        tri_merged\n        .sort_values(["hadm_id", "intime"], na_position="last")\n        .drop_duplicates(subset=["hadm_id"], keep="first")\n        .drop(columns=["stay_id", "intime"])\n        .reset_index(drop=True)\n    )\n    print("ED triage rows:", len(ed_triage))\n\n    # 3) First vitals per stay across full ED vitals table, then reduce to earliest vitals-time per hadm.\n    SQL["ed_first_vitals_all_sql"] = f"""\n    WITH vs_ranked AS (\n      SELECT\n        v.stay_id,\n        v.charttime,\n        v.temperature,\n        v.heartrate,\n        v.resprate,\n        v.o2sat,\n        v.sbp,\n        v.dbp,\n        v.rhythm,\n        v.pain,\n        ROW_NUMBER() OVER (PARTITION BY v.stay_id ORDER BY v.charttime) AS rn\n      FROM `{PHYS}.{ED}.vitalsign` v\n    )\n    SELECT\n      stay_id,\n      charttime    AS ed_first_vitals_time,\n      temperature  AS ed_first_temp,\n      heartrate    AS ed_first_hr,\n      resprate     AS ed_first_rr,\n      o2sat        AS ed_first_o2sat,\n      sbp          AS ed_first_sbp,\n      dbp          AS ed_first_dbp,\n      rhythm       AS ed_first_rhythm,\n      pain         AS ed_first_pain\n    FROM vs_ranked\n    WHERE rn = 1\n    """\n    first_stay_all = run_sql_bq(sql("ed_first_vitals_all_sql"))\n    first_stay_all["stay_id"] = pd.to_numeric(first_stay_all["stay_id"], errors="coerce").astype("Int64")\n    first_stay_all["ed_first_vitals_time"] = pd.to_datetime(first_stay_all["ed_first_vitals_time"], errors="coerce")\n\n    first_merged = (\n        edmap[["stay_id", "hadm_id"]]\n        .merge(first_stay_all, on="stay_id", how="left")\n    )\n    ed_first = (\n        first_merged\n        .sort_values(["hadm_id", "ed_first_vitals_time"], na_position="last")\n        .drop_duplicates(subset=["hadm_id"], keep="first")\n        .drop(columns=["stay_id"])\n        .reset_index(drop=True)\n    )\n    print("ED first vitals rows:", len(ed_first))\n', 'execution_count': 12}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ED map rows (cohort): 41394\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ED triage rows: 41325\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ED first vitals rows: 41325\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 35
[NbConvertApp] Skipping non-executing cell 36
[NbConvertApp] Executing cell:
# Purpose: Build first-ICU metadata via full-table pull + local hadm filter to avoid large ARRAY parameter stalls.

SQL["icu_all_sql"] = f"""
SELECT
  hadm_id,
  stay_id AS first_icu_stay_id,
  intime  AS icu_intime,
  outtime AS icu_outtime
FROM `{PHYS}.{ICU}.icustays`
WHERE hadm_id IS NOT NULL
"""
icu_all = run_sql_bq(sql("icu_all_sql"))
icu_all["hadm_id"] = pd.to_numeric(icu_all["hadm_id"], errors="coerce").astype("Int64")
icu_all["first_icu_stay_id"] = pd.to_numeric(icu_all["first_icu_stay_id"], errors="coerce").astype("Int64")
icu_all["icu_intime"] = pd.to_datetime(icu_all["icu_intime"], errors="coerce")
icu_all["icu_outtime"] = pd.to_datetime(icu_all["icu_outtime"], errors="coerce")

hadms_for_icu = set(pd.Series(hadm_list).dropna().astype("int64").tolist())
icu_all = icu_all[icu_all["hadm_id"].isin(hadms_for_icu)].copy()

icu_meta = (
    icu_all
    .sort_values(["hadm_id", "icu_intime", "first_icu_stay_id"], na_position="last")
    .drop_duplicates(subset=["hadm_id"], keep="first")
    .reset_index(drop=True)
)
icu_meta["icu_los_days"] = (
    (icu_meta["icu_outtime"] - icu_meta["icu_intime"]).dt.total_seconds() / 86400.0
)

print("ICU meta rows:", len(icu_meta))

[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Purpose: Build first-ICU metadata via full-table pull + local hadm filter to avoid large ARRAY parameter stalls.\n\nSQL["icu_all_sql"] = f"""\nSELECT\n  hadm_id,\n  stay_id AS first_icu_stay_id,\n  intime  AS icu_intime,\n  outtime AS icu_outtime\nFROM `{PHYS}.{ICU}.icustays`\nWHERE hadm_id IS NOT NULL\n"""\nicu_all = run_sql_bq(sql("icu_all_sql"))\nicu_all["hadm_id"] = pd.to_numeric(icu_all["hadm_id"], errors="coerce").astype("Int64")\nicu_all["first_icu_stay_id"] = pd.to_numeric(icu_all["first_icu_stay_id"], errors="coerce").astype("Int64")\nicu_all["icu_intime"] = pd.to_datetime(icu_all["icu_intime"], errors="coerce")\nicu_all["icu_outtime"] = pd.to_datetime(icu_all["icu_outtime"], errors="coerce")\n\nhadms_for_icu = set(pd.Series(hadm_list).dropna().astype("int64").tolist())\nicu_all = icu_all[icu_all["hadm_id"].isin(hadms_for_icu)].copy()\n\nicu_meta = (\n    icu_all\n    .sort_values(["hadm_id", "icu_intime", "first_icu_stay_id"], na_position="last")\n    .drop_duplicates(subset=["hadm_id"], keep="first")\n    .reset_index(drop=True)\n)\nicu_meta["icu_los_days"] = (\n    (icu_meta["icu_outtime"] - icu_meta["icu_intime"]).dt.total_seconds() / 86400.0\n)\n\nprint("ICU meta rows:", len(icu_meta))\n', 'execution_count': 13}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ICU meta rows: 85215\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 38
[NbConvertApp] Skipping non-executing cell 39
[NbConvertApp] Executing cell:
# Purpose: Build IMV/NIV ICD flags from targeted procedure codes, then filter to cohort hadm_ids locally.

SQL["vent_proc_sql"] = f"""
SELECT
  hadm_id,
  icd_version,
  REPLACE(icd_code, '.', '') AS code_norm
FROM `{PHYS}.{HOSP}.procedures_icd`
WHERE
  (icd_version = 10 AND icd_code IN ('5A1935Z','5A1945Z','5A1955Z','0BH17EZ','0BH18EZ','5A09357','5A09457','5A09557'))
  OR
  (icd_version = 9 AND REPLACE(icd_code, '.', '') IN ('9670','9671','9672','9604','9390','9391','9399'))
"""

vent_proc = run_sql_bq(sql("vent_proc_sql"))
vent_proc["hadm_id"] = pd.to_numeric(vent_proc["hadm_id"], errors="coerce").astype("Int64")
vent_proc["icd_version"] = pd.to_numeric(vent_proc["icd_version"], errors="coerce").astype("Int64")
vent_proc["code_norm"] = vent_proc["code_norm"].astype(str)

hadms_for_vent = set(pd.Series(hadm_list).dropna().astype("int64").tolist())
vent_proc = vent_proc[vent_proc["hadm_id"].isin(hadms_for_vent)].copy()

imv_codes_10 = {"5A1935Z", "5A1945Z", "5A1955Z", "0BH17EZ", "0BH18EZ"}
imv_codes_9 = {"9670", "9671", "9672", "9604"}
niv_codes_10 = {"5A09357", "5A09457", "5A09557"}
niv_codes_9 = {"9390", "9391", "9399"}

vent_proc["imv_hit"] = (
    ((vent_proc["icd_version"] == 10) & vent_proc["code_norm"].isin(imv_codes_10)) |
    ((vent_proc["icd_version"] == 9) & vent_proc["code_norm"].isin(imv_codes_9))
)
vent_proc["niv_hit"] = (
    ((vent_proc["icd_version"] == 10) & vent_proc["code_norm"].isin(niv_codes_10)) |
    ((vent_proc["icd_version"] == 9) & vent_proc["code_norm"].isin(niv_codes_9))
)

vent = (
    vent_proc
    .groupby("hadm_id", as_index=False)
    .agg(
        imv_flag=("imv_hit", "max"),
        niv_flag=("niv_hit", "max"),
    )
)
vent["imv_flag"] = vent["imv_flag"].astype(int)
vent["niv_flag"] = vent["niv_flag"].astype(int)
vent["any_vent_flag"] = ((vent["imv_flag"] == 1) | (vent["niv_flag"] == 1)).astype(int)

# Preserve one row per hadm in hadm_list even if no vent procedure codes were found.
vent = pd.DataFrame({"hadm_id": pd.Series(hadm_list, dtype="Int64")}).drop_duplicates().merge(vent, on="hadm_id", how="left")
vent[["imv_flag", "niv_flag", "any_vent_flag"]] = vent[["imv_flag", "niv_flag", "any_vent_flag"]].fillna(0).astype(int)

print("Vent rows:", len(vent))

[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Purpose: Build IMV/NIV ICD flags from targeted procedure codes, then filter to cohort hadm_ids locally.\n\nSQL["vent_proc_sql"] = f"""\nSELECT\n  hadm_id,\n  icd_version,\n  REPLACE(icd_code, \'.\', \'\') AS code_norm\nFROM `{PHYS}.{HOSP}.procedures_icd`\nWHERE\n  (icd_version = 10 AND icd_code IN (\'5A1935Z\',\'5A1945Z\',\'5A1955Z\',\'0BH17EZ\',\'0BH18EZ\',\'5A09357\',\'5A09457\',\'5A09557\'))\n  OR\n  (icd_version = 9 AND REPLACE(icd_code, \'.\', \'\') IN (\'9670\',\'9671\',\'9672\',\'9604\',\'9390\',\'9391\',\'9399\'))\n"""\n\nvent_proc = run_sql_bq(sql("vent_proc_sql"))\nvent_proc["hadm_id"] = pd.to_numeric(vent_proc["hadm_id"], errors="coerce").astype("Int64")\nvent_proc["icd_version"] = pd.to_numeric(vent_proc["icd_version"], errors="coerce").astype("Int64")\nvent_proc["code_norm"] = vent_proc["code_norm"].astype(str)\n\nhadms_for_vent = set(pd.Series(hadm_list).dropna().astype("int64").tolist())\nvent_proc = vent_proc[vent_proc["hadm_id"].isin(hadms_for_vent)].copy()\n\nimv_codes_10 = {"5A1935Z", "5A1945Z", "5A1955Z", "0BH17EZ", "0BH18EZ"}\nimv_codes_9 = {"9670", "9671", "9672", "9604"}\nniv_codes_10 = {"5A09357", "5A09457", "5A09557"}\nniv_codes_9 = {"9390", "9391", "9399"}\n\nvent_proc["imv_hit"] = (\n    ((vent_proc["icd_version"] == 10) & vent_proc["code_norm"].isin(imv_codes_10)) |\n    ((vent_proc["icd_version"] == 9) & vent_proc["code_norm"].isin(imv_codes_9))\n)\nvent_proc["niv_hit"] = (\n    ((vent_proc["icd_version"] == 10) & vent_proc["code_norm"].isin(niv_codes_10)) |\n    ((vent_proc["icd_version"] == 9) & vent_proc["code_norm"].isin(niv_codes_9))\n)\n\nvent = (\n    vent_proc\n    .groupby("hadm_id", as_index=False)\n    .agg(\n        imv_flag=("imv_hit", "max"),\n        niv_flag=("niv_hit", "max"),\n    )\n)\nvent["imv_flag"] = vent["imv_flag"].astype(int)\nvent["niv_flag"] = vent["niv_flag"].astype(int)\nvent["any_vent_flag"] = ((vent["imv_flag"] == 1) | (vent["niv_flag"] == 1)).astype(int)\n\n# Preserve one row per hadm in hadm_list even if no vent procedure codes were found.\nvent = pd.DataFrame({"hadm_id": pd.Series(hadm_list, dtype="Int64")}).drop_duplicates().merge(vent, on="hadm_id", how="left")\nvent[["imv_flag", "niv_flag", "any_vent_flag"]] = vent[["imv_flag", "niv_flag", "any_vent_flag"]].fillna(0).astype(int)\n\nprint("Vent rows:", len(vent))\n', 'execution_count': 14}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Vent rows: 115179\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 41
[NbConvertApp] Executing cell:
# Purpose: Extract earliest NIV/IMV charttimes using prefiltered ventilation itemids for better performance.

SQL["vent_chart_sql"] = f"""
WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x),

stays AS (
  SELECT DISTINCT hadm_id, stay_id
  FROM `{PHYS}.{ICU}.icustays`
  WHERE hadm_id IN (SELECT hadm_id FROM hadms)
),

vent_itemids AS (
  SELECT itemid, LOWER(label) AS lbl
  FROM `{PHYS}.{ICU}.d_items`
  WHERE REGEXP_CONTAINS(LOWER(label), r'(vent|ventilator|mode|bipap|bi[- ]?pap|cpap|nippv|niv|mask|ett|endotracheal)')
),

cand AS (
  SELECT
    s.hadm_id,
    ce.charttime,
    vi.lbl,
    LOWER(COALESCE(ce.value, '')) AS valstr
  FROM `{PHYS}.{ICU}.chartevents` ce
  JOIN stays s ON s.stay_id = ce.stay_id
  JOIN vent_itemids vi ON vi.itemid = ce.itemid
),

flags AS (
  SELECT
    hadm_id,
    MIN(IF(
          REGEXP_CONTAINS(lbl, r'(non[- ]?invasive|niv|nippv|bipap|bi[- ]?pap|cpap)')
          OR REGEXP_CONTAINS(valstr, r'(non[- ]?invasive|niv|nippv|bipap|bi[- ]?pap|cpap)'),
          charttime, NULL)) AS first_niv_time,
    MIN(IF(
          REGEXP_CONTAINS(lbl, r'(invasive ventilation|endotracheal|ett|mech(|anical)? vent|ventilator mode|ac/|simv|prvc|aprv|pcv|vcv|assist\s*control)')
          OR REGEXP_CONTAINS(valstr, r'(invasive ventilation|endotracheal|ett|ac/|simv|prvc|aprv|pcv|vcv|assist\s*control)'),
          charttime, NULL)) AS first_imv_time,
    MAX(CASE
          WHEN REGEXP_CONTAINS(lbl, r'(non[- ]?invasive|niv|nippv|bipap|bi[- ]?pap|cpap)')
            OR REGEXP_CONTAINS(valstr, r'(non[- ]?invasive|niv|nippv|bipap|bi[- ]?pap|cpap)')
          THEN 1 ELSE 0 END) AS niv_chart_flag,
    MAX(CASE
          WHEN REGEXP_CONTAINS(lbl, r'(invasive ventilation|endotracheal|ett|mech(|anical)? vent|ventilator mode|ac/|simv|prvc|aprv|pcv|vcv|assist\s*control)')
            OR REGEXP_CONTAINS(valstr, r'(invasive ventilation|endotracheal|ett|ac/|simv|prvc|aprv|pcv|vcv|assist\s*control)')
          THEN 1 ELSE 0 END) AS imv_chart_flag
  FROM cand
  GROUP BY hadm_id
)

SELECT hadm_id, niv_chart_flag, imv_chart_flag, first_niv_time, first_imv_time
FROM flags
"""

try:
    vent_chart = run_sql_bq(sql("vent_chart_sql"), {"hadms": hadm_list})
except Exception as e:
    print("WARNING: vent chart extraction failed; falling back to ICD-only vent timing.", e)
    vent_chart = pd.DataFrame(columns=["hadm_id", "niv_chart_flag", "imv_chart_flag", "first_niv_time", "first_imv_time"])

[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Purpose: Extract earliest NIV/IMV charttimes using prefiltered ventilation itemids for better performance.\n\nSQL["vent_chart_sql"] = f"""\nWITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x),\n\nstays AS (\n  SELECT DISTINCT hadm_id, stay_id\n  FROM `{PHYS}.{ICU}.icustays`\n  WHERE hadm_id IN (SELECT hadm_id FROM hadms)\n),\n\nvent_itemids AS (\n  SELECT itemid, LOWER(label) AS lbl\n  FROM `{PHYS}.{ICU}.d_items`\n  WHERE REGEXP_CONTAINS(LOWER(label), r\'(vent|ventilator|mode|bipap|bi[- ]?pap|cpap|nippv|niv|mask|ett|endotracheal)\')\n),\n\ncand AS (\n  SELECT\n    s.hadm_id,\n    ce.charttime,\n    vi.lbl,\n    LOWER(COALESCE(ce.value, \'\')) AS valstr\n  FROM `{PHYS}.{ICU}.chartevents` ce\n  JOIN stays s ON s.stay_id = ce.stay_id\n  JOIN vent_itemids vi ON vi.itemid = ce.itemid\n),\n\nflags AS (\n  SELECT\n    hadm_id,\n    MIN(IF(\n          REGEXP_CONTAINS(lbl, r\'(non[- ]?invasive|niv|nippv|bipap|bi[- ]?pap|cpap)\')\n          OR REGEXP_CONTAINS(valstr, r\'(non[- ]?invasive|niv|nippv|bipap|bi[- ]?pap|cpap)\'),\n          charttime, NULL)) AS first_niv_time,\n    MIN(IF(\n          REGEXP_CONTAINS(lbl, r\'(invasive ventilation|endotracheal|ett|mech(|anical)? vent|ventilator mode|ac/|simv|prvc|aprv|pcv|vcv|assist\\s*control)\')\n          OR REGEXP_CONTAINS(valstr, r\'(invasive ventilation|endotracheal|ett|ac/|simv|prvc|aprv|pcv|vcv|assist\\s*control)\'),\n          charttime, NULL)) AS first_imv_time,\n    MAX(CASE\n          WHEN REGEXP_CONTAINS(lbl, r\'(non[- ]?invasive|niv|nippv|bipap|bi[- ]?pap|cpap)\')\n            OR REGEXP_CONTAINS(valstr, r\'(non[- ]?invasive|niv|nippv|bipap|bi[- ]?pap|cpap)\')\n          THEN 1 ELSE 0 END) AS niv_chart_flag,\n    MAX(CASE\n          WHEN REGEXP_CONTAINS(lbl, r\'(invasive ventilation|endotracheal|ett|mech(|anical)? vent|ventilator mode|ac/|simv|prvc|aprv|pcv|vcv|assist\\s*control)\')\n            OR REGEXP_CONTAINS(valstr, r\'(invasive ventilation|endotracheal|ett|ac/|simv|prvc|aprv|pcv|vcv|assist\\s*control)\')\n          THEN 1 ELSE 0 END) AS imv_chart_flag\n  FROM cand\n  GROUP BY hadm_id\n)\n\nSELECT hadm_id, niv_chart_flag, imv_chart_flag, first_niv_time, first_imv_time\nFROM flags\n"""\n\ntry:\n    vent_chart = run_sql_bq(sql("vent_chart_sql"), {"hadms": hadm_list})\nexcept Exception as e:\n    print("WARNING: vent chart extraction failed; falling back to ICD-only vent timing.", e)\n    vent_chart = pd.DataFrame(columns=["hadm_id", "niv_chart_flag", "imv_chart_flag", "first_niv_time", "first_imv_time"])\n', 'execution_count': 15}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
# Purpose: Derive respiratory support flags and timing fields for IMV/NIV exposure.

# If your existing ICD-only result is called `vent`, rename for clarity:
vent_proc = vent.copy()

# Outer merge so we keep hadm_ids that appear in only one source
vent_combined = vent_proc.merge(vent_chart, on="hadm_id", how="outer")

# Fill missing with 0 before taking maxima
for c in ["imv_flag","niv_flag","any_vent_flag","imv_chart_flag","niv_chart_flag"]:
    if c in vent_combined.columns:
        vent_combined[c] = vent_combined[c].fillna(0).astype("Int64")

# Final "any-source" flags
vent_combined["imv_flag"]       = vent_combined[["imv_flag","imv_chart_flag"]].max(axis=1).astype("Int64")
vent_combined["niv_flag"]       = vent_combined[["niv_flag","niv_chart_flag"]].max(axis=1).astype("Int64")
vent_combined["any_vent_flag"]  = vent_combined[["imv_flag","niv_flag"]].max(axis=1).astype("Int64")

vent_combined = vent_combined[["hadm_id","imv_flag","niv_flag","any_vent_flag","first_imv_time","first_niv_time"]]
print("After combining ICD + chart signals:",
      "\nIMV=1:", int((vent_combined["imv_flag"]==1).sum()),
      "\nNIV=1:", int((vent_combined["niv_flag"]==1).sum()))


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Purpose: Derive respiratory support flags and timing fields for IMV/NIV exposure.\n\n# If your existing ICD-only result is called `vent`, rename for clarity:\nvent_proc = vent.copy()\n\n# Outer merge so we keep hadm_ids that appear in only one source\nvent_combined = vent_proc.merge(vent_chart, on="hadm_id", how="outer")\n\n# Fill missing with 0 before taking maxima\nfor c in ["imv_flag","niv_flag","any_vent_flag","imv_chart_flag","niv_chart_flag"]:\n    if c in vent_combined.columns:\n        vent_combined[c] = vent_combined[c].fillna(0).astype("Int64")\n\n# Final "any-source" flags\nvent_combined["imv_flag"]       = vent_combined[["imv_flag","imv_chart_flag"]].max(axis=1).astype("Int64")\nvent_combined["niv_flag"]       = vent_combined[["niv_flag","niv_chart_flag"]].max(axis=1).astype("Int64")\nvent_combined["any_vent_flag"]  = vent_combined[["imv_flag","niv_flag"]].max(axis=1).astype("Int64")\n\nvent_combined = vent_combined[["hadm_id","imv_flag","niv_flag","any_vent_flag","first_imv_time","first_niv_time"]]\nprint("After combining ICD + chart signals:",\n      "\\nIMV=1:", int((vent_combined["imv_flag"]==1).sum()),\n      "\\nNIV=1:", int((vent_combined["niv_flag"]==1).sum()))\n\n', 'execution_count': 16}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'After combining ICD + chart signals: \nIMV=1: 41084 \nNIV=1: 26347\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 44
[NbConvertApp] Skipping non-executing cell 45
[NbConvertApp] Executing cell:
# Purpose: Extract and standardize first ABG/VBG physiology fields for baseline characterization.

# Canonical base (carries authoritative subject_id)
df = demo.copy()

# Cohort flags / thresholds / labs / etc.
df = safe_merge_on_hadm(df, cohort_any, right_name="cohort_any", check_subject="warn")
df = safe_merge_on_hadm(df, bg_pairs,   right_name="bg_pairs")
df = safe_merge_on_hadm(df, race_eth,   right_name="race_eth")
df = safe_merge_on_hadm(df, ed_triage,  right_name="ed_triage")
df = safe_merge_on_hadm(df, ed_first,   right_name="ed_first")
df = safe_merge_on_hadm(df, icu_meta,   right_name="icu_meta")
df = safe_merge_on_hadm(df, vent_combined,       right_name="vent_combined")

# Anchor to first ED presentation (per admission)
if "ed_intime_first" in globals():
    df = safe_merge_on_hadm(df, ed_intime_first, right_name="ed_intime_first")

# Derived timing: first NIV/IMV relative to ED presentation
if "ed_intime_first" in df.columns and "first_imv_time" in df.columns:
    df["dt_first_imv_hours"] = (df["first_imv_time"] - df["ed_intime_first"]).dt.total_seconds() / 3600.0
if "ed_intime_first" in df.columns and "first_niv_time" in df.columns:
    df["dt_first_niv_hours"] = (df["first_niv_time"] - df["ed_intime_first"]).dt.total_seconds() / 3600.0

# ABG/VBG before IMV (hadm-level)
if {"first_abg_time", "first_imv_time"}.issubset(df.columns):
    df["abg_before_imv"] = (
        df["first_abg_time"].notna() & df["first_imv_time"].notna() &
        (df["first_abg_time"] < df["first_imv_time"])
    ).astype("Int64")
if {"first_vbg_time", "first_imv_time"}.issubset(df.columns):
    df["vbg_before_imv"] = (
        df["first_vbg_time"].notna() & df["first_imv_time"].notna() &
        (df["first_vbg_time"] < df["first_imv_time"])
    ).astype("Int64")

print("Final df rows:", len(df), "cols:", len(df.columns))

# Safety checks
assert "subject_id" in df.columns, "subject_id missing from final df"
assert not any(c.endswith("_x") or c.endswith("_y") for c in df.columns), "Found suffixed columns"
print("Final df rows:", len(df), "cols:", len(df.columns))
df.head(3)


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Purpose: Extract and standardize first ABG/VBG physiology fields for baseline characterization.\n\n# Canonical base (carries authoritative subject_id)\ndf = demo.copy()\n\n# Cohort flags / thresholds / labs / etc.\ndf = safe_merge_on_hadm(df, cohort_any, right_name="cohort_any", check_subject="warn")\ndf = safe_merge_on_hadm(df, bg_pairs,   right_name="bg_pairs")\ndf = safe_merge_on_hadm(df, race_eth,   right_name="race_eth")\ndf = safe_merge_on_hadm(df, ed_triage,  right_name="ed_triage")\ndf = safe_merge_on_hadm(df, ed_first,   right_name="ed_first")\ndf = safe_merge_on_hadm(df, icu_meta,   right_name="icu_meta")\ndf = safe_merge_on_hadm(df, vent_combined,       right_name="vent_combined")\n\n# Anchor to first ED presentation (per admission)\nif "ed_intime_first" in globals():\n    df = safe_merge_on_hadm(df, ed_intime_first, right_name="ed_intime_first")\n\n# Derived timing: first NIV/IMV relative to ED presentation\nif "ed_intime_first" in df.columns and "first_imv_time" in df.columns:\n    df["dt_first_imv_hours"] = (df["first_imv_time"] - df["ed_intime_first"]).dt.total_seconds() / 3600.0\nif "ed_intime_first" in df.columns and "first_niv_time" in df.columns:\n    df["dt_first_niv_hours"] = (df["first_niv_time"] - df["ed_intime_first"]).dt.total_seconds() / 3600.0\n\n# ABG/VBG before IMV (hadm-level)\nif {"first_abg_time", "first_imv_time"}.issubset(df.columns):\n    df["abg_before_imv"] = (\n        df["first_abg_time"].notna() & df["first_imv_time"].notna() &\n        (df["first_abg_time"] < df["first_imv_time"])\n    ).astype("Int64")\nif {"first_vbg_time", "first_imv_time"}.issubset(df.columns):\n    df["vbg_before_imv"] = (\n        df["first_vbg_time"].notna() & df["first_imv_time"].notna() &\n        (df["first_vbg_time"] < df["first_imv_time"])\n    ).astype("Int64")\n\nprint("Final df rows:", len(df), "cols:", len(df.columns))\n\n# Safety checks\nassert "subject_id" in df.columns, "subject_id missing from final df"\nassert not any(c.endswith("_x") or c.endswith("_y") for c in df.columns), "Found suffixed columns"\nprint("Final df rows:", len(df), "cols:", len(df.columns))\ndf.head(3)\n\n', 'execution_count': 17}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Final df rows: 115179 cols: 104\nFinal df rows: 115179 cols: 104\n'}
[NbConvertApp] msg_type: execute_result
[NbConvertApp] content: {'data': {'text/plain': '    hadm_id  subject_id           admittime           dischtime  \\\n0  26713233    10106244 2147-05-09 10:34:00 2147-05-12 13:43:00   \n1  27961368    15443666 2168-12-30 23:30:00 2169-01-05 16:02:00   \n2  23485217    10584718 2165-02-12 15:41:00 2165-03-06 08:20:00   \n\n            deathtime     admission_type  \\\n0                 NaT       DIRECT EMER.   \n1                 NaT  OBSERVATION ADMIT   \n2 2165-03-06 08:20:00           EW EMER.   \n\n                       admission_location discharge_location insurance  \\\n0                      PHYSICIAN REFERRAL               HOME   Private   \n1                          EMERGENCY ROOM   HOME HEALTH CARE  Medicare   \n2  TRANSFER FROM SKILLED NURSING FACILITY               DIED  Medicare   \n\n   hosp_los_days  ...          icu_intime         icu_outtime  icu_los_days  \\\n0       3.125000  ... 2147-05-09 10:35:46 2147-05-11 23:45:18      2.548287   \n1       5.708333  ... 2168-12-31 00:19:16 2168-12-31 17:18:37      0.707882   \n2      21.708333  ... 2165-02-27 21:41:10 2165-03-06 11:09:58      6.561667   \n\n   imv_flag  niv_flag  any_vent_flag      first_imv_time  first_niv_time  \\\n0         1         0              1 2147-05-09 20:58:00             NaT   \n1         0         0              0                 NaT             NaT   \n2         1         0              1 2165-02-28 00:00:00             NaT   \n\n   abg_before_imv  vbg_before_imv  \n0               1               0  \n1               0               0  \n2               0               1  \n\n[3 rows x 104 columns]', 'text/html': '<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>hadm_id</th>\n      <th>subject_id</th>\n      <th>admittime</th>\n      <th>dischtime</th>\n      <th>deathtime</th>\n      <th>admission_type</th>\n      <th>admission_location</th>\n      <th>discharge_location</th>\n      <th>insurance</th>\n      <th>hosp_los_days</th>\n      <th>...</th>\n      <th>icu_intime</th>\n      <th>icu_outtime</th>\n      <th>icu_los_days</th>\n      <th>imv_flag</th>\n      <th>niv_flag</th>\n      <th>any_vent_flag</th>\n      <th>first_imv_time</th>\n      <th>first_niv_time</th>\n      <th>abg_before_imv</th>\n      <th>vbg_before_imv</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>26713233</td>\n      <td>10106244</td>\n      <td>2147-05-09 10:34:00</td>\n      <td>2147-05-12 13:43:00</td>\n      <td>NaT</td>\n      <td>DIRECT EMER.</td>\n      <td>PHYSICIAN REFERRAL</td>\n      <td>HOME</td>\n      <td>Private</td>\n      <td>3.125000</td>\n      <td>...</td>\n      <td>2147-05-09 10:35:46</td>\n      <td>2147-05-11 23:45:18</td>\n      <td>2.548287</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2147-05-09 20:58:00</td>\n      <td>NaT</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>27961368</td>\n      <td>15443666</td>\n      <td>2168-12-30 23:30:00</td>\n      <td>2169-01-05 16:02:00</td>\n      <td>NaT</td>\n      <td>OBSERVATION ADMIT</td>\n      <td>EMERGENCY ROOM</td>\n      <td>HOME HEALTH CARE</td>\n      <td>Medicare</td>\n      <td>5.708333</td>\n      <td>...</td>\n      <td>2168-12-31 00:19:16</td>\n      <td>2168-12-31 17:18:37</td>\n      <td>0.707882</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaT</td>\n      <td>NaT</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>23485217</td>\n      <td>10584718</td>\n      <td>2165-02-12 15:41:00</td>\n      <td>2165-03-06 08:20:00</td>\n      <td>2165-03-06 08:20:00</td>\n      <td>EW EMER.</td>\n      <td>TRANSFER FROM SKILLED NURSING FACILITY</td>\n      <td>DIED</td>\n      <td>Medicare</td>\n      <td>21.708333</td>\n      <td>...</td>\n      <td>2165-02-27 21:41:10</td>\n      <td>2165-03-06 11:09:58</td>\n      <td>6.561667</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2165-02-28 00:00:00</td>\n      <td>NaT</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 104 columns</p>\n</div>'}, 'metadata': {}, 'execution_count': 17}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.

# --- Cohort flow counts (ED / ICU / blood gas / hypercapnia / CC) ---

# 1) Dataset-level ED counts
SQL["ed_counts_sql"] = f"""
SELECT
  COUNT(*) AS total_ed_encounters,
  COUNTIF(hadm_id IS NOT NULL) AS ed_encounters_with_hadm
FROM `{PHYS}.{ED}.edstays`
"""

SQL["ed_to_icu_sql"] = f"""
SELECT
  COUNT(DISTINCT e.hadm_id) AS ed_to_icu_hadm,
  COUNT(DISTINCT e.stay_id) AS ed_to_icu_edstays
FROM `{PHYS}.{ED}.edstays` e
JOIN `{PHYS}.{ICU}.icustays` i USING (hadm_id)
WHERE e.hadm_id IS NOT NULL
"""

try:
    ed_counts = run_sql_bq(sql("ed_counts_sql"))
    ed_to_icu = run_sql_bq(sql("ed_to_icu_sql"))
except Exception as e:
    print("Warning: ED/ICU counts query failed:", e)
    ed_counts = None
    ed_to_icu = None

# 2) Cohort-level counts (admission-level)
cohort_union = int((cohort_any["enrolled_any"] == 1).sum()) if "cohort_any" in globals() and "enrolled_any" in cohort_any.columns else len(hadm_list)
cohort_df_n = len(df)

# Any blood gas present (ABG/VBG, LAB/POC)
co2_cols = [c for c in [
    "lab_abg_paco2", "lab_vbg_paco2", "poc_abg_paco2", "poc_vbg_paco2"
] if c in df.columns]
any_bg = int(df[co2_cols].notna().any(axis=1).sum()) if co2_cols else None

# Hypercapnia thresholds and ICD
icd_count = int((df["any_hypercap_icd"] == 1).sum()) if "any_hypercap_icd" in df.columns else None
threshold_count = int((df["pco2_threshold_any"] == 1).sum()) if "pco2_threshold_any" in df.columns else None

# ED chief complaint missing / present (within cohort)
if "ed_triage_cc" in df.columns:
    mask_cc_present = df["ed_triage_cc"].notna() & (df["ed_triage_cc"].astype(str).str.strip() != "")
    cc_present = int(mask_cc_present.sum())
    cc_missing = int((~mask_cc_present).sum())
else:
    cc_present = None
    cc_missing = None

# ED→ICU within cohort (admissions with ED triage data and ICU stay)
if "first_icu_stay_id" in df.columns and "ed_triage_cc" in df.columns:
    cohort_ed_to_icu = int((df["first_icu_stay_id"].notna() & mask_cc_present).sum())
else:
    cohort_ed_to_icu = None

rows = []
if ed_counts is not None:
    rows.append({"step": "Total ED encounters (edstays)", "count": int(ed_counts.loc[0, "total_ed_encounters"]), "scope": "All ED dataset"})
    rows.append({"step": "ED encounters with hadm_id", "count": int(ed_counts.loc[0, "ed_encounters_with_hadm"]), "scope": "All ED dataset"})
if ed_to_icu is not None:
    rows.append({"step": "ED→ICU admissions (distinct hadm_id)", "count": int(ed_to_icu.loc[0, "ed_to_icu_hadm"]), "scope": "All ED+ICU"})
    rows.append({"step": "ED→ICU ED-stays (distinct stay_id)", "count": int(ed_to_icu.loc[0, "ed_to_icu_edstays"]), "scope": "All ED+ICU"})

rows.append({"step": "Cohort admissions (union ICD ∪ thresholds)", "count": cohort_union, "scope": "Cohort"})
rows.append({"step": "Cohort admissions after merges (df rows)", "count": cohort_df_n, "scope": "Cohort"})
if any_bg is not None:
    rows.append({"step": "Cohort with any ABG/VBG (LAB or POC)", "count": any_bg, "scope": "Cohort"})
if threshold_count is not None:
    rows.append({"step": "Cohort meeting hypercapnia thresholds", "count": threshold_count, "scope": "Cohort"})
if icd_count is not None:
    rows.append({"step": "Cohort meeting ICD code criteria", "count": icd_count, "scope": "Cohort"})
if cc_present is not None:
    rows.append({"step": "Cohort with ED chief complaint present", "count": cc_present, "scope": "Cohort"})
if cc_missing is not None:
    rows.append({"step": "Cohort excluded for missing ED chief complaint", "count": cc_missing, "scope": "Cohort"})
if cohort_ed_to_icu is not None:
    rows.append({"step": "Cohort ED→ICU (ED CC present + ICU stay)", "count": cohort_ed_to_icu, "scope": "Cohort"})

flow_counts = pd.DataFrame(rows)
flow_counts


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.\n\n# --- Cohort flow counts (ED / ICU / blood gas / hypercapnia / CC) ---\n\n# 1) Dataset-level ED counts\nSQL["ed_counts_sql"] = f"""\nSELECT\n  COUNT(*) AS total_ed_encounters,\n  COUNTIF(hadm_id IS NOT NULL) AS ed_encounters_with_hadm\nFROM `{PHYS}.{ED}.edstays`\n"""\n\nSQL["ed_to_icu_sql"] = f"""\nSELECT\n  COUNT(DISTINCT e.hadm_id) AS ed_to_icu_hadm,\n  COUNT(DISTINCT e.stay_id) AS ed_to_icu_edstays\nFROM `{PHYS}.{ED}.edstays` e\nJOIN `{PHYS}.{ICU}.icustays` i USING (hadm_id)\nWHERE e.hadm_id IS NOT NULL\n"""\n\ntry:\n    ed_counts = run_sql_bq(sql("ed_counts_sql"))\n    ed_to_icu = run_sql_bq(sql("ed_to_icu_sql"))\nexcept Exception as e:\n    print("Warning: ED/ICU counts query failed:", e)\n    ed_counts = None\n    ed_to_icu = None\n\n# 2) Cohort-level counts (admission-level)\ncohort_union = int((cohort_any["enrolled_any"] == 1).sum()) if "cohort_any" in globals() and "enrolled_any" in cohort_any.columns else len(hadm_list)\ncohort_df_n = len(df)\n\n# Any blood gas present (ABG/VBG, LAB/POC)\nco2_cols = [c for c in [\n    "lab_abg_paco2", "lab_vbg_paco2", "poc_abg_paco2", "poc_vbg_paco2"\n] if c in df.columns]\nany_bg = int(df[co2_cols].notna().any(axis=1).sum()) if co2_cols else None\n\n# Hypercapnia thresholds and ICD\nicd_count = int((df["any_hypercap_icd"] == 1).sum()) if "any_hypercap_icd" in df.columns else None\nthreshold_count = int((df["pco2_threshold_any"] == 1).sum()) if "pco2_threshold_any" in df.columns else None\n\n# ED chief complaint missing / present (within cohort)\nif "ed_triage_cc" in df.columns:\n    mask_cc_present = df["ed_triage_cc"].notna() & (df["ed_triage_cc"].astype(str).str.strip() != "")\n    cc_present = int(mask_cc_present.sum())\n    cc_missing = int((~mask_cc_present).sum())\nelse:\n    cc_present = None\n    cc_missing = None\n\n# ED→ICU within cohort (admissions with ED triage data and ICU stay)\nif "first_icu_stay_id" in df.columns and "ed_triage_cc" in df.columns:\n    cohort_ed_to_icu = int((df["first_icu_stay_id"].notna() & mask_cc_present).sum())\nelse:\n    cohort_ed_to_icu = None\n\nrows = []\nif ed_counts is not None:\n    rows.append({"step": "Total ED encounters (edstays)", "count": int(ed_counts.loc[0, "total_ed_encounters"]), "scope": "All ED dataset"})\n    rows.append({"step": "ED encounters with hadm_id", "count": int(ed_counts.loc[0, "ed_encounters_with_hadm"]), "scope": "All ED dataset"})\nif ed_to_icu is not None:\n    rows.append({"step": "ED→ICU admissions (distinct hadm_id)", "count": int(ed_to_icu.loc[0, "ed_to_icu_hadm"]), "scope": "All ED+ICU"})\n    rows.append({"step": "ED→ICU ED-stays (distinct stay_id)", "count": int(ed_to_icu.loc[0, "ed_to_icu_edstays"]), "scope": "All ED+ICU"})\n\nrows.append({"step": "Cohort admissions (union ICD ∪ thresholds)", "count": cohort_union, "scope": "Cohort"})\nrows.append({"step": "Cohort admissions after merges (df rows)", "count": cohort_df_n, "scope": "Cohort"})\nif any_bg is not None:\n    rows.append({"step": "Cohort with any ABG/VBG (LAB or POC)", "count": any_bg, "scope": "Cohort"})\nif threshold_count is not None:\n    rows.append({"step": "Cohort meeting hypercapnia thresholds", "count": threshold_count, "scope": "Cohort"})\nif icd_count is not None:\n    rows.append({"step": "Cohort meeting ICD code criteria", "count": icd_count, "scope": "Cohort"})\nif cc_present is not None:\n    rows.append({"step": "Cohort with ED chief complaint present", "count": cc_present, "scope": "Cohort"})\nif cc_missing is not None:\n    rows.append({"step": "Cohort excluded for missing ED chief complaint", "count": cc_missing, "scope": "Cohort"})\nif cohort_ed_to_icu is not None:\n    rows.append({"step": "Cohort ED→ICU (ED CC present + ICU stay)", "count": cohort_ed_to_icu, "scope": "Cohort"})\n\nflow_counts = pd.DataFrame(rows)\nflow_counts\n\n', 'execution_count': 18}
[NbConvertApp] msg_type: execute_result
[NbConvertApp] content: {'data': {'text/plain': '                                              step   count           scope\n0                    Total ED encounters (edstays)  425087  All ED dataset\n1                       ED encounters with hadm_id  203016  All ED dataset\n2             ED→ICU admissions (distinct hadm_id)   31862      All ED+ICU\n3               ED→ICU ED-stays (distinct stay_id)   31916      All ED+ICU\n4       Cohort admissions (union ICD ∪ thresholds)  115180          Cohort\n5         Cohort admissions after merges (df rows)  115179          Cohort\n6             Cohort with any ABG/VBG (LAB or POC)   85058          Cohort\n7            Cohort meeting hypercapnia thresholds  114632          Cohort\n8                 Cohort meeting ICD code criteria    4237          Cohort\n9           Cohort with ED chief complaint present   41322          Cohort\n10  Cohort excluded for missing ED chief complaint   73857          Cohort\n11        Cohort ED→ICU (ED CC present + ICU stay)   31854          Cohort', 'text/html': '<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>step</th>\n      <th>count</th>\n      <th>scope</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Total ED encounters (edstays)</td>\n      <td>425087</td>\n      <td>All ED dataset</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ED encounters with hadm_id</td>\n      <td>203016</td>\n      <td>All ED dataset</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ED→ICU admissions (distinct hadm_id)</td>\n      <td>31862</td>\n      <td>All ED+ICU</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ED→ICU ED-stays (distinct stay_id)</td>\n      <td>31916</td>\n      <td>All ED+ICU</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Cohort admissions (union ICD ∪ thresholds)</td>\n      <td>115180</td>\n      <td>Cohort</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Cohort admissions after merges (df rows)</td>\n      <td>115179</td>\n      <td>Cohort</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Cohort with any ABG/VBG (LAB or POC)</td>\n      <td>85058</td>\n      <td>Cohort</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Cohort meeting hypercapnia thresholds</td>\n      <td>114632</td>\n      <td>Cohort</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Cohort meeting ICD code criteria</td>\n      <td>4237</td>\n      <td>Cohort</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Cohort with ED chief complaint present</td>\n      <td>41322</td>\n      <td>Cohort</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Cohort excluded for missing ED chief complaint</td>\n      <td>73857</td>\n      <td>Cohort</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Cohort ED→ICU (ED CC present + ICU stay)</td>\n      <td>31854</td>\n      <td>Cohort</td>\n    </tr>\n  </tbody>\n</table>\n</div>'}, 'metadata': {}, 'execution_count': 18}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
# Purpose: Quantify overlap between ascertainment routes so non-exclusive cohorts are explicit.

# --- Ascertainment overlap counts (ABG/VBG/ICD) ---

required = ["abg_hypercap_threshold", "vbg_hypercap_threshold", "other_hypercap_threshold", "any_hypercap_icd"]
missing = [c for c in required if c not in df.columns]
if missing:
    raise KeyError(f"Missing required columns for overlap counts: {missing}")

abg = pd.to_numeric(df["abg_hypercap_threshold"], errors="coerce").fillna(0).astype(int)
vbg = pd.to_numeric(df["vbg_hypercap_threshold"], errors="coerce").fillna(0).astype(int)
other = pd.to_numeric(df["other_hypercap_threshold"], errors="coerce").fillna(0).astype(int)

gas_any = (
    pd.to_numeric(df.get("pco2_threshold_any", None), errors="coerce")
    if "pco2_threshold_any" in df.columns else (abg | vbg | other)
)
if hasattr(gas_any, "fillna"):
    gas_any = gas_any.fillna(0).astype(int)
else:
    gas_any = gas_any.astype(int)

icd = pd.to_numeric(df["any_hypercap_icd"], errors="coerce").fillna(0).astype(int)

total_n = len(df)
ngas = int((gas_any == 1).sum())

abg_vbg_overlap = pd.DataFrame([
    {"group": "ABG-only", "count": int(((abg==1) & (vbg==0) & (other==0)).sum())},
    {"group": "VBG-only", "count": int(((vbg==1) & (abg==0) & (other==0)).sum())},
    {"group": "Other-only", "count": int(((other==1) & (abg==0) & (vbg==0)).sum())},
    {"group": "Mixed (>=2 routes)", "count": int((((abg + vbg + other) >= 2).sum()))},
])
if ngas > 0:
    abg_vbg_overlap["pct_of_gas"] = (abg_vbg_overlap["count"] / ngas * 100).round(1)
else:
    abg_vbg_overlap["pct_of_gas"] = 0.0
abg_vbg_overlap["pct_of_cohort"] = (abg_vbg_overlap["count"] / max(total_n,1) * 100).round(1)

icd_gas_overlap = pd.DataFrame([
    {"group": "ICD+Gas", "count": int(((icd==1) & (gas_any==1)).sum())},
    {"group": "ICD-only", "count": int(((icd==1) & (gas_any==0)).sum())},
    {"group": "Gas-only", "count": int(((icd==0) & (gas_any==1)).sum())},
    {"group": "Neither", "count": int(((icd==0) & (gas_any==0)).sum())},
])
icd_gas_overlap["pct_of_cohort"] = (icd_gas_overlap["count"] / max(total_n,1) * 100).round(1)

print("ABG/VBG/Other overlap (among gas-positive):")
print(abg_vbg_overlap.to_string(index=False))
print("ICD vs Gas overlap (cohort-level):")
print(icd_gas_overlap.to_string(index=False))


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Purpose: Quantify overlap between ascertainment routes so non-exclusive cohorts are explicit.\n\n# --- Ascertainment overlap counts (ABG/VBG/ICD) ---\n\nrequired = ["abg_hypercap_threshold", "vbg_hypercap_threshold", "other_hypercap_threshold", "any_hypercap_icd"]\nmissing = [c for c in required if c not in df.columns]\nif missing:\n    raise KeyError(f"Missing required columns for overlap counts: {missing}")\n\nabg = pd.to_numeric(df["abg_hypercap_threshold"], errors="coerce").fillna(0).astype(int)\nvbg = pd.to_numeric(df["vbg_hypercap_threshold"], errors="coerce").fillna(0).astype(int)\nother = pd.to_numeric(df["other_hypercap_threshold"], errors="coerce").fillna(0).astype(int)\n\ngas_any = (\n    pd.to_numeric(df.get("pco2_threshold_any", None), errors="coerce")\n    if "pco2_threshold_any" in df.columns else (abg | vbg | other)\n)\nif hasattr(gas_any, "fillna"):\n    gas_any = gas_any.fillna(0).astype(int)\nelse:\n    gas_any = gas_any.astype(int)\n\nicd = pd.to_numeric(df["any_hypercap_icd"], errors="coerce").fillna(0).astype(int)\n\ntotal_n = len(df)\nngas = int((gas_any == 1).sum())\n\nabg_vbg_overlap = pd.DataFrame([\n    {"group": "ABG-only", "count": int(((abg==1) & (vbg==0) & (other==0)).sum())},\n    {"group": "VBG-only", "count": int(((vbg==1) & (abg==0) & (other==0)).sum())},\n    {"group": "Other-only", "count": int(((other==1) & (abg==0) & (vbg==0)).sum())},\n    {"group": "Mixed (>=2 routes)", "count": int((((abg + vbg + other) >= 2).sum()))},\n])\nif ngas > 0:\n    abg_vbg_overlap["pct_of_gas"] = (abg_vbg_overlap["count"] / ngas * 100).round(1)\nelse:\n    abg_vbg_overlap["pct_of_gas"] = 0.0\nabg_vbg_overlap["pct_of_cohort"] = (abg_vbg_overlap["count"] / max(total_n,1) * 100).round(1)\n\nicd_gas_overlap = pd.DataFrame([\n    {"group": "ICD+Gas", "count": int(((icd==1) & (gas_any==1)).sum())},\n    {"group": "ICD-only", "count": int(((icd==1) & (gas_any==0)).sum())},\n    {"group": "Gas-only", "count": int(((icd==0) & (gas_any==1)).sum())},\n    {"group": "Neither", "count": int(((icd==0) & (gas_any==0)).sum())},\n])\nicd_gas_overlap["pct_of_cohort"] = (icd_gas_overlap["count"] / max(total_n,1) * 100).round(1)\n\nprint("ABG/VBG/Other overlap (among gas-positive):")\nprint(abg_vbg_overlap.to_string(index=False))\nprint("ICD vs Gas overlap (cohort-level):")\nprint(icd_gas_overlap.to_string(index=False))\n\n', 'execution_count': 19}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ABG/VBG/Other overlap (among gas-positive):\n             group  count  pct_of_gas  pct_of_cohort\n          ABG-only      0         0.0            0.0\n          VBG-only  10487         9.1            9.1\n        Other-only  75616        66.0           65.7\nMixed (>=2 routes)  28529        24.9           24.8\nICD vs Gas overlap (cohort-level):\n   group  count  pct_of_cohort\n ICD+Gas   3690            3.2\nICD-only    547            0.5\nGas-only 110942           96.3\n Neither      0            0.0\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
# Purpose: Capture ED presentation context (chief complaint, acuity, and early vitals) at the ED-stay level.

# --- Missingness summary (chief complaint, race/ethnicity, ED triage/vitals) ---

import numpy as np

summary_rows = []

# Chief complaint missingness (ED triage CC)
if "ed_triage_cc" in df.columns:
    cc_present = df["ed_triage_cc"].notna() & (df["ed_triage_cc"].astype(str).str.strip() != "")
    summary_rows.append({
        "variable": "ed_triage_cc_present",
        "missing_n": int((~cc_present).sum()),
        "missing_pct": float((~cc_present).mean())
    })

# Race/Ethnicity missingness (NIH categories + raw sources)
unknown_tokens = {
    "unknown or not reported",
    "unknown",
    "not reported",
    "missing",
    "declined",
    "unable"
}

def _missing_rate(series):
    if series is None:
        return None, None
    s = series.astype(str).str.strip()
    is_missing = series.isna() | (s == "") | s.str.lower().isin(unknown_tokens)
    return int(is_missing.sum()), float(is_missing.mean())

for col in ["nih_race", "nih_ethnicity", "race_hosp_raw", "race_ed_raw"]:
    if col in df.columns:
        m_n, m_p = _missing_rate(df[col])
        summary_rows.append({
            "variable": col,
            "missing_n": m_n,
            "missing_pct": m_p
        })

missing_summary = pd.DataFrame(summary_rows)
print("Missingness summary (key variables):")
missing_summary

# ED triage + first ED vitals missingness
triage_cols = [c for c in df.columns if c.startswith("ed_triage_")]
first_cols  = [c for c in df.columns if c.startswith("ed_first_")]

vital_cols = triage_cols + first_cols
if vital_cols:
    miss_tbl = (
        pd.DataFrame({"variable": vital_cols})
        .assign(
            missing_n=lambda d: [int(df[c].isna().sum()) for c in d["variable"]],
            missing_pct=lambda d: [float(df[c].isna().mean()) for c in d["variable"]]
        )
        .sort_values("missing_pct", ascending=False)
    )
    print("Missingness summary (ED triage + first ED vitals):")
    miss_tbl
else:
    miss_tbl = pd.DataFrame(columns=["variable", "missing_n", "missing_pct"])


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Purpose: Capture ED presentation context (chief complaint, acuity, and early vitals) at the ED-stay level.\n\n# --- Missingness summary (chief complaint, race/ethnicity, ED triage/vitals) ---\n\nimport numpy as np\n\nsummary_rows = []\n\n# Chief complaint missingness (ED triage CC)\nif "ed_triage_cc" in df.columns:\n    cc_present = df["ed_triage_cc"].notna() & (df["ed_triage_cc"].astype(str).str.strip() != "")\n    summary_rows.append({\n        "variable": "ed_triage_cc_present",\n        "missing_n": int((~cc_present).sum()),\n        "missing_pct": float((~cc_present).mean())\n    })\n\n# Race/Ethnicity missingness (NIH categories + raw sources)\nunknown_tokens = {\n    "unknown or not reported",\n    "unknown",\n    "not reported",\n    "missing",\n    "declined",\n    "unable"\n}\n\ndef _missing_rate(series):\n    if series is None:\n        return None, None\n    s = series.astype(str).str.strip()\n    is_missing = series.isna() | (s == "") | s.str.lower().isin(unknown_tokens)\n    return int(is_missing.sum()), float(is_missing.mean())\n\nfor col in ["nih_race", "nih_ethnicity", "race_hosp_raw", "race_ed_raw"]:\n    if col in df.columns:\n        m_n, m_p = _missing_rate(df[col])\n        summary_rows.append({\n            "variable": col,\n            "missing_n": m_n,\n            "missing_pct": m_p\n        })\n\nmissing_summary = pd.DataFrame(summary_rows)\nprint("Missingness summary (key variables):")\nmissing_summary\n\n# ED triage + first ED vitals missingness\ntriage_cols = [c for c in df.columns if c.startswith("ed_triage_")]\nfirst_cols  = [c for c in df.columns if c.startswith("ed_first_")]\n\nvital_cols = triage_cols + first_cols\nif vital_cols:\n    miss_tbl = (\n        pd.DataFrame({"variable": vital_cols})\n        .assign(\n            missing_n=lambda d: [int(df[c].isna().sum()) for c in d["variable"]],\n            missing_pct=lambda d: [float(df[c].isna().mean()) for c in d["variable"]]\n        )\n        .sort_values("missing_pct", ascending=False)\n    )\n    print("Missingness summary (ED triage + first ED vitals):")\n    miss_tbl\nelse:\n    miss_tbl = pd.DataFrame(columns=["variable", "missing_n", "missing_pct"])\n\n', 'execution_count': 20}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Missingness summary (key variables):\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Missingness summary (ED triage + first ED vitals):\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 50
[NbConvertApp] Skipping non-executing cell 51
[NbConvertApp] Executing cell:
# Purpose: Derive respiratory support flags and timing fields for IMV/NIV exposure.


# ICU LOS negative?
if {"icu_los_days","first_icu_stay_id"}.issubset(df.columns):
    neg_los = int((df["icu_los_days"] < 0).fillna(False).sum())
    print("Negative ICU LOS rows:", neg_los)

# Vent flags consistency
vent_cols = {"imv_flag","niv_flag","any_vent_flag"}
if vent_cols.issubset(df.columns):
    any_calc = ((df["imv_flag"]==1) | (df["niv_flag"]==1)).fillna(False).astype(int)
    any_flag = pd.to_numeric(df["any_vent_flag"], errors="coerce").fillna(0).astype(int)
    mism = int((any_calc != any_flag).sum())
    print("any_vent_flag mismatches vs (imv|niv):", mism)

# UOMs: expect mmhg only
uom_cols = [c for c in df.columns if c.endswith("_paco2_uom")]
for c in uom_cols:
    vals = sorted(pd.Series(df[c]).dropna().astype(str).str.lower().str.strip().unique().tolist())
    print(c, vals)

# ABG/VBG coverage QC
def qc_pair(df, ph_col, co2_col, label, ph_lo=6.3, ph_hi=7.8, co2_lo=5, co2_hi=200):
    ph  = pd.to_numeric(df.get(ph_col), errors="coerce")
    co2 = pd.to_numeric(df.get(co2_col), errors="coerce")
    return {
        "pair": label,
        "present_any":  int(((ph.notna()) | (co2.notna())).sum()),
        "present_both": int(((ph.notna()) & (co2.notna())).sum()),
        "only_ph":      int(((ph.notna()) & (~co2.notna())).sum()),
        "only_pco2":    int(((co2.notna()) & (~ph.notna())).sum()),
        "ph_oob":       int((((ph  < ph_lo)  | (ph  > ph_hi))  & ph.notna()).sum()),
        "pco2_oob":     int((((co2 < co2_lo) | (co2 > co2_hi)) & co2.notna()).sum()),
    }

qc = pd.DataFrame([
    qc_pair(df, "lab_abg_ph","lab_abg_paco2","LAB ABG"),
    qc_pair(df, "lab_vbg_ph","lab_vbg_paco2","LAB VBG"),
    qc_pair(df, "poc_abg_ph","poc_abg_paco2","POC ABG"),
    qc_pair(df, "poc_vbg_ph","poc_vbg_paco2","POC VBG"),
])
qc

[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Purpose: Derive respiratory support flags and timing fields for IMV/NIV exposure.\n\n\n# ICU LOS negative?\nif {"icu_los_days","first_icu_stay_id"}.issubset(df.columns):\n    neg_los = int((df["icu_los_days"] < 0).fillna(False).sum())\n    print("Negative ICU LOS rows:", neg_los)\n\n# Vent flags consistency\nvent_cols = {"imv_flag","niv_flag","any_vent_flag"}\nif vent_cols.issubset(df.columns):\n    any_calc = ((df["imv_flag"]==1) | (df["niv_flag"]==1)).fillna(False).astype(int)\n    any_flag = pd.to_numeric(df["any_vent_flag"], errors="coerce").fillna(0).astype(int)\n    mism = int((any_calc != any_flag).sum())\n    print("any_vent_flag mismatches vs (imv|niv):", mism)\n\n# UOMs: expect mmhg only\nuom_cols = [c for c in df.columns if c.endswith("_paco2_uom")]\nfor c in uom_cols:\n    vals = sorted(pd.Series(df[c]).dropna().astype(str).str.lower().str.strip().unique().tolist())\n    print(c, vals)\n\n# ABG/VBG coverage QC\ndef qc_pair(df, ph_col, co2_col, label, ph_lo=6.3, ph_hi=7.8, co2_lo=5, co2_hi=200):\n    ph  = pd.to_numeric(df.get(ph_col), errors="coerce")\n    co2 = pd.to_numeric(df.get(co2_col), errors="coerce")\n    return {\n        "pair": label,\n        "present_any":  int(((ph.notna()) | (co2.notna())).sum()),\n        "present_both": int(((ph.notna()) & (co2.notna())).sum()),\n        "only_ph":      int(((ph.notna()) & (~co2.notna())).sum()),\n        "only_pco2":    int(((co2.notna()) & (~ph.notna())).sum()),\n        "ph_oob":       int((((ph  < ph_lo)  | (ph  > ph_hi))  & ph.notna()).sum()),\n        "pco2_oob":     int((((co2 < co2_lo) | (co2 > co2_hi)) & co2.notna()).sum()),\n    }\n\nqc = pd.DataFrame([\n    qc_pair(df, "lab_abg_ph","lab_abg_paco2","LAB ABG"),\n    qc_pair(df, "lab_vbg_ph","lab_vbg_paco2","LAB VBG"),\n    qc_pair(df, "poc_abg_ph","poc_abg_paco2","POC ABG"),\n    qc_pair(df, "poc_vbg_ph","poc_vbg_paco2","POC VBG"),\n])\nqc\n', 'execution_count': 21}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': "Negative ICU LOS rows: 0\nany_vent_flag mismatches vs (imv|niv): 0\nlab_abg_paco2_uom ['mmhg']\nlab_vbg_paco2_uom ['mmhg']\nlab_other_paco2_uom ['mmhg']\npoc_abg_paco2_uom ['mmhg']\npoc_vbg_paco2_uom ['mmhg']\npoc_other_paco2_uom ['mmhg']\n"}
[NbConvertApp] msg_type: execute_result
[NbConvertApp] content: {'data': {'text/plain': '      pair  present_any  present_both  only_ph  only_pco2  ph_oob  pco2_oob\n0  LAB ABG        56417         55835      570         12       0         0\n1  LAB VBG        46269         37252     9009          8       0         0\n2  POC ABG        46395         20183        0      26212       0         0\n3  POC VBG        39488         19826        0      19662       0         0', 'text/html': '<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>pair</th>\n      <th>present_any</th>\n      <th>present_both</th>\n      <th>only_ph</th>\n      <th>only_pco2</th>\n      <th>ph_oob</th>\n      <th>pco2_oob</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>LAB ABG</td>\n      <td>56417</td>\n      <td>55835</td>\n      <td>570</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>LAB VBG</td>\n      <td>46269</td>\n      <td>37252</td>\n      <td>9009</td>\n      <td>8</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>POC ABG</td>\n      <td>46395</td>\n      <td>20183</td>\n      <td>0</td>\n      <td>26212</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>POC VBG</td>\n      <td>39488</td>\n      <td>19826</td>\n      <td>0</td>\n      <td>19662</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>'}, 'metadata': {}, 'execution_count': 21}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 53
[NbConvertApp] Skipping non-executing cell 54
[NbConvertApp] Executing cell:
# Purpose: Optional archive export of the full hadm-level cohort workbook.

from datetime import datetime

WRITE_ARCHIVE_XLSX_EXPORTS = os.getenv("WRITE_ARCHIVE_XLSX_EXPORTS", "0") == "1"

if WRITE_ARCHIVE_XLSX_EXPORTS:
    prior_runs_dir = DATA_DIR / "prior runs"
    prior_runs_dir.mkdir(parents=True, exist_ok=True)
    out_path = prior_runs_dir / f"mimic_hypercap_EXT_bq_abg_vbg_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
    with pd.ExcelWriter(out_path, engine="openpyxl") as xw:
        df.to_excel(xw, sheet_name="cohort", index=False)
        try:
            qc.to_excel(xw, sheet_name="qc_abg_vbg", index=False)
        except Exception:
            pass
    out_path
else:
    print("Skipping archive hadm-level workbook export (set WRITE_ARCHIVE_XLSX_EXPORTS=1 to enable).")

[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Purpose: Optional archive export of the full hadm-level cohort workbook.\n\nfrom datetime import datetime\n\nWRITE_ARCHIVE_XLSX_EXPORTS = os.getenv("WRITE_ARCHIVE_XLSX_EXPORTS", "0") == "1"\n\nif WRITE_ARCHIVE_XLSX_EXPORTS:\n    prior_runs_dir = DATA_DIR / "prior runs"\n    prior_runs_dir.mkdir(parents=True, exist_ok=True)\n    out_path = prior_runs_dir / f"mimic_hypercap_EXT_bq_abg_vbg_{datetime.now().strftime(\'%Y%m%d_%H%M%S\')}.xlsx"\n    with pd.ExcelWriter(out_path, engine="openpyxl") as xw:\n        df.to_excel(xw, sheet_name="cohort", index=False)\n        try:\n            qc.to_excel(xw, sheet_name="qc_abg_vbg", index=False)\n        except Exception:\n            pass\n    out_path\nelse:\n    print("Skipping archive hadm-level workbook export (set WRITE_ARCHIVE_XLSX_EXPORTS=1 to enable).")\n', 'execution_count': 22}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Skipping archive hadm-level workbook export (set WRITE_ARCHIVE_XLSX_EXPORTS=1 to enable).\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 56
[NbConvertApp] Skipping non-executing cell 57
[NbConvertApp] Executing cell:
# Purpose: Capture ED presentation context (chief complaint, acuity, and early vitals) at the ED-stay level.

# ---- Extra exports: (1) ED chief-complaint only; (2) random sample of 160 patients ----
from datetime import datetime

# Dated artifacts go to archive folder (optional)
prior_runs_dir = DATA_DIR / "prior runs"
prior_runs_dir.mkdir(parents=True, exist_ok=True)
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
WRITE_ARCHIVE_XLSX_EXPORTS = os.getenv("WRITE_ARCHIVE_XLSX_EXPORTS", "0") == "1"

# 1) Filter to rows with a non-empty ED chief complaint
if "ed_triage_cc" not in df.columns:
    raise KeyError(
        "Column 'ed_triage_cc' not found in df. "
        "Ensure the ED triage merge cell ran earlier."
    )

mask_cc = df["ed_triage_cc"].notna() & (df["ed_triage_cc"].astype(str).str.strip() != "")
df_cc = df.loc[mask_cc].copy()

print(f"ED-CC present rows: {len(df_cc)} of {len(df)} "
      f"({(len(df_cc) / max(len(df),1)):.1%} of cohort).")

# Save ED-CC-only cohort only when archive exports are enabled
if WRITE_ARCHIVE_XLSX_EXPORTS:
    out_path_cc = prior_runs_dir / f"mimic_hypercap_EXT_EDcc_only_bq_abg_vbg_{timestamp}.xlsx"
    with pd.ExcelWriter(out_path_cc, engine="openpyxl") as xw:
        df_cc.to_excel(xw, sheet_name="cohort_cc_only", index=False)
        try:
            qc.to_excel(xw, sheet_name="qc_abg_vbg", index=False)
        except Exception:
            pass
    print("Saved:", out_path_cc)

# 2) Random sample of n = 160 patients (distinct subject_id), one row per patient
if "subject_id" not in df_cc.columns:
    raise KeyError("Column 'subject_id' missing; cannot sample by patient.")

# Make a one-row-per-patient frame by earliest admission
if "admittime" in df_cc.columns:
    df_cc_one = (
        df_cc.sort_values(["subject_id", "admittime"])
             .groupby("subject_id", as_index=False)
             .head(1)
    )
else:
    # Fallback if admittime not present: choose the smallest hadm_id per patient
    df_cc_one = (
        df_cc.sort_values(["subject_id", "hadm_id"])
             .groupby("subject_id", as_index=False)
             .head(1)
    )

N = 160
n_avail = len(df_cc_one)
n_take = min(N, n_avail)
if n_avail < N:
    print(f"Warning: only {n_avail} unique patients with ED chief complaint; sampling all of them.")

RANDOM_SEED = 42
df_cc_sample = df_cc_one.sample(n=n_take, random_state=RANDOM_SEED)

# Save the sample only when archive exports are enabled
if WRITE_ARCHIVE_XLSX_EXPORTS:
    out_path_cc_sample = prior_runs_dir / f"mimic_hypercap_EXT_EDcc_sample{n_take}_bq_abg_vbg_{timestamp}.xlsx"
    with pd.ExcelWriter(out_path_cc_sample, engine="openpyxl") as xw:
        df_cc_sample.to_excel(xw, sheet_name="cohort_cc_sample", index=False)
        try:
            qc.to_excel(xw, sheet_name="qc_abg_vbg", index=False)
        except Exception:
            pass
    print("Saved:", out_path_cc_sample)
else:
    print("Skipping ED-CC archive exports (set WRITE_ARCHIVE_XLSX_EXPORTS=1 to enable).")

[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Purpose: Capture ED presentation context (chief complaint, acuity, and early vitals) at the ED-stay level.\n\n# ---- Extra exports: (1) ED chief-complaint only; (2) random sample of 160 patients ----\nfrom datetime import datetime\n\n# Dated artifacts go to archive folder (optional)\nprior_runs_dir = DATA_DIR / "prior runs"\nprior_runs_dir.mkdir(parents=True, exist_ok=True)\ntimestamp = datetime.now().strftime(\'%Y%m%d_%H%M%S\')\nWRITE_ARCHIVE_XLSX_EXPORTS = os.getenv("WRITE_ARCHIVE_XLSX_EXPORTS", "0") == "1"\n\n# 1) Filter to rows with a non-empty ED chief complaint\nif "ed_triage_cc" not in df.columns:\n    raise KeyError(\n        "Column \'ed_triage_cc\' not found in df. "\n        "Ensure the ED triage merge cell ran earlier."\n    )\n\nmask_cc = df["ed_triage_cc"].notna() & (df["ed_triage_cc"].astype(str).str.strip() != "")\ndf_cc = df.loc[mask_cc].copy()\n\nprint(f"ED-CC present rows: {len(df_cc)} of {len(df)} "\n      f"({(len(df_cc) / max(len(df),1)):.1%} of cohort).")\n\n# Save ED-CC-only cohort only when archive exports are enabled\nif WRITE_ARCHIVE_XLSX_EXPORTS:\n    out_path_cc = prior_runs_dir / f"mimic_hypercap_EXT_EDcc_only_bq_abg_vbg_{timestamp}.xlsx"\n    with pd.ExcelWriter(out_path_cc, engine="openpyxl") as xw:\n        df_cc.to_excel(xw, sheet_name="cohort_cc_only", index=False)\n        try:\n            qc.to_excel(xw, sheet_name="qc_abg_vbg", index=False)\n        except Exception:\n            pass\n    print("Saved:", out_path_cc)\n\n# 2) Random sample of n = 160 patients (distinct subject_id), one row per patient\nif "subject_id" not in df_cc.columns:\n    raise KeyError("Column \'subject_id\' missing; cannot sample by patient.")\n\n# Make a one-row-per-patient frame by earliest admission\nif "admittime" in df_cc.columns:\n    df_cc_one = (\n        df_cc.sort_values(["subject_id", "admittime"])\n             .groupby("subject_id", as_index=False)\n             .head(1)\n    )\nelse:\n    # Fallback if admittime not present: choose the smallest hadm_id per patient\n    df_cc_one = (\n        df_cc.sort_values(["subject_id", "hadm_id"])\n             .groupby("subject_id", as_index=False)\n             .head(1)\n    )\n\nN = 160\nn_avail = len(df_cc_one)\nn_take = min(N, n_avail)\nif n_avail < N:\n    print(f"Warning: only {n_avail} unique patients with ED chief complaint; sampling all of them.")\n\nRANDOM_SEED = 42\ndf_cc_sample = df_cc_one.sample(n=n_take, random_state=RANDOM_SEED)\n\n# Save the sample only when archive exports are enabled\nif WRITE_ARCHIVE_XLSX_EXPORTS:\n    out_path_cc_sample = prior_runs_dir / f"mimic_hypercap_EXT_EDcc_sample{n_take}_bq_abg_vbg_{timestamp}.xlsx"\n    with pd.ExcelWriter(out_path_cc_sample, engine="openpyxl") as xw:\n        df_cc_sample.to_excel(xw, sheet_name="cohort_cc_sample", index=False)\n        try:\n            qc.to_excel(xw, sheet_name="qc_abg_vbg", index=False)\n        except Exception:\n            pass\n    print("Saved:", out_path_cc_sample)\nelse:\n    print("Skipping ED-CC archive exports (set WRITE_ARCHIVE_XLSX_EXPORTS=1 to enable).")\n', 'execution_count': 23}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ED-CC present rows: 41322 of 115179 (35.9% of cohort).\nSkipping ED-CC archive exports (set WRITE_ARCHIVE_XLSX_EXPORTS=1 to enable).\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 59
[NbConvertApp] Skipping non-executing cell 60
[NbConvertApp] Executing cell:
# Purpose: Inventory available fields and identify missing target variables before enrichment steps.

from pathlib import Path
import json
import pandas as pd

# Identify current cohort dataframe (admission-level)
if 'df' not in globals():
    raise NameError("Expected admission-level df to exist before inventory step.")

ED_KEY = "ed_stay_id"  # target key for ED-level cohort

print("Current admission-level df:", df.shape)
print("Current columns count:", len(df.columns))
if ED_KEY in df.columns:
    print("ED stay unique count:", int(df[ED_KEY].nunique()))
else:
    print("ED stay unique count: ED_KEY not in columns")

# Persist columns snapshot
cols_out = WORK_DIR / "current_columns.json"
cols_out.write_text(json.dumps(sorted(df.columns), indent=2))
print("Wrote:", cols_out)



# Persist ED-stay columns snapshot (if ed_df exists)
if "ed_df" in globals():
    ed_cols_out = WORK_DIR / "ed_columns.json"
    ed_cols_out.write_text(json.dumps(sorted(ed_df.columns), indent=2))
    print("Wrote:", ed_cols_out)

[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Purpose: Inventory available fields and identify missing target variables before enrichment steps.\n\nfrom pathlib import Path\nimport json\nimport pandas as pd\n\n# Identify current cohort dataframe (admission-level)\nif \'df\' not in globals():\n    raise NameError("Expected admission-level df to exist before inventory step.")\n\nED_KEY = "ed_stay_id"  # target key for ED-level cohort\n\nprint("Current admission-level df:", df.shape)\nprint("Current columns count:", len(df.columns))\nif ED_KEY in df.columns:\n    print("ED stay unique count:", int(df[ED_KEY].nunique()))\nelse:\n    print("ED stay unique count: ED_KEY not in columns")\n\n# Persist columns snapshot\ncols_out = WORK_DIR / "current_columns.json"\ncols_out.write_text(json.dumps(sorted(df.columns), indent=2))\nprint("Wrote:", cols_out)\n\n\n\n# Persist ED-stay columns snapshot (if ed_df exists)\nif "ed_df" in globals():\n    ed_cols_out = WORK_DIR / "ed_columns.json"\n    ed_cols_out.write_text(json.dumps(sorted(ed_df.columns), indent=2))\n    print("Wrote:", ed_cols_out)\n', 'execution_count': 24}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Current admission-level df: (115179, 104)\nCurrent columns count: 104\nED stay unique count: ED_KEY not in columns\nWrote: /Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/current_columns.json\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
# Purpose: Capture ED presentation context (chief complaint, acuity, and early vitals) at the ED-stay level.

# Target field registry
TARGET_RAW_FIELDS = {
    "ed_edstays": [
        "ed_stay_id","subject_id","hadm_id","ed_intime","ed_outtime","ed_intime_first",
        "arrival_transport","disposition","ed_gender","ed_race",
    ],
    "ed_triage": [
        "ed_triage_temp","ed_triage_hr","ed_triage_rr","ed_triage_o2sat",
        "ed_triage_sbp","ed_triage_dbp","ed_triage_pain","ed_triage_acuity","ed_triage_cc",
    ],
    "ed_vitals_first": [
        "ed_first_vitals_time","ed_first_temp","ed_first_hr","ed_first_rr",
        "ed_first_o2sat","ed_first_sbp","ed_first_dbp","ed_first_rhythm","ed_first_pain",
    ],
    "admissions": [
        "admittime","dischtime","deathtime","hospital_expire_flag",
        "admission_type","admission_location","discharge_location",
        "insurance","language","marital_status","hosp_race",
    ],
    "icu": [
        "icu_stay_id","icu_intime_first","icu_outtime_last","icu_los_total","n_icu_stays",
        "first_careunit","last_careunit",
    ],
    "labs_gas": [
        "first_gas_time","first_pco2","first_ph","first_hco3","first_lactate",
        "max_pco2_0_6h","min_ph_0_6h","max_pco2_0_24h","min_ph_0_24h",
        "flag_abg_hypercapnia","flag_vbg_hypercapnia","flag_other_hypercapnia","flag_any_gas_hypercapnia",
        "gas_source_other_rate",
        "dt_first_imv_hours","dt_first_niv_hours","first_other_time",
    ],
    "omr": [
        "bmi_closest_pre_ed","height_closest_pre_ed","weight_closest_pre_ed",
    ],
    "dx_flags": [
        "flag_copd","flag_osa_ohs","flag_chf","flag_neuromuscular",
        "flag_opioid_substance","flag_pneumonia",
    ],
    "timing": [
        "dt_first_qualifying_gas_hours","presenting_hypercapnia","late_hypercapnia",
        "dt_first_imv_hours","dt_first_niv_hours","abg_before_imv","vbg_before_imv",
        "ph_band","hco3_band","lactate_band",
    ],
}

TARGET_DERIVED_FIELDS = [
    "hospital_los_hours","in_hospital_death",
]

TARGET_FIELDS = sorted({c for v in TARGET_RAW_FIELDS.values() for c in v} | set(TARGET_DERIVED_FIELDS))

# Grouped missing report
missing_by_group = {}
for group, cols in TARGET_RAW_FIELDS.items():
    missing_by_group[group] = [c for c in cols if c not in df.columns]

missing_derived = [c for c in TARGET_DERIVED_FIELDS if c not in df.columns]
missing_by_group["derived"] = missing_derived

missing_flat = [c for cols in missing_by_group.values() for c in cols]
print("Missing fields total:", len(missing_flat))
for group, cols in missing_by_group.items():
    if cols:
        print(f"- {group}: {cols}")




[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Purpose: Capture ED presentation context (chief complaint, acuity, and early vitals) at the ED-stay level.\n\n# Target field registry\nTARGET_RAW_FIELDS = {\n    "ed_edstays": [\n        "ed_stay_id","subject_id","hadm_id","ed_intime","ed_outtime","ed_intime_first",\n        "arrival_transport","disposition","ed_gender","ed_race",\n    ],\n    "ed_triage": [\n        "ed_triage_temp","ed_triage_hr","ed_triage_rr","ed_triage_o2sat",\n        "ed_triage_sbp","ed_triage_dbp","ed_triage_pain","ed_triage_acuity","ed_triage_cc",\n    ],\n    "ed_vitals_first": [\n        "ed_first_vitals_time","ed_first_temp","ed_first_hr","ed_first_rr",\n        "ed_first_o2sat","ed_first_sbp","ed_first_dbp","ed_first_rhythm","ed_first_pain",\n    ],\n    "admissions": [\n        "admittime","dischtime","deathtime","hospital_expire_flag",\n        "admission_type","admission_location","discharge_location",\n        "insurance","language","marital_status","hosp_race",\n    ],\n    "icu": [\n        "icu_stay_id","icu_intime_first","icu_outtime_last","icu_los_total","n_icu_stays",\n        "first_careunit","last_careunit",\n    ],\n    "labs_gas": [\n        "first_gas_time","first_pco2","first_ph","first_hco3","first_lactate",\n        "max_pco2_0_6h","min_ph_0_6h","max_pco2_0_24h","min_ph_0_24h",\n        "flag_abg_hypercapnia","flag_vbg_hypercapnia","flag_other_hypercapnia","flag_any_gas_hypercapnia",\n        "gas_source_other_rate",\n        "dt_first_imv_hours","dt_first_niv_hours","first_other_time",\n    ],\n    "omr": [\n        "bmi_closest_pre_ed","height_closest_pre_ed","weight_closest_pre_ed",\n    ],\n    "dx_flags": [\n        "flag_copd","flag_osa_ohs","flag_chf","flag_neuromuscular",\n        "flag_opioid_substance","flag_pneumonia",\n    ],\n    "timing": [\n        "dt_first_qualifying_gas_hours","presenting_hypercapnia","late_hypercapnia",\n        "dt_first_imv_hours","dt_first_niv_hours","abg_before_imv","vbg_before_imv",\n        "ph_band","hco3_band","lactate_band",\n    ],\n}\n\nTARGET_DERIVED_FIELDS = [\n    "hospital_los_hours","in_hospital_death",\n]\n\nTARGET_FIELDS = sorted({c for v in TARGET_RAW_FIELDS.values() for c in v} | set(TARGET_DERIVED_FIELDS))\n\n# Grouped missing report\nmissing_by_group = {}\nfor group, cols in TARGET_RAW_FIELDS.items():\n    missing_by_group[group] = [c for c in cols if c not in df.columns]\n\nmissing_derived = [c for c in TARGET_DERIVED_FIELDS if c not in df.columns]\nmissing_by_group["derived"] = missing_derived\n\nmissing_flat = [c for cols in missing_by_group.values() for c in cols]\nprint("Missing fields total:", len(missing_flat))\nfor group, cols in missing_by_group.items():\n    if cols:\n        print(f"- {group}: {cols}")\n\n\n\n', 'execution_count': 25}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': "Missing fields total: 54\n- ed_edstays: ['ed_stay_id', 'ed_intime', 'ed_outtime', 'ed_intime_first', 'arrival_transport', 'disposition', 'ed_gender', 'ed_race']\n- admissions: ['hospital_expire_flag', 'language', 'marital_status', 'hosp_race']\n- icu: ['icu_stay_id', 'icu_intime_first', 'icu_outtime_last', 'icu_los_total', 'n_icu_stays', 'first_careunit', 'last_careunit']\n- labs_gas: ['first_gas_time', 'first_pco2', 'first_ph', 'first_hco3', 'first_lactate', 'max_pco2_0_6h', 'min_ph_0_6h', 'max_pco2_0_24h', 'min_ph_0_24h', 'flag_abg_hypercapnia', 'flag_vbg_hypercapnia', 'flag_other_hypercapnia', 'flag_any_gas_hypercapnia', 'gas_source_other_rate', 'dt_first_imv_hours', 'dt_first_niv_hours']\n- omr: ['bmi_closest_pre_ed', 'height_closest_pre_ed', 'weight_closest_pre_ed']\n- dx_flags: ['flag_copd', 'flag_osa_ohs', 'flag_chf', 'flag_neuromuscular', 'flag_opioid_substance', 'flag_pneumonia']\n- timing: ['dt_first_qualifying_gas_hours', 'presenting_hypercapnia', 'late_hypercapnia', 'dt_first_imv_hours', 'dt_first_niv_hours', 'ph_band', 'hco3_band', 'lactate_band']\n- derived: ['hospital_los_hours', 'in_hospital_death']\n"}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 63
[NbConvertApp] Executing cell:
# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.

# ED stay spine (rename stay_id to ed_stay_id)

SQL["ed_spine_sql"] = f"""
WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x)
SELECT
  s.stay_id AS ed_stay_id,
  s.subject_id,
  s.hadm_id,
  s.intime AS ed_intime,
  s.outtime AS ed_outtime,
  s.arrival_transport,
  s.disposition,
  s.gender AS ed_gender,
  s.race   AS ed_race
FROM `{PHYS}.{ED}.edstays` s
JOIN hadms h ON h.hadm_id = s.hadm_id
"""

ed_spine = run_sql_bq(sql("ed_spine_sql"), {"hadms": hadm_list})
print("ED spine rows:", len(ed_spine), "unique ed_stay_id:", ed_spine["ed_stay_id"].nunique())

# ensure uniqueness
if ed_spine["ed_stay_id"].nunique() != len(ed_spine):
    raise ValueError("ed_stay_id not unique in ED spine")

# First ED presentation time per admission
ed_intime_first = (
    ed_spine.groupby("hadm_id", as_index=False)["ed_intime"]
    .min()
    .rename(columns={"ed_intime": "ed_intime_first"})
)

# Start ED-level df
ed_df = ed_spine.copy()
ed_df = ed_df.merge(ed_intime_first, on="hadm_id", how="left")


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.\n\n# ED stay spine (rename stay_id to ed_stay_id)\n\nSQL["ed_spine_sql"] = f"""\nWITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x)\nSELECT\n  s.stay_id AS ed_stay_id,\n  s.subject_id,\n  s.hadm_id,\n  s.intime AS ed_intime,\n  s.outtime AS ed_outtime,\n  s.arrival_transport,\n  s.disposition,\n  s.gender AS ed_gender,\n  s.race   AS ed_race\nFROM `{PHYS}.{ED}.edstays` s\nJOIN hadms h ON h.hadm_id = s.hadm_id\n"""\n\ned_spine = run_sql_bq(sql("ed_spine_sql"), {"hadms": hadm_list})\nprint("ED spine rows:", len(ed_spine), "unique ed_stay_id:", ed_spine["ed_stay_id"].nunique())\n\n# ensure uniqueness\nif ed_spine["ed_stay_id"].nunique() != len(ed_spine):\n    raise ValueError("ed_stay_id not unique in ED spine")\n\n# First ED presentation time per admission\ned_intime_first = (\n    ed_spine.groupby("hadm_id", as_index=False)["ed_intime"]\n    .min()\n    .rename(columns={"ed_intime": "ed_intime_first"})\n)\n\n# Start ED-level df\ned_df = ed_spine.copy()\ned_df = ed_df.merge(ed_intime_first, on="hadm_id", how="left")\n\n', 'execution_count': 26}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ED spine rows: 41394 unique ed_stay_id: 41394\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.

# ED triage and first ED vitals (reuse existing logic if present; otherwise join)

def _needs_cols(df, cols):
    return (df is None) or any(c not in df.columns for c in cols)

# Use existing ed_triage / ed_first if already in memory from earlier cells
try:
    _ = ed_triage
except NameError:
    ed_triage = None

try:
    _ = ed_first
except NameError:
    ed_first = None

# Force re-query if required keys are missing
if _needs_cols(locals().get('ed_triage', None), ['ed_stay_id', 'hadm_id']):
    ed_triage = None
if _needs_cols(locals().get('ed_first', None), ['ed_stay_id']):
    ed_first = None

# If missing, prefer in-memory tables loaded earlier in the notebook.
# This avoids re-running expensive ED queries during nbconvert.
if ed_triage is None:
    if 'edmap' in globals() and 'tri_all' in globals():
        ed_triage = (
            edmap[['stay_id', 'hadm_id']]
            .merge(tri_all, on='stay_id', how='left')
            .rename(columns={'stay_id': 'ed_stay_id'})
        )
        print('ED triage rows (from in-memory tables):', len(ed_triage))
    else:
        ed_triage_sql = f"""
        WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x)
        SELECT
          s.stay_id AS ed_stay_id,
          s.hadm_id,
          t.temperature    AS ed_triage_temp,
          t.heartrate      AS ed_triage_hr,
          t.resprate       AS ed_triage_rr,
          t.o2sat          AS ed_triage_o2sat,
          t.sbp            AS ed_triage_sbp,
          t.dbp            AS ed_triage_dbp,
          t.pain           AS ed_triage_pain,
          t.acuity         AS ed_triage_acuity,
          t.chiefcomplaint AS ed_triage_cc
        FROM `{PHYS}.{ED}.edstays` s
        JOIN hadms h ON h.hadm_id = s.hadm_id
        LEFT JOIN `{PHYS}.{ED}.triage` t ON t.stay_id = s.stay_id
        """
        ed_triage = run_sql_bq(ed_triage_sql, {'hadms': hadm_list})
        print('ED triage rows (fallback query):', len(ed_triage))

if ed_first is None:
    if 'edmap' in globals() and 'first_stay_all' in globals():
        ed_first = (
            edmap[['stay_id', 'hadm_id']]
            .merge(first_stay_all, on='stay_id', how='left')
            .rename(columns={'stay_id': 'ed_stay_id'})
        )
        print('ED first vitals rows (from in-memory tables):', len(ed_first))
    else:
        ed_first_vitals_sql = f"""
        WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x),
        edmap AS (
          SELECT stay_id, hadm_id
          FROM `{PHYS}.{ED}.edstays`
          WHERE hadm_id IN (SELECT hadm_id FROM hadms)
        ),
        vs AS (
          SELECT stay_id, charttime, temperature, heartrate, resprate, o2sat, sbp, dbp, rhythm, pain
          FROM `{PHYS}.{ED}.vitalsign`
        ),
        first_vs AS (
          SELECT
            v.stay_id,
            (ARRAY_AGG(STRUCT(v.charttime, v.temperature, v.heartrate, v.resprate, v.o2sat, v.sbp, v.dbp, v.rhythm, v.pain)
                       ORDER BY v.charttime LIMIT 1))[OFFSET(0)] AS pick
          FROM vs v
          JOIN edmap m USING (stay_id)
          GROUP BY v.stay_id
        )
        SELECT
          f.stay_id AS ed_stay_id,
          m.hadm_id,
          pick.charttime AS ed_first_vitals_time,
          pick.temperature AS ed_first_temp,
          pick.heartrate AS ed_first_hr,
          pick.resprate AS ed_first_rr,
          pick.o2sat AS ed_first_o2sat,
          pick.sbp AS ed_first_sbp,
          pick.dbp AS ed_first_dbp,
          pick.rhythm AS ed_first_rhythm,
          pick.pain AS ed_first_pain
        FROM first_vs f
        JOIN edmap m ON m.stay_id = f.stay_id
        """
        ed_first = run_sql_bq(ed_first_vitals_sql, {'hadms': hadm_list})
        print('ED first vitals rows (fallback query):', len(ed_first))

# Debug columns before merge
print('ed_triage cols:', list(ed_triage.columns))
print('ed_first cols:', list(ed_first.columns))
print('ed_df cols:', list(ed_df.columns))

if 'ed_stay_id' not in ed_df.columns:
    raise KeyError('ed_df missing ed_stay_id; ensure ED spine cell ran.')

# Merge ED triage + vitals onto ed_df with available keys
merge_keys_triage = [k for k in ["ed_stay_id", "hadm_id"] if k in ed_df.columns and k in ed_triage.columns]
if not merge_keys_triage:
    raise KeyError("No common keys between ed_df and ed_triage")
ed_df = ed_df.merge(ed_triage, on=merge_keys_triage, how="left")
merge_keys_first = [k for k in ["ed_stay_id", "hadm_id"] if k in ed_df.columns and k in ed_first.columns]
if not merge_keys_first:
    raise KeyError("No common keys between ed_df and ed_first")
ed_df = ed_df.merge(ed_first, on=merge_keys_first, how="left")





[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.\n\n# ED triage and first ED vitals (reuse existing logic if present; otherwise join)\n\ndef _needs_cols(df, cols):\n    return (df is None) or any(c not in df.columns for c in cols)\n\n# Use existing ed_triage / ed_first if already in memory from earlier cells\ntry:\n    _ = ed_triage\nexcept NameError:\n    ed_triage = None\n\ntry:\n    _ = ed_first\nexcept NameError:\n    ed_first = None\n\n# Force re-query if required keys are missing\nif _needs_cols(locals().get(\'ed_triage\', None), [\'ed_stay_id\', \'hadm_id\']):\n    ed_triage = None\nif _needs_cols(locals().get(\'ed_first\', None), [\'ed_stay_id\']):\n    ed_first = None\n\n# If missing, prefer in-memory tables loaded earlier in the notebook.\n# This avoids re-running expensive ED queries during nbconvert.\nif ed_triage is None:\n    if \'edmap\' in globals() and \'tri_all\' in globals():\n        ed_triage = (\n            edmap[[\'stay_id\', \'hadm_id\']]\n            .merge(tri_all, on=\'stay_id\', how=\'left\')\n            .rename(columns={\'stay_id\': \'ed_stay_id\'})\n        )\n        print(\'ED triage rows (from in-memory tables):\', len(ed_triage))\n    else:\n        ed_triage_sql = f"""\n        WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x)\n        SELECT\n          s.stay_id AS ed_stay_id,\n          s.hadm_id,\n          t.temperature    AS ed_triage_temp,\n          t.heartrate      AS ed_triage_hr,\n          t.resprate       AS ed_triage_rr,\n          t.o2sat          AS ed_triage_o2sat,\n          t.sbp            AS ed_triage_sbp,\n          t.dbp            AS ed_triage_dbp,\n          t.pain           AS ed_triage_pain,\n          t.acuity         AS ed_triage_acuity,\n          t.chiefcomplaint AS ed_triage_cc\n        FROM `{PHYS}.{ED}.edstays` s\n        JOIN hadms h ON h.hadm_id = s.hadm_id\n        LEFT JOIN `{PHYS}.{ED}.triage` t ON t.stay_id = s.stay_id\n        """\n        ed_triage = run_sql_bq(ed_triage_sql, {\'hadms\': hadm_list})\n        print(\'ED triage rows (fallback query):\', len(ed_triage))\n\nif ed_first is None:\n    if \'edmap\' in globals() and \'first_stay_all\' in globals():\n        ed_first = (\n            edmap[[\'stay_id\', \'hadm_id\']]\n            .merge(first_stay_all, on=\'stay_id\', how=\'left\')\n            .rename(columns={\'stay_id\': \'ed_stay_id\'})\n        )\n        print(\'ED first vitals rows (from in-memory tables):\', len(ed_first))\n    else:\n        ed_first_vitals_sql = f"""\n        WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x),\n        edmap AS (\n          SELECT stay_id, hadm_id\n          FROM `{PHYS}.{ED}.edstays`\n          WHERE hadm_id IN (SELECT hadm_id FROM hadms)\n        ),\n        vs AS (\n          SELECT stay_id, charttime, temperature, heartrate, resprate, o2sat, sbp, dbp, rhythm, pain\n          FROM `{PHYS}.{ED}.vitalsign`\n        ),\n        first_vs AS (\n          SELECT\n            v.stay_id,\n            (ARRAY_AGG(STRUCT(v.charttime, v.temperature, v.heartrate, v.resprate, v.o2sat, v.sbp, v.dbp, v.rhythm, v.pain)\n                       ORDER BY v.charttime LIMIT 1))[OFFSET(0)] AS pick\n          FROM vs v\n          JOIN edmap m USING (stay_id)\n          GROUP BY v.stay_id\n        )\n        SELECT\n          f.stay_id AS ed_stay_id,\n          m.hadm_id,\n          pick.charttime AS ed_first_vitals_time,\n          pick.temperature AS ed_first_temp,\n          pick.heartrate AS ed_first_hr,\n          pick.resprate AS ed_first_rr,\n          pick.o2sat AS ed_first_o2sat,\n          pick.sbp AS ed_first_sbp,\n          pick.dbp AS ed_first_dbp,\n          pick.rhythm AS ed_first_rhythm,\n          pick.pain AS ed_first_pain\n        FROM first_vs f\n        JOIN edmap m ON m.stay_id = f.stay_id\n        """\n        ed_first = run_sql_bq(ed_first_vitals_sql, {\'hadms\': hadm_list})\n        print(\'ED first vitals rows (fallback query):\', len(ed_first))\n\n# Debug columns before merge\nprint(\'ed_triage cols:\', list(ed_triage.columns))\nprint(\'ed_first cols:\', list(ed_first.columns))\nprint(\'ed_df cols:\', list(ed_df.columns))\n\nif \'ed_stay_id\' not in ed_df.columns:\n    raise KeyError(\'ed_df missing ed_stay_id; ensure ED spine cell ran.\')\n\n# Merge ED triage + vitals onto ed_df with available keys\nmerge_keys_triage = [k for k in ["ed_stay_id", "hadm_id"] if k in ed_df.columns and k in ed_triage.columns]\nif not merge_keys_triage:\n    raise KeyError("No common keys between ed_df and ed_triage")\ned_df = ed_df.merge(ed_triage, on=merge_keys_triage, how="left")\nmerge_keys_first = [k for k in ["ed_stay_id", "hadm_id"] if k in ed_df.columns and k in ed_first.columns]\nif not merge_keys_first:\n    raise KeyError("No common keys between ed_df and ed_first")\ned_df = ed_df.merge(ed_first, on=merge_keys_first, how="left")\n\n\n\n\n', 'execution_count': 27}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': "ED triage rows (from in-memory tables): 41394\nED first vitals rows (from in-memory tables): 41394\ned_triage cols: ['ed_stay_id', 'hadm_id', 'ed_triage_temp', 'ed_triage_hr', 'ed_triage_rr', 'ed_triage_o2sat', 'ed_triage_sbp', 'ed_triage_dbp', 'ed_triage_pain', 'ed_triage_acuity', 'ed_triage_cc']\ned_first cols: ['ed_stay_id', 'hadm_id', 'ed_first_vitals_time', 'ed_first_temp', 'ed_first_hr', 'ed_first_rr', 'ed_first_o2sat', 'ed_first_sbp', 'ed_first_dbp', 'ed_first_rhythm', 'ed_first_pain']\ned_df cols: ['ed_stay_id', 'subject_id', 'hadm_id', 'ed_intime', 'ed_outtime', 'arrival_transport', 'disposition', 'ed_gender', 'ed_race', 'ed_intime_first']\n"}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 66
[NbConvertApp] Executing cell:
# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.

# Admissions fields
SQL["admit_sql"] = f"""
WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x)
SELECT
  a.hadm_id,
  a.admittime,
  a.dischtime,
  a.deathtime,
  a.hospital_expire_flag,
  a.admission_type,
  a.admission_location,
  a.discharge_location,
  a.insurance,
  a.language,
  a.marital_status,
  a.race AS hosp_race
FROM `{PHYS}.{HOSP}.admissions` a
JOIN hadms h USING (hadm_id)
"""

admit = run_sql_bq(sql("admit_sql"), {"hadms": hadm_list})
print("Admissions rows:", len(admit))

ed_df = ed_df.merge(admit, on="hadm_id", how="left")

# Merge ventilation flags/times (hadm-level)
if "vent_combined" in globals():
    ed_df = ed_df.merge(vent_combined, on="hadm_id", how="left")

# Derived outcomes
ed_df["hospital_los_hours"] = (ed_df["dischtime"] - ed_df["admittime"]).dt.total_seconds() / 3600.0
ed_df["in_hospital_death"] = ((ed_df["hospital_expire_flag"] == 1) | ed_df["deathtime"].notna()).astype("int64")

# Concordance check
discord = (
    ((ed_df["hospital_expire_flag"] == 1) & ed_df["deathtime"].isna()) |
    ((ed_df["hospital_expire_flag"] == 0) & ed_df["deathtime"].notna())
)
print("Admissions discordance (expire_flag vs deathtime):", int(discord.sum()))


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.\n\n# Admissions fields\nSQL["admit_sql"] = f"""\nWITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x)\nSELECT\n  a.hadm_id,\n  a.admittime,\n  a.dischtime,\n  a.deathtime,\n  a.hospital_expire_flag,\n  a.admission_type,\n  a.admission_location,\n  a.discharge_location,\n  a.insurance,\n  a.language,\n  a.marital_status,\n  a.race AS hosp_race\nFROM `{PHYS}.{HOSP}.admissions` a\nJOIN hadms h USING (hadm_id)\n"""\n\nadmit = run_sql_bq(sql("admit_sql"), {"hadms": hadm_list})\nprint("Admissions rows:", len(admit))\n\ned_df = ed_df.merge(admit, on="hadm_id", how="left")\n\n# Merge ventilation flags/times (hadm-level)\nif "vent_combined" in globals():\n    ed_df = ed_df.merge(vent_combined, on="hadm_id", how="left")\n\n# Derived outcomes\ned_df["hospital_los_hours"] = (ed_df["dischtime"] - ed_df["admittime"]).dt.total_seconds() / 3600.0\ned_df["in_hospital_death"] = ((ed_df["hospital_expire_flag"] == 1) | ed_df["deathtime"].notna()).astype("int64")\n\n# Concordance check\ndiscord = (\n    ((ed_df["hospital_expire_flag"] == 1) & ed_df["deathtime"].isna()) |\n    ((ed_df["hospital_expire_flag"] == 0) & ed_df["deathtime"].notna())\n)\nprint("Admissions discordance (expire_flag vs deathtime):", int(discord.sum()))\n\n', 'execution_count': 28}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Admissions rows: 115179\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Admissions discordance (expire_flag vs deathtime): 4\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 68
[NbConvertApp] Executing cell:
# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.

# ICU stays (aggregate per hadm)
SQL["icu_sql"] = f"""
WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x)
SELECT
  i.hadm_id,
  i.stay_id AS icu_stay_id,
  i.intime,
  i.outtime,
  i.los,
  i.first_careunit,
  i.last_careunit
FROM `{PHYS}.{ICU}.icustays` i
JOIN hadms h USING (hadm_id)
"""

icu = run_sql_bq(sql("icu_sql"), {"hadms": hadm_list})
print("ICU stay rows:", len(icu))

if len(icu) > 0:
    icu_agg = (
        icu.sort_values(["hadm_id", "intime"]).groupby("hadm_id", as_index=False)
        .agg(
            icu_intime_first=("intime", "min"),
            icu_outtime_last=("outtime", "max"),
            icu_los_total=("los", "sum"),
            n_icu_stays=("icu_stay_id", "nunique"),
            first_careunit=("first_careunit", "first"),
            last_careunit=("last_careunit", "last"),
        )
    )
    ed_df = ed_df.merge(icu_agg, on="hadm_id", how="left")
else:
    print("No ICU stays found for cohort.")


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.\n\n# ICU stays (aggregate per hadm)\nSQL["icu_sql"] = f"""\nWITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x)\nSELECT\n  i.hadm_id,\n  i.stay_id AS icu_stay_id,\n  i.intime,\n  i.outtime,\n  i.los,\n  i.first_careunit,\n  i.last_careunit\nFROM `{PHYS}.{ICU}.icustays` i\nJOIN hadms h USING (hadm_id)\n"""\n\nicu = run_sql_bq(sql("icu_sql"), {"hadms": hadm_list})\nprint("ICU stay rows:", len(icu))\n\nif len(icu) > 0:\n    icu_agg = (\n        icu.sort_values(["hadm_id", "intime"]).groupby("hadm_id", as_index=False)\n        .agg(\n            icu_intime_first=("intime", "min"),\n            icu_outtime_last=("outtime", "max"),\n            icu_los_total=("los", "sum"),\n            n_icu_stays=("icu_stay_id", "nunique"),\n            first_careunit=("first_careunit", "first"),\n            last_careunit=("last_careunit", "last"),\n        )\n    )\n    ed_df = ed_df.merge(icu_agg, on="hadm_id", how="left")\nelse:\n    print("No ICU stays found for cohort.")\n\n', 'execution_count': 29}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ICU stay rows: 94431\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 70
[NbConvertApp] Executing cell:
# Purpose: Build ED vitals features robustly without a single large BigQuery join that can stall.

# ED vitals long + aggregates (0-6h)

# Build the ED stay map from in-memory cohort tables to keep query scope tight and deterministic.
if "ed_df" not in globals():
    raise NameError("ed_df is required before ED vitals extraction")

required_cols = ["ed_stay_id", "hadm_id", "ed_intime"]
missing_required = [c for c in required_cols if c not in ed_df.columns]
if missing_required:
    raise KeyError(f"ed_df missing required columns for ED vitals extraction: {missing_required}")

edmap_local = (
    ed_df[required_cols]
    .dropna(subset=["ed_stay_id"])
    .drop_duplicates(subset=["ed_stay_id"])
    .copy()
)
edmap_local["ed_stay_id"] = pd.to_numeric(edmap_local["ed_stay_id"], errors="coerce")
edmap_local = edmap_local.dropna(subset=["ed_stay_id"])
edmap_local["ed_stay_id"] = edmap_local["ed_stay_id"].astype(int)

stay_ids = sorted(edmap_local["ed_stay_id"].unique().tolist())
print("ED stays for vitals pull:", len(stay_ids))

SQL["ed_vitals_sql"] = f"""
SELECT
  stay_id AS ed_stay_id,
  charttime,
  temperature,
  heartrate,
  resprate,
  o2sat,
  sbp,
  dbp,
  rhythm,
  pain
FROM `{PHYS}.{ED}.vitalsign`
WHERE stay_id IN UNNEST(@stay_ids)
"""

# Query in chunks so we avoid one very large parameter payload/job.
chunk_size = 5000
vitals_chunks = []
for i in range(0, len(stay_ids), chunk_size):
    chunk = stay_ids[i:i + chunk_size]
    part = run_sql_bq(sql("ed_vitals_sql"), {"stay_ids": chunk})
    if len(part) > 0:
        part["ed_stay_id"] = pd.to_numeric(part["ed_stay_id"], errors="coerce")
        part = part.dropna(subset=["ed_stay_id"])
        part["ed_stay_id"] = part["ed_stay_id"].astype(int)
        vitals_chunks.append(part)
    print(f"ED vitals chunk {i // chunk_size + 1}: rows={len(part)}")

if vitals_chunks:
    ed_vitals_long = pd.concat(vitals_chunks, ignore_index=True)
else:
    ed_vitals_long = pd.DataFrame(
        columns=[
            "ed_stay_id", "charttime", "temperature", "heartrate", "resprate",
            "o2sat", "sbp", "dbp", "rhythm", "pain"
        ]
    )

ed_vitals_long = ed_vitals_long.merge(edmap_local, on="ed_stay_id", how="inner")
print("ED vitals long rows:", len(ed_vitals_long))

# Window filter: 0-6h from ED intime
ed_vitals_long["dt_hours"] = (ed_vitals_long["charttime"] - ed_vitals_long["ed_intime"]).dt.total_seconds() / 3600.0
in_6h = ed_vitals_long["dt_hours"].between(0, 6, inclusive="both")

agg = (
    ed_vitals_long.loc[in_6h]
    .groupby("ed_stay_id", as_index=False)
    .agg(
        max_heartrate_0_6h=("heartrate", "max"),
        max_resprate_0_6h=("resprate", "max"),
        min_o2sat_0_6h=("o2sat", "min"),
        min_sbp_0_6h=("sbp", "min"),
        n_vitals_0_6h=("charttime", "count"),
    )
)

ed_df = ed_df.merge(agg, on="ed_stay_id", how="left")

# Range warnings (report only, do not drop)
range_checks = {
    "heartrate": (0, 300),
    "resprate": (0, 80),
    "o2sat": (0, 100),
    "sbp": (0, 300),
}
for col, (lo, hi) in range_checks.items():
    bad = ed_vitals_long[col].notna() & (~ed_vitals_long[col].between(lo, hi))
    if bad.any():
        print(f"Warning: {col} out of range count:", int(bad.sum()))


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Purpose: Build ED vitals features robustly without a single large BigQuery join that can stall.\n\n# ED vitals long + aggregates (0-6h)\n\n# Build the ED stay map from in-memory cohort tables to keep query scope tight and deterministic.\nif "ed_df" not in globals():\n    raise NameError("ed_df is required before ED vitals extraction")\n\nrequired_cols = ["ed_stay_id", "hadm_id", "ed_intime"]\nmissing_required = [c for c in required_cols if c not in ed_df.columns]\nif missing_required:\n    raise KeyError(f"ed_df missing required columns for ED vitals extraction: {missing_required}")\n\nedmap_local = (\n    ed_df[required_cols]\n    .dropna(subset=["ed_stay_id"])\n    .drop_duplicates(subset=["ed_stay_id"])\n    .copy()\n)\nedmap_local["ed_stay_id"] = pd.to_numeric(edmap_local["ed_stay_id"], errors="coerce")\nedmap_local = edmap_local.dropna(subset=["ed_stay_id"])\nedmap_local["ed_stay_id"] = edmap_local["ed_stay_id"].astype(int)\n\nstay_ids = sorted(edmap_local["ed_stay_id"].unique().tolist())\nprint("ED stays for vitals pull:", len(stay_ids))\n\nSQL["ed_vitals_sql"] = f"""\nSELECT\n  stay_id AS ed_stay_id,\n  charttime,\n  temperature,\n  heartrate,\n  resprate,\n  o2sat,\n  sbp,\n  dbp,\n  rhythm,\n  pain\nFROM `{PHYS}.{ED}.vitalsign`\nWHERE stay_id IN UNNEST(@stay_ids)\n"""\n\n# Query in chunks so we avoid one very large parameter payload/job.\nchunk_size = 5000\nvitals_chunks = []\nfor i in range(0, len(stay_ids), chunk_size):\n    chunk = stay_ids[i:i + chunk_size]\n    part = run_sql_bq(sql("ed_vitals_sql"), {"stay_ids": chunk})\n    if len(part) > 0:\n        part["ed_stay_id"] = pd.to_numeric(part["ed_stay_id"], errors="coerce")\n        part = part.dropna(subset=["ed_stay_id"])\n        part["ed_stay_id"] = part["ed_stay_id"].astype(int)\n        vitals_chunks.append(part)\n    print(f"ED vitals chunk {i // chunk_size + 1}: rows={len(part)}")\n\nif vitals_chunks:\n    ed_vitals_long = pd.concat(vitals_chunks, ignore_index=True)\nelse:\n    ed_vitals_long = pd.DataFrame(\n        columns=[\n            "ed_stay_id", "charttime", "temperature", "heartrate", "resprate",\n            "o2sat", "sbp", "dbp", "rhythm", "pain"\n        ]\n    )\n\ned_vitals_long = ed_vitals_long.merge(edmap_local, on="ed_stay_id", how="inner")\nprint("ED vitals long rows:", len(ed_vitals_long))\n\n# Window filter: 0-6h from ED intime\ned_vitals_long["dt_hours"] = (ed_vitals_long["charttime"] - ed_vitals_long["ed_intime"]).dt.total_seconds() / 3600.0\nin_6h = ed_vitals_long["dt_hours"].between(0, 6, inclusive="both")\n\nagg = (\n    ed_vitals_long.loc[in_6h]\n    .groupby("ed_stay_id", as_index=False)\n    .agg(\n        max_heartrate_0_6h=("heartrate", "max"),\n        max_resprate_0_6h=("resprate", "max"),\n        min_o2sat_0_6h=("o2sat", "min"),\n        min_sbp_0_6h=("sbp", "min"),\n        n_vitals_0_6h=("charttime", "count"),\n    )\n)\n\ned_df = ed_df.merge(agg, on="ed_stay_id", how="left")\n\n# Range warnings (report only, do not drop)\nrange_checks = {\n    "heartrate": (0, 300),\n    "resprate": (0, 80),\n    "o2sat": (0, 100),\n    "sbp": (0, 300),\n}\nfor col, (lo, hi) in range_checks.items():\n    bad = ed_vitals_long[col].notna() & (~ed_vitals_long[col].between(lo, hi))\n    if bad.any():\n        print(f"Warning: {col} out of range count:", int(bad.sum()))\n\n', 'execution_count': 30}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ED stays for vitals pull: 41394\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ED vitals chunk 1: rows=31146\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ED vitals chunk 2: rows=30528\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ED vitals chunk 3: rows=29989\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ED vitals chunk 4: rows=31430\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ED vitals chunk 5: rows=30571\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ED vitals chunk 6: rows=31182\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ED vitals chunk 7: rows=30502\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ED vitals chunk 8: rows=30139\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ED vitals chunk 9: rows=8665\nED vitals long rows: 254152\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Warning: heartrate out of range count: 1\nWarning: resprate out of range count: 4\nWarning: o2sat out of range count: 5\nWarning: sbp out of range count: 1\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 72
[NbConvertApp] Executing cell:
# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.

import re
import json

# Discover itemids from d_labitems
SQL["labitems_sql"] = f"""
SELECT itemid, label, fluid, category
FROM `{PHYS}.{HOSP}.d_labitems`
"""

labitems = run_sql_bq(sql("labitems_sql"))

patterns = {
    "gas_pco2": re.compile(r"\bp\s*co2\b|pco2|pco₂", re.I),
    "gas_ph": re.compile(r"\bph\b", re.I),
    "gas_hco3": re.compile(r"hco3|bicarbonate", re.I),
    "gas_lactate": re.compile(r"lactate", re.I),
    "gas_specimen": re.compile(r"specimen|source|type", re.I),
    "chem_creatinine": re.compile(r"creatinine", re.I),
    "chem_sodium": re.compile(r"\bsodium\b", re.I),
    "chem_chloride": re.compile(r"\bchloride\b", re.I),
    "chem_total_co2": re.compile(r"carbon dioxide|total co2|\bco2\b", re.I),
    "cbc_hemoglobin": re.compile(r"hemoglobin", re.I),
}

# category filters
cat_gas = re.compile(r"blood\s*gas|blood gas|arterial|venous", re.I)
cat_chem = re.compile(r"chemistry|chem|blood", re.I)
cat_cbc = re.compile(r"hematology|cbc", re.I)

matches = {}
for name, pat in patterns.items():
    dfm = labitems.copy()
    dfm = dfm[dfm["label"].str.contains(pat, na=False)]
    if name.startswith("gas_"):
        dfm = dfm[dfm["category"].str.contains(cat_gas, na=False)]
    elif name.startswith("chem_"):
        dfm = dfm[dfm["category"].str.contains(cat_chem, na=False)]
    elif name.startswith("cbc_"):
        dfm = dfm[dfm["category"].str.contains(cat_cbc, na=False)]
    matches[name] = dfm[["itemid","label","category"]]

# Build lab_item_map with counts in cohort
itemids_all = sorted({int(i) for dfm in matches.values() for i in dfm["itemid"].tolist()})

SQL["counts_sql"] = f"""
WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x)
SELECT itemid, COUNT(*) AS n
FROM `{PHYS}.{HOSP}.labevents`
WHERE hadm_id IN (SELECT hadm_id FROM hadms)
  AND itemid IN UNNEST(@itemids)
GROUP BY itemid
"""

counts = run_sql_bq(sql("counts_sql"), {"hadms": hadm_list, "itemids": itemids_all}) if itemids_all else pd.DataFrame(columns=["itemid","n"])

lab_item_map = {}
for name, dfm in matches.items():
    tmp = dfm.merge(counts, on="itemid", how="left").fillna({"n":0})
    lab_item_map[name] = {
        "pattern": patterns[name].pattern,
        "items": tmp.sort_values("n", ascending=False).to_dict(orient="records"),
    }

lab_item_map_path = WORK_DIR / "lab_item_map.json"
lab_item_map_path.write_text(json.dumps(lab_item_map, indent=2))
print("Wrote:", lab_item_map_path)



[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Purpose: Define a reusable BigQuery execution helper so SQL calls are consistent across notebook stages.\n\nimport re\nimport json\n\n# Discover itemids from d_labitems\nSQL["labitems_sql"] = f"""\nSELECT itemid, label, fluid, category\nFROM `{PHYS}.{HOSP}.d_labitems`\n"""\n\nlabitems = run_sql_bq(sql("labitems_sql"))\n\npatterns = {\n    "gas_pco2": re.compile(r"\\bp\\s*co2\\b|pco2|pco₂", re.I),\n    "gas_ph": re.compile(r"\\bph\\b", re.I),\n    "gas_hco3": re.compile(r"hco3|bicarbonate", re.I),\n    "gas_lactate": re.compile(r"lactate", re.I),\n    "gas_specimen": re.compile(r"specimen|source|type", re.I),\n    "chem_creatinine": re.compile(r"creatinine", re.I),\n    "chem_sodium": re.compile(r"\\bsodium\\b", re.I),\n    "chem_chloride": re.compile(r"\\bchloride\\b", re.I),\n    "chem_total_co2": re.compile(r"carbon dioxide|total co2|\\bco2\\b", re.I),\n    "cbc_hemoglobin": re.compile(r"hemoglobin", re.I),\n}\n\n# category filters\ncat_gas = re.compile(r"blood\\s*gas|blood gas|arterial|venous", re.I)\ncat_chem = re.compile(r"chemistry|chem|blood", re.I)\ncat_cbc = re.compile(r"hematology|cbc", re.I)\n\nmatches = {}\nfor name, pat in patterns.items():\n    dfm = labitems.copy()\n    dfm = dfm[dfm["label"].str.contains(pat, na=False)]\n    if name.startswith("gas_"):\n        dfm = dfm[dfm["category"].str.contains(cat_gas, na=False)]\n    elif name.startswith("chem_"):\n        dfm = dfm[dfm["category"].str.contains(cat_chem, na=False)]\n    elif name.startswith("cbc_"):\n        dfm = dfm[dfm["category"].str.contains(cat_cbc, na=False)]\n    matches[name] = dfm[["itemid","label","category"]]\n\n# Build lab_item_map with counts in cohort\nitemids_all = sorted({int(i) for dfm in matches.values() for i in dfm["itemid"].tolist()})\n\nSQL["counts_sql"] = f"""\nWITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x)\nSELECT itemid, COUNT(*) AS n\nFROM `{PHYS}.{HOSP}.labevents`\nWHERE hadm_id IN (SELECT hadm_id FROM hadms)\n  AND itemid IN UNNEST(@itemids)\nGROUP BY itemid\n"""\n\ncounts = run_sql_bq(sql("counts_sql"), {"hadms": hadm_list, "itemids": itemids_all}) if itemids_all else pd.DataFrame(columns=["itemid","n"])\n\nlab_item_map = {}\nfor name, dfm in matches.items():\n    tmp = dfm.merge(counts, on="itemid", how="left").fillna({"n":0})\n    lab_item_map[name] = {\n        "pattern": patterns[name].pattern,\n        "items": tmp.sort_values("n", ascending=False).to_dict(orient="records"),\n    }\n\nlab_item_map_path = WORK_DIR / "lab_item_map.json"\nlab_item_map_path.write_text(json.dumps(lab_item_map, indent=2))\nprint("Wrote:", lab_item_map_path)\n\n\n', 'execution_count': 31}
[IPKernelApp] WARNING | Parent appears to have exited, shutting down.
