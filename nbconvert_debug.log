[NbConvertApp] Searching ['/Users/blocke/.jupyter', '/Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/.venv/etc/jupyter', '/usr/local/etc/jupyter', '/etc/jupyter'] for config files
[NbConvertApp] Looking for jupyter_config in /etc/jupyter
[NbConvertApp] Looking for jupyter_config in /usr/local/etc/jupyter
[NbConvertApp] Looking for jupyter_config in /Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/.venv/etc/jupyter
[NbConvertApp] Looking for jupyter_config in /Users/blocke/.jupyter
[NbConvertApp] Looking for jupyter_nbconvert_config in /etc/jupyter
[NbConvertApp] Looking for jupyter_nbconvert_config in /usr/local/etc/jupyter
[NbConvertApp] Looking for jupyter_nbconvert_config in /Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/.venv/etc/jupyter
[NbConvertApp] Looking for jupyter_nbconvert_config in /Users/blocke/.jupyter
[NbConvertApp] Looping through config variables with prefix "JUPYTER_NBCONVERT"
[NbConvertApp] Converting notebook MIMICIV_hypercap_EXT_cohort.ipynb to notebook
[NbConvertApp] Notebook name is 'MIMICIV_hypercap_EXT_cohort'
/Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/.venv/lib/python3.11/site-packages/nbformat/__init__.py:96: MissingIDFieldWarning: Cell is missing an id field, this will become a hard error in future nbformat versions. You may want to use `normalize()` on your notebooks before validations (available since nbformat 5.1.4). Previous versions of nbformat are fixing this issue transparently, and will stop doing so in the future.
  validate(nb)
[NbConvertApp] Applying preprocessor: ClearOutputPreprocessor
[NbConvertApp] Applying preprocessor: ExecutePreprocessor
[NbConvertApp] Instantiating kernel 'Hypercap-CC-NLP (.venv)' with kernel provisioner: local-provisioner
[NbConvertApp] Starting kernel: ['./.venv/bin/python', '-m', 'ipykernel_launcher', '-f', '/private/var/folders/vf/n84t7b8171lf64smq4ddvw1m0000gn/T/tmpu05kc7k6.json']
[NbConvertApp] Connecting to: tcp://127.0.0.1:56365
[NbConvertApp] connecting iopub channel to tcp://127.0.0.1:56362
[NbConvertApp] Connecting to: tcp://127.0.0.1:56362
[NbConvertApp] connecting shell channel to tcp://127.0.0.1:56361
[NbConvertApp] Connecting to: tcp://127.0.0.1:56361
[NbConvertApp] connecting stdin channel to tcp://127.0.0.1:56363
[NbConvertApp] Connecting to: tcp://127.0.0.1:56363
[NbConvertApp] connecting heartbeat channel to tcp://127.0.0.1:56364
[NbConvertApp] connecting control channel to tcp://127.0.0.1:56365
[NbConvertApp] Connecting to: tcp://127.0.0.1:56365
[NbConvertApp] Skipping non-executing cell 0
[NbConvertApp] Skipping non-executing cell 1
[NbConvertApp] Skipping non-executing cell 2
[NbConvertApp] Skipping non-executing cell 3
[NbConvertApp] Executing cell:
import sys
print(sys.executable)

# Central SQL registry (define all query templates here)
SQL = {}

def sql(name: str) -> str:
    if name not in SQL:
        raise KeyError(f"SQL template not found: {name}")
    return SQL[name]

# SQL templates (populated below in-place to keep notebook linear)
# Names: admit_sql, co2_thresholds_sql, cohort_icd_sql, counts_sql, demo_sql, ditems_sql, ed_counts_sql, ed_first_vitals_sql, ed_spine_sql, ed_to_icu_sql, ed_triage_sql, ed_vitals_sql, icd_sql, icu_meta_sql, icu_sql, labitems_sql, labs_sql, vent_chart_sql, vent_sql

[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': 'import sys\nprint(sys.executable)\n\n# Central SQL registry (define all query templates here)\nSQL = {}\n\ndef sql(name: str) -> str:\n    if name not in SQL:\n        raise KeyError(f"SQL template not found: {name}")\n    return SQL[name]\n\n# SQL templates (populated below in-place to keep notebook linear)\n# Names: admit_sql, co2_thresholds_sql, cohort_icd_sql, counts_sql, demo_sql, ditems_sql, ed_counts_sql, ed_first_vitals_sql, ed_spine_sql, ed_to_icu_sql, ed_triage_sql, ed_vitals_sql, icd_sql, icu_meta_sql, icu_sql, labitems_sql, labs_sql, vent_chart_sql, vent_sql\n', 'execution_count': 1}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '/Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/.venv/bin/python\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 5
[NbConvertApp] Skipping non-executing cell 6
[NbConvertApp] Skipping non-executing cell 7
[NbConvertApp] Skipping non-executing cell 8
[NbConvertApp] Skipping non-executing cell 9
[NbConvertApp] Executing cell:
# --- Imports & environment
import os, re, json, math, textwrap
from pathlib import Path

import numpy as np
import pandas as pd

from google.cloud import bigquery
from google.oauth2 import service_account
from dotenv import load_dotenv

load_dotenv()

WORK_DIR = Path(os.getenv("WORK_DIR", Path.cwd())).expanduser().resolve()
DATA_DIR = WORK_DIR / "MIMIC tabular data"
DATA_DIR.mkdir(parents=True, exist_ok=True)

# ---- Backend selection (we use BigQuery)
BACKEND = os.getenv("MIMIC_BACKEND", "bigquery").strip().lower()
assert BACKEND == "bigquery", "This notebook is BigQuery-specific."

WORK_PROJECT = os.getenv("WORK_PROJECT", "").strip()  # your billing project
PHYS = os.getenv("BQ_PHYSIONET_PROJECT", "physionet-data").strip()  # hosting project (read-only)

# Default to MIMIC-IV v3.1 datasets
HOSP = os.getenv("BQ_DATASET_HOSP", "mimiciv_3_1_hosp").strip()
ICU  = os.getenv("BQ_DATASET_ICU",  "mimiciv_3_1_icu").strip()
ED   = os.getenv("BQ_DATASET_ED",   "").strip()  # may be empty; we will auto-detect

# BigQuery client
client = bigquery.Client(project=WORK_PROJECT)

print("Project:", WORK_PROJECT)
print("PhysioNet host:", PHYS)
print("HOSP:", HOSP, "ICU:", ICU, "ED (pref):", ED)
print("WORK_DIR:", WORK_DIR)


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# --- Imports & environment\nimport os, re, json, math, textwrap\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nfrom google.cloud import bigquery\nfrom google.oauth2 import service_account\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nWORK_DIR = Path(os.getenv("WORK_DIR", Path.cwd())).expanduser().resolve()\nDATA_DIR = WORK_DIR / "MIMIC tabular data"\nDATA_DIR.mkdir(parents=True, exist_ok=True)\n\n# ---- Backend selection (we use BigQuery)\nBACKEND = os.getenv("MIMIC_BACKEND", "bigquery").strip().lower()\nassert BACKEND == "bigquery", "This notebook is BigQuery-specific."\n\nWORK_PROJECT = os.getenv("WORK_PROJECT", "").strip()  # your billing project\nPHYS = os.getenv("BQ_PHYSIONET_PROJECT", "physionet-data").strip()  # hosting project (read-only)\n\n# Default to MIMIC-IV v3.1 datasets\nHOSP = os.getenv("BQ_DATASET_HOSP", "mimiciv_3_1_hosp").strip()\nICU  = os.getenv("BQ_DATASET_ICU",  "mimiciv_3_1_icu").strip()\nED   = os.getenv("BQ_DATASET_ED",   "").strip()  # may be empty; we will auto-detect\n\n# BigQuery client\nclient = bigquery.Client(project=WORK_PROJECT)\n\nprint("Project:", WORK_PROJECT)\nprint("PhysioNet host:", PHYS)\nprint("HOSP:", HOSP, "ICU:", ICU, "ED (pref):", ED)\nprint("WORK_DIR:", WORK_DIR)\n\n', 'execution_count': 2}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Project: mimic-hypercapnia\nPhysioNet host: physionet-data\nHOSP: mimiciv_3_1_hosp ICU: mimiciv_3_1_icu ED (pref): mimiciv_ed\nWORK_DIR: /Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
# --- Helper: run SQL with optional named parameters
def run_sql_bq(sql: str, params: dict | None = None) -> pd.DataFrame:
    job_config = bigquery.QueryJobConfig()
    if params:
        bq_params = []
        for k, v in params.items():
            if isinstance(v, (list, tuple, np.ndarray, pd.Series)):
                # BigQuery ARRAY<INT64> if all ints; else ARRAY<STRING>
                v_list = list(v)
                if all(isinstance(x, (int, np.integer)) for x in v_list):
                    bq_params.append(bigquery.ArrayQueryParameter(k, "INT64", list(map(int, v_list))))
                else:
                    bq_params.append(bigquery.ArrayQueryParameter(k, "STRING", list(map(str, v_list))))
            else:
                # scalar
                if isinstance(v, (int, np.integer)):
                    bq_params.append(bigquery.ScalarQueryParameter(k, "INT64", int(v)))
                elif isinstance(v, float):
                    bq_params.append(bigquery.ScalarQueryParameter(k, "FLOAT64", float(v)))
                else:
                    bq_params.append(bigquery.ScalarQueryParameter(k, "STRING", str(v)))
        job_config.query_parameters = bq_params
    job = client.query(sql, job_config=job_config)
    try:
        return job.result().to_dataframe(create_bqstorage_client=True)
    except TypeError:
        return job.result().to_dataframe()

# --- Helper: test if a fully-qualified table exists and is accessible
def table_exists(fqtn: str) -> bool:
    try:
        _ = run_sql_bq(f"SELECT 1 FROM `{fqtn}` LIMIT 1")
        return True
    except Exception:
        return False

# --- Auto-detect ED dataset if not set
if not ED:
    candidates = [
        f"{PHYS}.mimiciv_ed.edstays",
        f"{PHYS}.mimiciv_3_1_ed.edstays",
    ]
    for cand in candidates:
        if table_exists(cand):
            ED = cand.split(".")[1]  # dataset
            break
if not ED:
    raise RuntimeError("No accessible ED dataset found. Request access to MIMIC-IV-ED in PhysioNet (BigQuery).")

print("Using ED dataset:", ED)


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# --- Helper: run SQL with optional named parameters\ndef run_sql_bq(sql: str, params: dict | None = None) -> pd.DataFrame:\n    job_config = bigquery.QueryJobConfig()\n    if params:\n        bq_params = []\n        for k, v in params.items():\n            if isinstance(v, (list, tuple, np.ndarray, pd.Series)):\n                # BigQuery ARRAY<INT64> if all ints; else ARRAY<STRING>\n                v_list = list(v)\n                if all(isinstance(x, (int, np.integer)) for x in v_list):\n                    bq_params.append(bigquery.ArrayQueryParameter(k, "INT64", list(map(int, v_list))))\n                else:\n                    bq_params.append(bigquery.ArrayQueryParameter(k, "STRING", list(map(str, v_list))))\n            else:\n                # scalar\n                if isinstance(v, (int, np.integer)):\n                    bq_params.append(bigquery.ScalarQueryParameter(k, "INT64", int(v)))\n                elif isinstance(v, float):\n                    bq_params.append(bigquery.ScalarQueryParameter(k, "FLOAT64", float(v)))\n                else:\n                    bq_params.append(bigquery.ScalarQueryParameter(k, "STRING", str(v)))\n        job_config.query_parameters = bq_params\n    job = client.query(sql, job_config=job_config)\n    try:\n        return job.result().to_dataframe(create_bqstorage_client=True)\n    except TypeError:\n        return job.result().to_dataframe()\n\n# --- Helper: test if a fully-qualified table exists and is accessible\ndef table_exists(fqtn: str) -> bool:\n    try:\n        _ = run_sql_bq(f"SELECT 1 FROM `{fqtn}` LIMIT 1")\n        return True\n    except Exception:\n        return False\n\n# --- Auto-detect ED dataset if not set\nif not ED:\n    candidates = [\n        f"{PHYS}.mimiciv_ed.edstays",\n        f"{PHYS}.mimiciv_3_1_ed.edstays",\n    ]\n    for cand in candidates:\n        if table_exists(cand):\n            ED = cand.split(".")[1]  # dataset\n            break\nif not ED:\n    raise RuntimeError("No accessible ED dataset found. Request access to MIMIC-IV-ED in PhysioNet (BigQuery).")\n\nprint("Using ED dataset:", ED)\n\n', 'execution_count': 3}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Using ED dataset: mimiciv_ed\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
# --- Helper utilities for reproducibility and safe joins
def require_cols(df: pd.DataFrame, cols: list[str], name: str) -> None:
    missing = [c for c in cols if c not in df.columns]
    if missing:
        raise KeyError(f"{name} missing columns: {missing}")

def assert_unique(df: pd.DataFrame, key: str, name: str) -> None:
    if df[key].duplicated().any():
        n = int(df[key].duplicated().sum())
        raise ValueError(f"{name} has {n} duplicate {key} values")

def safe_merge(left: pd.DataFrame, right: pd.DataFrame, on: list[str] | str, how: str, name: str) -> pd.DataFrame:
    # guard against accidental duplicate columns
    overlap = set(left.columns) & set(right.columns)
    if isinstance(on, str):
        on_cols = {on}
    else:
        on_cols = set(on)
    overlap = overlap - on_cols
    if overlap:
        raise ValueError(f"{name} merge would duplicate columns: {sorted(overlap)}")
    return left.merge(right, on=on, how=how)

def check_ranges(df: pd.DataFrame, ranges: dict[str, tuple[float, float]]) -> pd.DataFrame:
    rows = []
    for col, (lo, hi) in ranges.items():
        if col not in df.columns:
            continue
        bad = df[col].notna() & ((df[col] < lo) | (df[col] > hi))
        rows.append({"col": col, "n_bad": int(bad.sum())})
    return pd.DataFrame(rows)


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# --- Helper utilities for reproducibility and safe joins\ndef require_cols(df: pd.DataFrame, cols: list[str], name: str) -> None:\n    missing = [c for c in cols if c not in df.columns]\n    if missing:\n        raise KeyError(f"{name} missing columns: {missing}")\n\ndef assert_unique(df: pd.DataFrame, key: str, name: str) -> None:\n    if df[key].duplicated().any():\n        n = int(df[key].duplicated().sum())\n        raise ValueError(f"{name} has {n} duplicate {key} values")\n\ndef safe_merge(left: pd.DataFrame, right: pd.DataFrame, on: list[str] | str, how: str, name: str) -> pd.DataFrame:\n    # guard against accidental duplicate columns\n    overlap = set(left.columns) & set(right.columns)\n    if isinstance(on, str):\n        on_cols = {on}\n    else:\n        on_cols = set(on)\n    overlap = overlap - on_cols\n    if overlap:\n        raise ValueError(f"{name} merge would duplicate columns: {sorted(overlap)}")\n    return left.merge(right, on=on, how=how)\n\ndef check_ranges(df: pd.DataFrame, ranges: dict[str, tuple[float, float]]) -> pd.DataFrame:\n    rows = []\n    for col, (lo, hi) in ranges.items():\n        if col not in df.columns:\n            continue\n        bad = df[col].notna() & ((df[col] < lo) | (df[col] > hi))\n        rows.append({"col": col, "n_bad": int(bad.sum())})\n    return pd.DataFrame(rows)\n\n', 'execution_count': 4}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 13
[NbConvertApp] Skipping non-executing cell 14
[NbConvertApp] Executing cell:
# Target ICD codes (dotless, uppercase)
ICD10_CODES = ['J9602','J9612','J9622','J9692','E662']
ICD9_CODES  = ['27803']

SQL["cohort_icd_sql"] = f"""
-- ICD-based cohort flags per admission
WITH target_codes AS (
  SELECT 'J9602' AS code, 10 AS ver UNION ALL
  SELECT 'J9612', 10 UNION ALL
  SELECT 'J9622', 10 UNION ALL
  SELECT 'J9692', 10 UNION ALL
  SELECT 'E662',  10 UNION ALL
  SELECT '27803', 9
),

-- Hospital ICDs restricted to target codes
hosp_dx AS (
  SELECT
    d.subject_id,
    d.hadm_id,
    UPPER(REPLACE(d.icd_code, '.', '')) AS code_norm,
    d.icd_version
  FROM `{PHYS}.{HOSP}.diagnoses_icd` d
  JOIN target_codes t
    ON t.ver = d.icd_version AND t.code = UPPER(REPLACE(d.icd_code, '.', ''))
  WHERE d.hadm_id IS NOT NULL
),

-- Hospital flags per admission
hosp_flags AS (
  SELECT
    subject_id, hadm_id,
    MAX(IF(icd_version=10 AND code_norm='J9602',1,0)) AS ICD10_J9602,
    MAX(IF(icd_version=10 AND code_norm='J9612',1,0)) AS ICD10_J9612,
    MAX(IF(icd_version=10 AND code_norm='J9622',1,0)) AS ICD10_J9622,
    MAX(IF(icd_version=10 AND code_norm='J9692',1,0)) AS ICD10_J9692,
    MAX(IF(icd_version=10 AND code_norm='E662', 1,0)) AS ICD10_E662,
    MAX(IF(icd_version=9  AND code_norm='27803',1,0)) AS ICD9_27803
  FROM hosp_dx
  GROUP BY subject_id, hadm_id
),

-- ED ICDs restricted to target codes (map to hadm via edstays)
ed_dx AS (
  SELECT
    s.subject_id,
    s.hadm_id,
    s.stay_id,
    s.intime AS ed_intime,
    UPPER(REPLACE(d.icd_code, '.', '')) AS code_norm,
    d.icd_version
  FROM `{PHYS}.{ED}.diagnosis` d
  JOIN `{PHYS}.{ED}.edstays` s
    ON s.subject_id = d.subject_id AND s.stay_id = d.stay_id
  JOIN target_codes t
    ON t.ver = d.icd_version AND t.code = UPPER(REPLACE(d.icd_code, '.', ''))
  WHERE s.hadm_id IS NOT NULL
),

-- ED flags per ED stay (so we can both: OR flags across stays and also pick earliest stay_id)
ed_flags_by_stay AS (
  SELECT
    subject_id, hadm_id, stay_id, MIN(ed_intime) AS ed_intime,
    MAX(IF(icd_version=10 AND code_norm='J9602',1,0)) AS ICD10_J9602,
    MAX(IF(icd_version=10 AND code_norm='J9612',1,0)) AS ICD10_J9612,
    MAX(IF(icd_version=10 AND code_norm='J9622',1,0)) AS ICD10_J9622,
    MAX(IF(icd_version=10 AND code_norm='J9692',1,0)) AS ICD10_J9692,
    MAX(IF(icd_version=10 AND code_norm='E662', 1,0)) AS ICD10_E662,
    MAX(IF(icd_version=9  AND code_norm='27803',1,0)) AS ICD9_27803
  FROM ed_dx
  GROUP BY subject_id, hadm_id, stay_id
),

-- OR the ED flags across all ED stays mapped to the same hadm
ed_flags_or AS (
  SELECT
    subject_id, hadm_id,
    MAX(ICD10_J9602) AS ICD10_J9602,
    MAX(ICD10_J9612) AS ICD10_J9612,
    MAX(ICD10_J9622) AS ICD10_J9622,
    MAX(ICD10_J9692) AS ICD10_J9692,
    MAX(ICD10_E662 ) AS ICD10_E662,
    MAX(ICD9_27803) AS ICD9_27803
  FROM ed_flags_by_stay
  GROUP BY subject_id, hadm_id
),

-- Earliest ED stay_id per hadm (NO UNNEST of aggregates; use [OFFSET(0)])
ed_earliest AS (
  SELECT
    subject_id,
    hadm_id,
    (ARRAY_AGG(STRUCT(stay_id, ed_intime) ORDER BY ed_intime LIMIT 1))[OFFSET(0)].stay_id AS stay_id
  FROM ed_flags_by_stay
  GROUP BY subject_id, hadm_id
),

-- Bring flags and earliest stay_id together
ed_by_hadm AS (
  SELECT
    f.subject_id,
    f.hadm_id,
    e.stay_id,
    f.ICD10_J9602,
    f.ICD10_J9612,
    f.ICD10_J9622,
    f.ICD10_J9692,
    f.ICD10_E662,
    f.ICD9_27803
  FROM ed_flags_or f
  LEFT JOIN ed_earliest e
    USING (subject_id, hadm_id)
),

-- Combine ED and hospital flags at the admission level
combined AS (
  SELECT
    COALESCE(h.subject_id, e.subject_id) AS subject_id,
    COALESCE(h.hadm_id,     e.hadm_id)   AS hadm_id,
    GREATEST(IFNULL(h.ICD10_J9602,0), IFNULL(e.ICD10_J9602,0)) AS ICD10_J9602,
    GREATEST(IFNULL(h.ICD10_J9612,0), IFNULL(e.ICD10_J9612,0)) AS ICD10_J9612,
    GREATEST(IFNULL(h.ICD10_J9622,0), IFNULL(e.ICD10_J9622,0)) AS ICD10_J9622,
    GREATEST(IFNULL(h.ICD10_J9692,0), IFNULL(e.ICD10_J9692,0)) AS ICD10_J9692,
    GREATEST(IFNULL(h.ICD10_E662 ,0), IFNULL(e.ICD10_E662 ,0)) AS ICD10_E662,
    GREATEST(IFNULL(h.ICD9_27803,0), IFNULL(e.ICD9_27803,0)) AS ICD9_27803,
    IF((IFNULL(h.ICD10_J9602,0)+IFNULL(h.ICD10_J9612,0)+IFNULL(h.ICD10_J9622,0)+IFNULL(h.ICD10_J9692,0)+IFNULL(h.ICD10_E662,0)+IFNULL(h.ICD9_27803,0)) > 0, 1, 0) AS any_hypercap_icd_hosp,
    IF((IFNULL(e.ICD10_J9602,0)+IFNULL(e.ICD10_J9612,0)+IFNULL(e.ICD10_J9622,0)+IFNULL(e.ICD10_J9692,0)+IFNULL(e.ICD10_E662,0)+IFNULL(e.ICD9_27803,0)) > 0, 1, 0) AS any_hypercap_icd_ed
  FROM hosp_flags h
  FULL OUTER JOIN ed_by_hadm e
    ON h.hadm_id = e.hadm_id
)

SELECT
  subject_id, hadm_id,
  ICD10_J9602, ICD10_J9612, ICD10_J9622, ICD10_J9692, ICD10_E662, ICD9_27803,
  IF((ICD10_J9602+ICD10_J9612+ICD10_J9622+ICD10_J9692+ICD10_E662+ICD9_27803) > 0, 1, 0) AS any_hypercap_icd,
  any_hypercap_icd_hosp,
  any_hypercap_icd_ed,
  CASE
    WHEN any_hypercap_icd_hosp=1 AND any_hypercap_icd_ed=1 THEN 'ED+HOSP'
    WHEN any_hypercap_icd_ed=1 THEN 'ED'
    WHEN any_hypercap_icd_hosp=1 THEN 'HOSP'
    ELSE 'NONE'
  END AS icd_source
FROM combined
"""

cohort_icd = run_sql_bq(sql("cohort_icd_sql"))
print("ICD cohort admissions:", len(cohort_icd))
cohort_icd.head(3)


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Target ICD codes (dotless, uppercase)\nICD10_CODES = [\'J9602\',\'J9612\',\'J9622\',\'J9692\',\'E662\']\nICD9_CODES  = [\'27803\']\n\nSQL["cohort_icd_sql"] = f"""\n-- ICD-based cohort flags per admission\nWITH target_codes AS (\n  SELECT \'J9602\' AS code, 10 AS ver UNION ALL\n  SELECT \'J9612\', 10 UNION ALL\n  SELECT \'J9622\', 10 UNION ALL\n  SELECT \'J9692\', 10 UNION ALL\n  SELECT \'E662\',  10 UNION ALL\n  SELECT \'27803\', 9\n),\n\n-- Hospital ICDs restricted to target codes\nhosp_dx AS (\n  SELECT\n    d.subject_id,\n    d.hadm_id,\n    UPPER(REPLACE(d.icd_code, \'.\', \'\')) AS code_norm,\n    d.icd_version\n  FROM `{PHYS}.{HOSP}.diagnoses_icd` d\n  JOIN target_codes t\n    ON t.ver = d.icd_version AND t.code = UPPER(REPLACE(d.icd_code, \'.\', \'\'))\n  WHERE d.hadm_id IS NOT NULL\n),\n\n-- Hospital flags per admission\nhosp_flags AS (\n  SELECT\n    subject_id, hadm_id,\n    MAX(IF(icd_version=10 AND code_norm=\'J9602\',1,0)) AS ICD10_J9602,\n    MAX(IF(icd_version=10 AND code_norm=\'J9612\',1,0)) AS ICD10_J9612,\n    MAX(IF(icd_version=10 AND code_norm=\'J9622\',1,0)) AS ICD10_J9622,\n    MAX(IF(icd_version=10 AND code_norm=\'J9692\',1,0)) AS ICD10_J9692,\n    MAX(IF(icd_version=10 AND code_norm=\'E662\', 1,0)) AS ICD10_E662,\n    MAX(IF(icd_version=9  AND code_norm=\'27803\',1,0)) AS ICD9_27803\n  FROM hosp_dx\n  GROUP BY subject_id, hadm_id\n),\n\n-- ED ICDs restricted to target codes (map to hadm via edstays)\ned_dx AS (\n  SELECT\n    s.subject_id,\n    s.hadm_id,\n    s.stay_id,\n    s.intime AS ed_intime,\n    UPPER(REPLACE(d.icd_code, \'.\', \'\')) AS code_norm,\n    d.icd_version\n  FROM `{PHYS}.{ED}.diagnosis` d\n  JOIN `{PHYS}.{ED}.edstays` s\n    ON s.subject_id = d.subject_id AND s.stay_id = d.stay_id\n  JOIN target_codes t\n    ON t.ver = d.icd_version AND t.code = UPPER(REPLACE(d.icd_code, \'.\', \'\'))\n  WHERE s.hadm_id IS NOT NULL\n),\n\n-- ED flags per ED stay (so we can both: OR flags across stays and also pick earliest stay_id)\ned_flags_by_stay AS (\n  SELECT\n    subject_id, hadm_id, stay_id, MIN(ed_intime) AS ed_intime,\n    MAX(IF(icd_version=10 AND code_norm=\'J9602\',1,0)) AS ICD10_J9602,\n    MAX(IF(icd_version=10 AND code_norm=\'J9612\',1,0)) AS ICD10_J9612,\n    MAX(IF(icd_version=10 AND code_norm=\'J9622\',1,0)) AS ICD10_J9622,\n    MAX(IF(icd_version=10 AND code_norm=\'J9692\',1,0)) AS ICD10_J9692,\n    MAX(IF(icd_version=10 AND code_norm=\'E662\', 1,0)) AS ICD10_E662,\n    MAX(IF(icd_version=9  AND code_norm=\'27803\',1,0)) AS ICD9_27803\n  FROM ed_dx\n  GROUP BY subject_id, hadm_id, stay_id\n),\n\n-- OR the ED flags across all ED stays mapped to the same hadm\ned_flags_or AS (\n  SELECT\n    subject_id, hadm_id,\n    MAX(ICD10_J9602) AS ICD10_J9602,\n    MAX(ICD10_J9612) AS ICD10_J9612,\n    MAX(ICD10_J9622) AS ICD10_J9622,\n    MAX(ICD10_J9692) AS ICD10_J9692,\n    MAX(ICD10_E662 ) AS ICD10_E662,\n    MAX(ICD9_27803) AS ICD9_27803\n  FROM ed_flags_by_stay\n  GROUP BY subject_id, hadm_id\n),\n\n-- Earliest ED stay_id per hadm (NO UNNEST of aggregates; use [OFFSET(0)])\ned_earliest AS (\n  SELECT\n    subject_id,\n    hadm_id,\n    (ARRAY_AGG(STRUCT(stay_id, ed_intime) ORDER BY ed_intime LIMIT 1))[OFFSET(0)].stay_id AS stay_id\n  FROM ed_flags_by_stay\n  GROUP BY subject_id, hadm_id\n),\n\n-- Bring flags and earliest stay_id together\ned_by_hadm AS (\n  SELECT\n    f.subject_id,\n    f.hadm_id,\n    e.stay_id,\n    f.ICD10_J9602,\n    f.ICD10_J9612,\n    f.ICD10_J9622,\n    f.ICD10_J9692,\n    f.ICD10_E662,\n    f.ICD9_27803\n  FROM ed_flags_or f\n  LEFT JOIN ed_earliest e\n    USING (subject_id, hadm_id)\n),\n\n-- Combine ED and hospital flags at the admission level\ncombined AS (\n  SELECT\n    COALESCE(h.subject_id, e.subject_id) AS subject_id,\n    COALESCE(h.hadm_id,     e.hadm_id)   AS hadm_id,\n    GREATEST(IFNULL(h.ICD10_J9602,0), IFNULL(e.ICD10_J9602,0)) AS ICD10_J9602,\n    GREATEST(IFNULL(h.ICD10_J9612,0), IFNULL(e.ICD10_J9612,0)) AS ICD10_J9612,\n    GREATEST(IFNULL(h.ICD10_J9622,0), IFNULL(e.ICD10_J9622,0)) AS ICD10_J9622,\n    GREATEST(IFNULL(h.ICD10_J9692,0), IFNULL(e.ICD10_J9692,0)) AS ICD10_J9692,\n    GREATEST(IFNULL(h.ICD10_E662 ,0), IFNULL(e.ICD10_E662 ,0)) AS ICD10_E662,\n    GREATEST(IFNULL(h.ICD9_27803,0), IFNULL(e.ICD9_27803,0)) AS ICD9_27803,\n    IF((IFNULL(h.ICD10_J9602,0)+IFNULL(h.ICD10_J9612,0)+IFNULL(h.ICD10_J9622,0)+IFNULL(h.ICD10_J9692,0)+IFNULL(h.ICD10_E662,0)+IFNULL(h.ICD9_27803,0)) > 0, 1, 0) AS any_hypercap_icd_hosp,\n    IF((IFNULL(e.ICD10_J9602,0)+IFNULL(e.ICD10_J9612,0)+IFNULL(e.ICD10_J9622,0)+IFNULL(e.ICD10_J9692,0)+IFNULL(e.ICD10_E662,0)+IFNULL(e.ICD9_27803,0)) > 0, 1, 0) AS any_hypercap_icd_ed\n  FROM hosp_flags h\n  FULL OUTER JOIN ed_by_hadm e\n    ON h.hadm_id = e.hadm_id\n)\n\nSELECT\n  subject_id, hadm_id,\n  ICD10_J9602, ICD10_J9612, ICD10_J9622, ICD10_J9692, ICD10_E662, ICD9_27803,\n  IF((ICD10_J9602+ICD10_J9612+ICD10_J9622+ICD10_J9692+ICD10_E662+ICD9_27803) > 0, 1, 0) AS any_hypercap_icd,\n  any_hypercap_icd_hosp,\n  any_hypercap_icd_ed,\n  CASE\n    WHEN any_hypercap_icd_hosp=1 AND any_hypercap_icd_ed=1 THEN \'ED+HOSP\'\n    WHEN any_hypercap_icd_ed=1 THEN \'ED\'\n    WHEN any_hypercap_icd_hosp=1 THEN \'HOSP\'\n    ELSE \'NONE\'\n  END AS icd_source\nFROM combined\n"""\n\ncohort_icd = run_sql_bq(sql("cohort_icd_sql"))\nprint("ICD cohort admissions:", len(cohort_icd))\ncohort_icd.head(3)\n\n', 'execution_count': 5}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ICD cohort admissions: 4237\n'}
[NbConvertApp] msg_type: execute_result
[NbConvertApp] content: {'data': {'text/plain': '   subject_id   hadm_id  ICD10_J9602  ICD10_J9612  ICD10_J9622  ICD10_J9692  \\\n0    10485425  21207827            0            0            1            0   \n1    16826447  21830147            0            1            0            0   \n2    11482582  22869017            0            0            0            0   \n\n   ICD10_E662  ICD9_27803  any_hypercap_icd  any_hypercap_icd_hosp  \\\n0           1           0                 1                      1   \n1           1           0                 1                      1   \n2           0           1                 1                      1   \n\n   any_hypercap_icd_ed icd_source  \n0                    0       HOSP  \n1                    0       HOSP  \n2                    0       HOSP  ', 'text/html': '<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>subject_id</th>\n      <th>hadm_id</th>\n      <th>ICD10_J9602</th>\n      <th>ICD10_J9612</th>\n      <th>ICD10_J9622</th>\n      <th>ICD10_J9692</th>\n      <th>ICD10_E662</th>\n      <th>ICD9_27803</th>\n      <th>any_hypercap_icd</th>\n      <th>any_hypercap_icd_hosp</th>\n      <th>any_hypercap_icd_ed</th>\n      <th>icd_source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10485425</td>\n      <td>21207827</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>HOSP</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>16826447</td>\n      <td>21830147</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>HOSP</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>11482582</td>\n      <td>22869017</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>HOSP</td>\n    </tr>\n  </tbody>\n</table>\n</div>'}, 'metadata': {}, 'execution_count': 5}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 16
[NbConvertApp] Skipping non-executing cell 17
[NbConvertApp] Executing cell:
SQL["co2_thresholds_sql"] = f"""
/* ---- LAB (HOSP) pCO2 across entire dataset ---- */
WITH hosp_cand AS (
  SELECT
    le.hadm_id, le.charttime, le.specimen_id,
    COALESCE(
      CAST(le.valuenum AS FLOAT64),
      SAFE_CAST(REGEXP_EXTRACT(LOWER(le.value), r'(-?\d+(?:\.\d+)?)') AS FLOAT64)
    ) AS val,
    LOWER(REPLACE(COALESCE(le.valueuom,''),' ','')) AS uom_nospace,
    LOWER(di.label) AS lbl,
    LOWER(COALESCE(di.fluid,'')) AS fl
  FROM `{PHYS}.{HOSP}.labevents` le
  JOIN `{PHYS}.{HOSP}.d_labitems` di ON di.itemid = le.itemid
  WHERE (le.valuenum IS NOT NULL OR le.value IS NOT NULL)
    AND (
      LOWER(COALESCE(di.category,'')) LIKE '%blood gas%' OR
      LOWER(di.label) LIKE '%pco2%' OR
      REGEXP_CONTAINS(LOWER(di.label), r'\bpa?\s*co(?:2|₂)\b')
    )
    AND NOT REGEXP_CONTAINS(LOWER(di.label),
        r'(et\s*co2|end[- ]?tidal|t\s*co2|tco2|total\s*co2|hco3|bicar)')
),
hosp_spec AS (
  SELECT le.specimen_id, LOWER(COALESCE(le.value,'')) AS spec_val
  FROM `{PHYS}.{HOSP}.labevents` le
  JOIN `{PHYS}.{HOSP}.d_labitems` di ON di.itemid = le.itemid
  WHERE le.specimen_id IS NOT NULL
    AND REGEXP_CONTAINS(LOWER(di.label), r'(specimen|sample)')
),
hosp_pco2 AS (
  SELECT
    c.hadm_id, c.charttime,
    CASE
      WHEN REGEXP_CONTAINS(s.spec_val, r'arter') OR REGEXP_CONTAINS(s.spec_val, r'\bart\b') THEN 'arterial'
      WHEN REGEXP_CONTAINS(s.spec_val, r'ven|mixed|central') THEN 'venous'
      WHEN REGEXP_CONTAINS(c.fl, r'arter') THEN 'arterial'
      WHEN REGEXP_CONTAINS(c.fl, r'ven') THEN 'venous'
      WHEN c.fl LIKE '%arterial%' OR REGEXP_CONTAINS(c.lbl, r'\b(abg|art|arterial|a[- ]?line)\b') THEN 'arterial'
      WHEN c.fl LIKE '%ven%'      OR REGEXP_CONTAINS(c.lbl, r'\b(vbg|ven|venous|mixed|central)\b') THEN 'venous'
      ELSE NULL
    END AS site,
    CASE WHEN c.uom_nospace='kpa' THEN c.val*7.50062 ELSE c.val END AS pco2_mmHg
  FROM hosp_cand c
  LEFT JOIN hosp_spec s USING (specimen_id)
  WHERE c.val IS NOT NULL
),
hosp_pco2_std AS (
  SELECT hadm_id, site, charttime, pco2_mmHg
  FROM hosp_pco2
  WHERE site IN ('arterial','venous') AND pco2_mmHg BETWEEN 5 AND 200
),

/* ---- ICU (POC) pCO2 across entire dataset ---- */
icu_raw AS (
  SELECT
    ie.hadm_id,
    ce.stay_id,
    ce.charttime,
    LOWER(di.label) AS lbl,
    LOWER(REPLACE(COALESCE(ce.valueuom,''),' ','')) AS uom_nospace,
    LOWER(COALESCE(ce.value,'')) AS valstr,
    COALESCE(
      CAST(ce.valuenum AS FLOAT64),
      SAFE_CAST(REGEXP_EXTRACT(LOWER(ce.value), r'(-?\d+(?:\.\d+)?)') AS FLOAT64)
    ) AS val
  FROM `{PHYS}.{ICU}.chartevents` ce
  JOIN `{PHYS}.{ICU}.d_items`  di ON di.itemid = ce.itemid
  JOIN `{PHYS}.{ICU}.icustays` ie ON ie.stay_id = ce.stay_id
),
icu_cand AS (
  SELECT
    hadm_id, stay_id, charttime, lbl, uom_nospace, valstr, val,
    CASE
      WHEN (
            REGEXP_CONTAINS(lbl, r'(^|[^a-z])p(?:a)?\s*co\s*(?:2|₂)([^a-z]|$)')
            OR uom_nospace IN ('mmhg','kpa')
            OR REGEXP_CONTAINS(valstr, r'\b(mm\s*hg|kpa)\b')
           )
           AND NOT REGEXP_CONTAINS(lbl,
               r'(et\s*co2|end[- ]?tidal|t\s*co2|tco2|total\s*co2|hco3|bicar|v\s*co2|vco2|co2\s*(prod|elimin|production|elimination))')
      THEN 'pco2'
      ELSE NULL
    END AS analyte,
    CASE
      WHEN REGEXP_CONTAINS(lbl, r'\b(abg|art|arterial|a[- ]?line)\b') THEN 'arterial'
      WHEN REGEXP_CONTAINS(lbl, r'\b(vbg|ven|venous|mixed|central)\b') THEN 'venous'
      ELSE NULL
    END AS site
  FROM icu_raw
  WHERE val IS NOT NULL
    AND (
      REGEXP_CONTAINS(lbl, r'(^|[^a-z])p(?:a)?\s*co\s*(?:2|₂)([^a-z]|$)')
      OR uom_nospace IN ('mmhg','kpa')
      OR REGEXP_CONTAINS(valstr, r'\b(mm\s*hg|kpa)\b')
    )
    AND NOT REGEXP_CONTAINS(lbl,
        r'(et\s*co2|end[- ]?tidal|t\s*co2|tco2|total\s*co2|hco3|bicar|v\s*co2|vco2|co2\s*(prod|elimin|production|elimination))')
),
icu_co2_std AS (
  SELECT
    hadm_id,
    site,
    charttime,
    CASE WHEN uom_nospace='kpa' OR REGEXP_CONTAINS(valstr, r'\bkpa\b') THEN val*7.50062 ELSE val END AS pco2_mmHg
  FROM icu_cand
  WHERE analyte='pco2' AND site IN ('arterial','venous') AND val BETWEEN 5 AND 200
),

/* ---- Combine and threshold per admission ---- */
all_pco2 AS (
  SELECT * FROM hosp_pco2_std
  UNION ALL
  SELECT * FROM icu_co2_std
),
thresh AS (
  SELECT
    hadm_id,
    MAX(IF(site='arterial' AND pco2_mmHg >= 45.0, 1, 0)) AS abg_hypercap_threshold,
    MAX(IF(site='venous'   AND pco2_mmHg >= 50.0, 1, 0)) AS vbg_hypercap_threshold
  FROM all_pco2
  GROUP BY hadm_id
)
SELECT * FROM thresh
"""

co2_thresh = run_sql_bq(sql("co2_thresholds_sql"))
print("Admissions meeting thresholds:", len(co2_thresh))
co2_thresh.head(3)


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': 'SQL["co2_thresholds_sql"] = f"""\n/* ---- LAB (HOSP) pCO2 across entire dataset ---- */\nWITH hosp_cand AS (\n  SELECT\n    le.hadm_id, le.charttime, le.specimen_id,\n    COALESCE(\n      CAST(le.valuenum AS FLOAT64),\n      SAFE_CAST(REGEXP_EXTRACT(LOWER(le.value), r\'(-?\\d+(?:\\.\\d+)?)\') AS FLOAT64)\n    ) AS val,\n    LOWER(REPLACE(COALESCE(le.valueuom,\'\'),\' \',\'\')) AS uom_nospace,\n    LOWER(di.label) AS lbl,\n    LOWER(COALESCE(di.fluid,\'\')) AS fl\n  FROM `{PHYS}.{HOSP}.labevents` le\n  JOIN `{PHYS}.{HOSP}.d_labitems` di ON di.itemid = le.itemid\n  WHERE (le.valuenum IS NOT NULL OR le.value IS NOT NULL)\n    AND (\n      LOWER(COALESCE(di.category,\'\')) LIKE \'%blood gas%\' OR\n      LOWER(di.label) LIKE \'%pco2%\' OR\n      REGEXP_CONTAINS(LOWER(di.label), r\'\\bpa?\\s*co(?:2|₂)\\b\')\n    )\n    AND NOT REGEXP_CONTAINS(LOWER(di.label),\n        r\'(et\\s*co2|end[- ]?tidal|t\\s*co2|tco2|total\\s*co2|hco3|bicar)\')\n),\nhosp_spec AS (\n  SELECT le.specimen_id, LOWER(COALESCE(le.value,\'\')) AS spec_val\n  FROM `{PHYS}.{HOSP}.labevents` le\n  JOIN `{PHYS}.{HOSP}.d_labitems` di ON di.itemid = le.itemid\n  WHERE le.specimen_id IS NOT NULL\n    AND REGEXP_CONTAINS(LOWER(di.label), r\'(specimen|sample)\')\n),\nhosp_pco2 AS (\n  SELECT\n    c.hadm_id, c.charttime,\n    CASE\n      WHEN REGEXP_CONTAINS(s.spec_val, r\'arter\') OR REGEXP_CONTAINS(s.spec_val, r\'\\bart\\b\') THEN \'arterial\'\n      WHEN REGEXP_CONTAINS(s.spec_val, r\'ven|mixed|central\') THEN \'venous\'\n      WHEN REGEXP_CONTAINS(c.fl, r\'arter\') THEN \'arterial\'\n      WHEN REGEXP_CONTAINS(c.fl, r\'ven\') THEN \'venous\'\n      WHEN c.fl LIKE \'%arterial%\' OR REGEXP_CONTAINS(c.lbl, r\'\\b(abg|art|arterial|a[- ]?line)\\b\') THEN \'arterial\'\n      WHEN c.fl LIKE \'%ven%\'      OR REGEXP_CONTAINS(c.lbl, r\'\\b(vbg|ven|venous|mixed|central)\\b\') THEN \'venous\'\n      ELSE NULL\n    END AS site,\n    CASE WHEN c.uom_nospace=\'kpa\' THEN c.val*7.50062 ELSE c.val END AS pco2_mmHg\n  FROM hosp_cand c\n  LEFT JOIN hosp_spec s USING (specimen_id)\n  WHERE c.val IS NOT NULL\n),\nhosp_pco2_std AS (\n  SELECT hadm_id, site, charttime, pco2_mmHg\n  FROM hosp_pco2\n  WHERE site IN (\'arterial\',\'venous\') AND pco2_mmHg BETWEEN 5 AND 200\n),\n\n/* ---- ICU (POC) pCO2 across entire dataset ---- */\nicu_raw AS (\n  SELECT\n    ie.hadm_id,\n    ce.stay_id,\n    ce.charttime,\n    LOWER(di.label) AS lbl,\n    LOWER(REPLACE(COALESCE(ce.valueuom,\'\'),\' \',\'\')) AS uom_nospace,\n    LOWER(COALESCE(ce.value,\'\')) AS valstr,\n    COALESCE(\n      CAST(ce.valuenum AS FLOAT64),\n      SAFE_CAST(REGEXP_EXTRACT(LOWER(ce.value), r\'(-?\\d+(?:\\.\\d+)?)\') AS FLOAT64)\n    ) AS val\n  FROM `{PHYS}.{ICU}.chartevents` ce\n  JOIN `{PHYS}.{ICU}.d_items`  di ON di.itemid = ce.itemid\n  JOIN `{PHYS}.{ICU}.icustays` ie ON ie.stay_id = ce.stay_id\n),\nicu_cand AS (\n  SELECT\n    hadm_id, stay_id, charttime, lbl, uom_nospace, valstr, val,\n    CASE\n      WHEN (\n            REGEXP_CONTAINS(lbl, r\'(^|[^a-z])p(?:a)?\\s*co\\s*(?:2|₂)([^a-z]|$)\')\n            OR uom_nospace IN (\'mmhg\',\'kpa\')\n            OR REGEXP_CONTAINS(valstr, r\'\\b(mm\\s*hg|kpa)\\b\')\n           )\n           AND NOT REGEXP_CONTAINS(lbl,\n               r\'(et\\s*co2|end[- ]?tidal|t\\s*co2|tco2|total\\s*co2|hco3|bicar|v\\s*co2|vco2|co2\\s*(prod|elimin|production|elimination))\')\n      THEN \'pco2\'\n      ELSE NULL\n    END AS analyte,\n    CASE\n      WHEN REGEXP_CONTAINS(lbl, r\'\\b(abg|art|arterial|a[- ]?line)\\b\') THEN \'arterial\'\n      WHEN REGEXP_CONTAINS(lbl, r\'\\b(vbg|ven|venous|mixed|central)\\b\') THEN \'venous\'\n      ELSE NULL\n    END AS site\n  FROM icu_raw\n  WHERE val IS NOT NULL\n    AND (\n      REGEXP_CONTAINS(lbl, r\'(^|[^a-z])p(?:a)?\\s*co\\s*(?:2|₂)([^a-z]|$)\')\n      OR uom_nospace IN (\'mmhg\',\'kpa\')\n      OR REGEXP_CONTAINS(valstr, r\'\\b(mm\\s*hg|kpa)\\b\')\n    )\n    AND NOT REGEXP_CONTAINS(lbl,\n        r\'(et\\s*co2|end[- ]?tidal|t\\s*co2|tco2|total\\s*co2|hco3|bicar|v\\s*co2|vco2|co2\\s*(prod|elimin|production|elimination))\')\n),\nicu_co2_std AS (\n  SELECT\n    hadm_id,\n    site,\n    charttime,\n    CASE WHEN uom_nospace=\'kpa\' OR REGEXP_CONTAINS(valstr, r\'\\bkpa\\b\') THEN val*7.50062 ELSE val END AS pco2_mmHg\n  FROM icu_cand\n  WHERE analyte=\'pco2\' AND site IN (\'arterial\',\'venous\') AND val BETWEEN 5 AND 200\n),\n\n/* ---- Combine and threshold per admission ---- */\nall_pco2 AS (\n  SELECT * FROM hosp_pco2_std\n  UNION ALL\n  SELECT * FROM icu_co2_std\n),\nthresh AS (\n  SELECT\n    hadm_id,\n    MAX(IF(site=\'arterial\' AND pco2_mmHg >= 45.0, 1, 0)) AS abg_hypercap_threshold,\n    MAX(IF(site=\'venous\'   AND pco2_mmHg >= 50.0, 1, 0)) AS vbg_hypercap_threshold\n  FROM all_pco2\n  GROUP BY hadm_id\n)\nSELECT * FROM thresh\n"""\n\nco2_thresh = run_sql_bq(sql("co2_thresholds_sql"))\nprint("Admissions meeting thresholds:", len(co2_thresh))\nco2_thresh.head(3)\n\n', 'execution_count': 6}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Admissions meeting thresholds: 52715\n'}
[NbConvertApp] msg_type: execute_result
[NbConvertApp] content: {'data': {'text/plain': '    hadm_id  abg_hypercap_threshold  vbg_hypercap_threshold\n0  26892251                       0                       0\n1  21943565                       0                       0\n2  27109430                       0                       0', 'text/html': '<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>hadm_id</th>\n      <th>abg_hypercap_threshold</th>\n      <th>vbg_hypercap_threshold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>26892251</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>21943565</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>27109430</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>'}, 'metadata': {}, 'execution_count': 6}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 19
[NbConvertApp] Skipping non-executing cell 20
[NbConvertApp] Executing cell:
# Outer-join because thresholds can identify hadm_id with no ICD codes and vice versa
cohort_any = cohort_icd.merge(co2_thresh, how="outer", on="hadm_id")

# Fill missing flags with 0 where appropriate
icd_cols = ["ICD10_J9602","ICD10_J9612","ICD10_J9622","ICD10_J9692","ICD10_E662","ICD9_27803","any_hypercap_icd","any_hypercap_icd_hosp","any_hypercap_icd_ed"]
for c in icd_cols:
    if c in cohort_any.columns:
        cohort_any[c] = cohort_any[c].fillna(0).astype(int)

for c in ["abg_hypercap_threshold","vbg_hypercap_threshold"]:
    if c in cohort_any.columns:
        cohort_any[c] = cohort_any[c].fillna(0).astype(int)

# Final enrollment flag
cohort_any["pco2_threshold_any"] = ((cohort_any["abg_hypercap_threshold"]==1) | (cohort_any["vbg_hypercap_threshold"]==1)).astype(int)
cohort_any["enrolled_any"] = ((cohort_any["any_hypercap_icd"]==1) | (cohort_any["pco2_threshold_any"]==1)).astype(int)

print("ICD-only admissions        :", int((cohort_any["any_hypercap_icd"]==1).sum()))
print("Threshold-only admissions  :", int(((cohort_any["pco2_threshold_any"]==1) & (cohort_any["any_hypercap_icd"]==0)).sum()))
print("Both ICD and threshold     :", int(((cohort_any["pco2_threshold_any"]==1) & (cohort_any["any_hypercap_icd"]==1)).sum()))
print("Total enrolled (union)     :", int((cohort_any["enrolled_any"]==1).sum()))

# New hadm list used for the rest of the notebook
hadm_list = cohort_any.loc[cohort_any["enrolled_any"]==1, "hadm_id"].dropna().astype("int64").tolist()
len(hadm_list)


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Outer-join because thresholds can identify hadm_id with no ICD codes and vice versa\ncohort_any = cohort_icd.merge(co2_thresh, how="outer", on="hadm_id")\n\n# Fill missing flags with 0 where appropriate\nicd_cols = ["ICD10_J9602","ICD10_J9612","ICD10_J9622","ICD10_J9692","ICD10_E662","ICD9_27803","any_hypercap_icd","any_hypercap_icd_hosp","any_hypercap_icd_ed"]\nfor c in icd_cols:\n    if c in cohort_any.columns:\n        cohort_any[c] = cohort_any[c].fillna(0).astype(int)\n\nfor c in ["abg_hypercap_threshold","vbg_hypercap_threshold"]:\n    if c in cohort_any.columns:\n        cohort_any[c] = cohort_any[c].fillna(0).astype(int)\n\n# Final enrollment flag\ncohort_any["pco2_threshold_any"] = ((cohort_any["abg_hypercap_threshold"]==1) | (cohort_any["vbg_hypercap_threshold"]==1)).astype(int)\ncohort_any["enrolled_any"] = ((cohort_any["any_hypercap_icd"]==1) | (cohort_any["pco2_threshold_any"]==1)).astype(int)\n\nprint("ICD-only admissions        :", int((cohort_any["any_hypercap_icd"]==1).sum()))\nprint("Threshold-only admissions  :", int(((cohort_any["pco2_threshold_any"]==1) & (cohort_any["any_hypercap_icd"]==0)).sum()))\nprint("Both ICD and threshold     :", int(((cohort_any["pco2_threshold_any"]==1) & (cohort_any["any_hypercap_icd"]==1)).sum()))\nprint("Total enrolled (union)     :", int((cohort_any["enrolled_any"]==1).sum()))\n\n# New hadm list used for the rest of the notebook\nhadm_list = cohort_any.loc[cohort_any["enrolled_any"]==1, "hadm_id"].dropna().astype("int64").tolist()\nlen(hadm_list)\n\n', 'execution_count': 7}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ICD-only admissions        : 4237\nThreshold-only admissions  : 35915\nBoth ICD and threshold     : 3102\nTotal enrolled (union)     : 40152\n'}
[NbConvertApp] msg_type: execute_result
[NbConvertApp] content: {'data': {'text/plain': '40151'}, 'metadata': {}, 'execution_count': 7}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 22
[NbConvertApp] Skipping non-executing cell 23
[NbConvertApp] Executing cell:
params = {"hadms": hadm_list}

bg_pairs_sql = rf"""
WITH hadms AS (SELECT hadm_id FROM UNNEST(@hadms) AS hadm_id),

/* ---------------- LAB (HOSP) ---------------- */
hosp_cand AS (
  SELECT
    le.subject_id, le.hadm_id, le.charttime, le.specimen_id,
    CAST(le.valuenum AS FLOAT64) AS val,
    LOWER(COALESCE(le.valueuom,'')) AS uom,
    LOWER(di.label) AS lbl,
    LOWER(COALESCE(di.fluid,'')) AS fl,
    LOWER(COALESCE(di.category,'')) AS cat
  FROM `{PHYS}.{HOSP}.labevents`  le
  JOIN `{PHYS}.{HOSP}.d_labitems` di ON di.itemid = le.itemid
  JOIN hadms h ON h.hadm_id = le.hadm_id
  WHERE le.valuenum IS NOT NULL
    AND (
         LOWER(COALESCE(di.category,'')) LIKE '%blood gas%' OR
         LOWER(di.label) LIKE '%pco2%' OR
         REGEXP_CONTAINS(LOWER(di.label), r'\bph\b') OR
         REGEXP_CONTAINS(LOWER(di.label), r'\bpa?\s*co(?:2|₂)\b')
        )
    AND NOT REGEXP_CONTAINS(LOWER(di.label), r'(et\s*co2|end[- ]?tidal|t\s*co2|tco2|total\s*co2|hco3|bicar)')
),
hosp_spec AS (
  SELECT le.specimen_id, LOWER(COALESCE(le.value,'')) AS spec_val
  FROM `{PHYS}.{HOSP}.labevents` le
  JOIN `{PHYS}.{HOSP}.d_labitems` di ON di.itemid = le.itemid
  WHERE le.specimen_id IS NOT NULL
    AND REGEXP_CONTAINS(LOWER(di.label), r'(specimen|sample)')
),
hosp_class AS (
  SELECT
    c.hadm_id, c.charttime, c.specimen_id, c.val, c.uom, c.lbl, c.fl,
    CASE
      WHEN REGEXP_CONTAINS(c.lbl, r'\b(?:blood\s*)?ph\b') THEN 'ph'
      WHEN (c.lbl LIKE '%pco2%' OR REGEXP_CONTAINS(c.lbl, r'\bpa?\s*co(?:2|₂)\b')) THEN 'pco2'
      ELSE NULL
    END AS analyte,
    CASE
      WHEN REGEXP_CONTAINS(s.spec_val, r'arter') OR REGEXP_CONTAINS(s.spec_val, r'\bart\b') THEN 'arterial'
      WHEN REGEXP_CONTAINS(s.spec_val, r'ven|mixed|central') THEN 'venous'
      WHEN c.fl LIKE '%arterial%' OR REGEXP_CONTAINS(c.lbl, r'\b(abg|art|arterial|a[- ]?line)\b') THEN 'arterial'
      WHEN c.fl LIKE '%ven%'      OR REGEXP_CONTAINS(c.lbl, r'\b(vbg|ven|venous|mixed|central)\b') THEN 'venous'
      ELSE NULL
    END AS site
  FROM hosp_cand c
  LEFT JOIN hosp_spec s USING (specimen_id)
),
hosp_pairs AS (
  SELECT
    hadm_id, specimen_id,
    MIN(charttime) AS sample_time,
    MAX(IF(analyte='ph',   val, NULL)) AS ph,
    MAX(IF(analyte='pco2', val, NULL)) AS pco2_raw,
    (ARRAY_AGG(IF(analyte='pco2', uom, NULL) IGNORE NULLS LIMIT 1))[OFFSET(0)] AS pco2_uom,
    (ARRAY_AGG(IF(analyte='ph',   uom, NULL) IGNORE NULLS LIMIT 1))[OFFSET(0)] AS ph_uom,
    (ARRAY_AGG(site IGNORE NULLS LIMIT 1))[OFFSET(0)] AS site
  FROM hosp_class
  GROUP BY hadm_id, specimen_id
  HAVING (ph IS NOT NULL OR pco2_raw IS NOT NULL) AND site IN ('arterial','venous')
),
hosp_pairs_std AS (
  SELECT
    hadm_id, specimen_id, sample_time, site,
    ph, ph_uom,
    CASE WHEN pco2_uom = 'kpa' THEN pco2_raw * 7.50062 ELSE pco2_raw END AS pco2_mmHg,
    'mmhg' AS pco2_uom_norm
  FROM hosp_pairs
  WHERE (ph IS NULL OR (ph BETWEEN 6.3 AND 7.8))
    AND (pco2_raw IS NULL OR (CASE WHEN pco2_uom='kpa' THEN pco2_raw*7.50062 ELSE pco2_raw END) BETWEEN 5 AND 200)
),
lab_abg AS (
  SELECT hadm_id,
         ph            AS lab_abg_ph,
         ph_uom        AS lab_abg_ph_uom,
         pco2_mmHg     AS lab_abg_paco2,
         'mmhg'        AS lab_abg_paco2_uom,
         sample_time   AS lab_abg_time
  FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY hadm_id ORDER BY sample_time) rn
        FROM hosp_pairs_std WHERE site='arterial') WHERE rn=1
),
lab_vbg AS (
  SELECT hadm_id,
         ph            AS lab_vbg_ph,
         ph_uom        AS lab_vbg_ph_uom,
         pco2_mmHg     AS lab_vbg_paco2,
         'mmhg'        AS lab_vbg_paco2_uom,
         sample_time   AS lab_vbg_time
  FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY hadm_id ORDER BY sample_time) rn
        FROM hosp_pairs_std WHERE site='venous') WHERE rn=1
),

/* ---------------- POC (ICU) ---------------- */
icu_raw AS (
  SELECT
    ie.hadm_id, ce.stay_id, ce.charttime,
    LOWER(di.label) AS lbl,
    LOWER(REPLACE(COALESCE(ce.valueuom,''),' ','')) AS uom_nospace,
    LOWER(COALESCE(ce.value,'')) AS valstr,
    COALESCE(
      CAST(ce.valuenum AS FLOAT64),
      SAFE_CAST(REGEXP_EXTRACT(LOWER(ce.value), r'(-?\d+(?:\.\d+)?)') AS FLOAT64)
    ) AS val
  FROM `{PHYS}.{ICU}.chartevents` ce
  JOIN `{PHYS}.{ICU}.d_items`  di ON di.itemid = ce.itemid
  JOIN `{PHYS}.{ICU}.icustays` ie ON ie.stay_id = ce.stay_id
  JOIN hadms h ON h.hadm_id = ie.hadm_id
),
icu_cand AS (
  SELECT
    hadm_id, stay_id, charttime, lbl, uom_nospace, valstr, val,
    CASE
      WHEN REGEXP_CONTAINS(lbl, r'(^|[^a-z])ph([^a-z]|$)') OR (val BETWEEN 6.3 AND 7.8) THEN 'ph'
      WHEN (
             REGEXP_CONTAINS(lbl, r'(^|[^a-z])p(?:a)?\s*co\s*(?:2|₂)([^a-z]|$)')
             OR uom_nospace IN ('mmhg','kpa')
             OR REGEXP_CONTAINS(valstr, r'\b(mm\s*hg|kpa)\b')
           )
           AND NOT REGEXP_CONTAINS(lbl, r'(et\s*co2|end[- ]?tidal|t\s*co2|tco2|total\s*co2|hco3|bicar|v\s*co2|vco2|co2\s*(prod|elimin|production|elimination))')
      THEN 'pco2'
      ELSE NULL
    END AS analyte,
    CASE
      WHEN REGEXP_CONTAINS(lbl, r'\b(abg|art|arterial|a[- ]?line)\b') THEN 'arterial'
      WHEN REGEXP_CONTAINS(lbl, r'\b(vbg|ven|venous|mixed|central)\b') THEN 'venous'
      ELSE NULL
    END AS site
  FROM icu_raw
  WHERE val IS NOT NULL
    AND (
      REGEXP_CONTAINS(lbl, r'(^|[^a-z])ph([^a-z]|$)') OR
      REGEXP_CONTAINS(lbl, r'(^|[^a-z])p(?:a)?\s*co\s*(?:2|₂)([^a-z]|$)') OR
      uom_nospace IN ('mmhg','kpa') OR
      REGEXP_CONTAINS(valstr, r'\b(mm\s*hg|kpa)\b')
    )
    AND NOT REGEXP_CONTAINS(lbl, r'(et\s*co2|end[- ]?tidal|t\s*co2|tco2|total\s*co2|hco3|bicar|v\s*co2|vco2|co2\s*(prod|elimin|production|elimination))')
),
icu_ph AS (
  SELECT hadm_id, stay_id, charttime, val AS ph, site AS site_ph
  FROM icu_cand WHERE analyte='ph'
),
icu_co2 AS (
  SELECT hadm_id, stay_id, charttime, val AS pco2_raw, uom_nospace, valstr, site AS site_co2
  FROM icu_cand WHERE analyte='pco2'
),
icu_pair_win AS (
  SELECT
    p.hadm_id, p.stay_id,
    COALESCE(p.site_ph, c.site_co2) AS site,
    p.charttime AS ph_time, c.charttime AS co2_time,
    p.ph,
    CASE
      WHEN c.uom_nospace='kpa' OR REGEXP_CONTAINS(c.valstr, r'\bkpa\b') THEN 'kpa'
      WHEN c.uom_nospace='mmhg' OR REGEXP_CONTAINS(c.valstr, r'mm\s*hg') THEN 'mmhg'
      ELSE c.uom_nospace
    END AS pco2_uom_norm_raw,
    c.pco2_raw,
    ABS(TIMESTAMP_DIFF(c.charttime, p.charttime, SECOND)) AS dt_sec
  FROM icu_ph p
  JOIN icu_co2 c
    ON c.hadm_id = p.hadm_id
   AND c.stay_id = p.stay_id
   AND (COALESCE(p.site_ph, c.site_co2) IN ('arterial','venous'))
   AND ABS(TIMESTAMP_DIFF(c.charttime, p.charttime, MINUTE)) <= 10
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY p.hadm_id, p.stay_id, p.charttime
    ORDER BY dt_sec
  ) = 1
),
icu_pairs_std AS (
  SELECT
    hadm_id, stay_id, site,
    LEAST(ph_time, co2_time) AS sample_time,
    ph,
    CAST(NULL AS STRING) AS ph_uom,              -- POC pH is unitless/null
    CASE WHEN pco2_uom_norm_raw='kpa' THEN pco2_raw*7.50062 ELSE pco2_raw END AS pco2_mmHg,
    'mmhg' AS pco2_uom_norm
  FROM icu_pair_win
  WHERE (ph BETWEEN 6.3 AND 7.8 OR ph IS NULL)
    AND (CASE WHEN pco2_uom_norm_raw='kpa' THEN pco2_raw*7.50062 ELSE pco2_raw END) BETWEEN 5 AND 200
),
icu_solo_pco2_std AS (
  SELECT
    hadm_id, stay_id, site_co2 AS site,
    charttime AS sample_time,
    CAST(NULL AS FLOAT64) AS ph,
    CAST(NULL AS STRING)  AS ph_uom,            -- no pH here
    CASE WHEN uom_nospace='kpa' OR REGEXP_CONTAINS(valstr, r'\bkpa\b') THEN pco2_raw*7.50062 ELSE pco2_raw END AS pco2_mmHg,
    'mmhg' AS pco2_uom_norm
  FROM icu_co2
  WHERE site_co2 IN ('arterial','venous')
    AND pco2_raw BETWEEN 5 AND 200
),
icu_all AS (
  SELECT * FROM icu_pairs_std
  UNION ALL
  SELECT * FROM icu_solo_pco2_std
),

poc_abg AS (
  SELECT hadm_id,
         ph            AS poc_abg_ph,
         ph_uom        AS poc_abg_ph_uom,
         pco2_mmHg     AS poc_abg_paco2,
         'mmhg'        AS poc_abg_paco2_uom,
         sample_time   AS poc_abg_time
  FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY hadm_id ORDER BY sample_time) rn
        FROM icu_all WHERE site='arterial') WHERE rn=1
),
poc_vbg AS (
  SELECT hadm_id,
         ph            AS poc_vbg_ph,
         ph_uom        AS poc_vbg_ph_uom,
         pco2_mmHg     AS poc_vbg_paco2,
         'mmhg'        AS poc_vbg_paco2_uom,
         sample_time   AS poc_vbg_time
  FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY hadm_id ORDER BY sample_time) rn
        FROM icu_all WHERE site='venous') WHERE rn=1
)

/* ---------------- Final one row per hadm ---------------- */
SELECT
  h.hadm_id,
  -- LAB-ABG / LAB-VBG
  la.lab_abg_ph, la.lab_abg_ph_uom, la.lab_abg_paco2, la.lab_abg_paco2_uom, la.lab_abg_time,
  lv.lab_vbg_ph, lv.lab_vbg_ph_uom, lv.lab_vbg_paco2, lv.lab_vbg_paco2_uom, lv.lab_vbg_time,
  -- POC-ABG / POC-VBG
  pa.poc_abg_ph, pa.poc_abg_ph_uom, pa.poc_abg_paco2, pa.poc_abg_paco2_uom, pa.poc_abg_time,
  pv.poc_vbg_ph, pv.poc_vbg_ph_uom, pv.poc_vbg_paco2, pv.poc_vbg_paco2_uom, pv.poc_vbg_time,
  -- First ABG across LAB+POC
  (SELECT AS STRUCT src, t, ph, pco2
   FROM (SELECT 'LAB' AS src, la.lab_abg_time AS t, la.lab_abg_ph AS ph, la.lab_abg_paco2 AS pco2
         UNION ALL
         SELECT 'POC', pa.poc_abg_time, pa.poc_abg_ph, pa.poc_abg_paco2)
   WHERE t IS NOT NULL
   ORDER BY t LIMIT 1) AS first_abg,
  -- First VBG across LAB+POC
  (SELECT AS STRUCT src, t, ph, pco2
   FROM (SELECT 'LAB' AS src, lv.lab_vbg_time AS t, lv.lab_vbg_ph AS ph, lv.lab_vbg_paco2 AS pco2
         UNION ALL
         SELECT 'POC', pv.poc_vbg_time, pv.poc_vbg_ph, pv.poc_vbg_paco2)
   WHERE t IS NOT NULL
   ORDER BY t LIMIT 1) AS first_vbg
FROM hadms h
LEFT JOIN lab_abg la USING (hadm_id)
LEFT JOIN lab_vbg lv USING (hadm_id)
LEFT JOIN poc_abg pa USING (hadm_id)
LEFT JOIN poc_vbg pv USING (hadm_id)
"""

bg_pairs = run_sql_bq(bg_pairs_sql, params)

# Flatten STRUCTs for first_abg and first_vbg
for col in ["first_abg","first_vbg"]:
    if col in bg_pairs.columns:
        bg_pairs[f"{col}_src"]  = bg_pairs[col].apply(lambda x: x.get("src") if isinstance(x, dict) else None)
        bg_pairs[f"{col}_time"] = bg_pairs[col].apply(lambda x: x.get("t")   if isinstance(x, dict) else None)
        bg_pairs[f"{col}_ph"]   = bg_pairs[col].apply(lambda x: x.get("ph")  if isinstance(x, dict) else None)
        bg_pairs[f"{col}_pco2"] = bg_pairs[col].apply(lambda x: x.get("pco2")if isinstance(x, dict) else None)
        bg_pairs = bg_pairs.drop(columns=[col])

bg_pairs.head(3)

[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': 'params = {"hadms": hadm_list}\n\nbg_pairs_sql = rf"""\nWITH hadms AS (SELECT hadm_id FROM UNNEST(@hadms) AS hadm_id),\n\n/* ---------------- LAB (HOSP) ---------------- */\nhosp_cand AS (\n  SELECT\n    le.subject_id, le.hadm_id, le.charttime, le.specimen_id,\n    CAST(le.valuenum AS FLOAT64) AS val,\n    LOWER(COALESCE(le.valueuom,\'\')) AS uom,\n    LOWER(di.label) AS lbl,\n    LOWER(COALESCE(di.fluid,\'\')) AS fl,\n    LOWER(COALESCE(di.category,\'\')) AS cat\n  FROM `{PHYS}.{HOSP}.labevents`  le\n  JOIN `{PHYS}.{HOSP}.d_labitems` di ON di.itemid = le.itemid\n  JOIN hadms h ON h.hadm_id = le.hadm_id\n  WHERE le.valuenum IS NOT NULL\n    AND (\n         LOWER(COALESCE(di.category,\'\')) LIKE \'%blood gas%\' OR\n         LOWER(di.label) LIKE \'%pco2%\' OR\n         REGEXP_CONTAINS(LOWER(di.label), r\'\\bph\\b\') OR\n         REGEXP_CONTAINS(LOWER(di.label), r\'\\bpa?\\s*co(?:2|₂)\\b\')\n        )\n    AND NOT REGEXP_CONTAINS(LOWER(di.label), r\'(et\\s*co2|end[- ]?tidal|t\\s*co2|tco2|total\\s*co2|hco3|bicar)\')\n),\nhosp_spec AS (\n  SELECT le.specimen_id, LOWER(COALESCE(le.value,\'\')) AS spec_val\n  FROM `{PHYS}.{HOSP}.labevents` le\n  JOIN `{PHYS}.{HOSP}.d_labitems` di ON di.itemid = le.itemid\n  WHERE le.specimen_id IS NOT NULL\n    AND REGEXP_CONTAINS(LOWER(di.label), r\'(specimen|sample)\')\n),\nhosp_class AS (\n  SELECT\n    c.hadm_id, c.charttime, c.specimen_id, c.val, c.uom, c.lbl, c.fl,\n    CASE\n      WHEN REGEXP_CONTAINS(c.lbl, r\'\\b(?:blood\\s*)?ph\\b\') THEN \'ph\'\n      WHEN (c.lbl LIKE \'%pco2%\' OR REGEXP_CONTAINS(c.lbl, r\'\\bpa?\\s*co(?:2|₂)\\b\')) THEN \'pco2\'\n      ELSE NULL\n    END AS analyte,\n    CASE\n      WHEN REGEXP_CONTAINS(s.spec_val, r\'arter\') OR REGEXP_CONTAINS(s.spec_val, r\'\\bart\\b\') THEN \'arterial\'\n      WHEN REGEXP_CONTAINS(s.spec_val, r\'ven|mixed|central\') THEN \'venous\'\n      WHEN c.fl LIKE \'%arterial%\' OR REGEXP_CONTAINS(c.lbl, r\'\\b(abg|art|arterial|a[- ]?line)\\b\') THEN \'arterial\'\n      WHEN c.fl LIKE \'%ven%\'      OR REGEXP_CONTAINS(c.lbl, r\'\\b(vbg|ven|venous|mixed|central)\\b\') THEN \'venous\'\n      ELSE NULL\n    END AS site\n  FROM hosp_cand c\n  LEFT JOIN hosp_spec s USING (specimen_id)\n),\nhosp_pairs AS (\n  SELECT\n    hadm_id, specimen_id,\n    MIN(charttime) AS sample_time,\n    MAX(IF(analyte=\'ph\',   val, NULL)) AS ph,\n    MAX(IF(analyte=\'pco2\', val, NULL)) AS pco2_raw,\n    (ARRAY_AGG(IF(analyte=\'pco2\', uom, NULL) IGNORE NULLS LIMIT 1))[OFFSET(0)] AS pco2_uom,\n    (ARRAY_AGG(IF(analyte=\'ph\',   uom, NULL) IGNORE NULLS LIMIT 1))[OFFSET(0)] AS ph_uom,\n    (ARRAY_AGG(site IGNORE NULLS LIMIT 1))[OFFSET(0)] AS site\n  FROM hosp_class\n  GROUP BY hadm_id, specimen_id\n  HAVING (ph IS NOT NULL OR pco2_raw IS NOT NULL) AND site IN (\'arterial\',\'venous\')\n),\nhosp_pairs_std AS (\n  SELECT\n    hadm_id, specimen_id, sample_time, site,\n    ph, ph_uom,\n    CASE WHEN pco2_uom = \'kpa\' THEN pco2_raw * 7.50062 ELSE pco2_raw END AS pco2_mmHg,\n    \'mmhg\' AS pco2_uom_norm\n  FROM hosp_pairs\n  WHERE (ph IS NULL OR (ph BETWEEN 6.3 AND 7.8))\n    AND (pco2_raw IS NULL OR (CASE WHEN pco2_uom=\'kpa\' THEN pco2_raw*7.50062 ELSE pco2_raw END) BETWEEN 5 AND 200)\n),\nlab_abg AS (\n  SELECT hadm_id,\n         ph            AS lab_abg_ph,\n         ph_uom        AS lab_abg_ph_uom,\n         pco2_mmHg     AS lab_abg_paco2,\n         \'mmhg\'        AS lab_abg_paco2_uom,\n         sample_time   AS lab_abg_time\n  FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY hadm_id ORDER BY sample_time) rn\n        FROM hosp_pairs_std WHERE site=\'arterial\') WHERE rn=1\n),\nlab_vbg AS (\n  SELECT hadm_id,\n         ph            AS lab_vbg_ph,\n         ph_uom        AS lab_vbg_ph_uom,\n         pco2_mmHg     AS lab_vbg_paco2,\n         \'mmhg\'        AS lab_vbg_paco2_uom,\n         sample_time   AS lab_vbg_time\n  FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY hadm_id ORDER BY sample_time) rn\n        FROM hosp_pairs_std WHERE site=\'venous\') WHERE rn=1\n),\n\n/* ---------------- POC (ICU) ---------------- */\nicu_raw AS (\n  SELECT\n    ie.hadm_id, ce.stay_id, ce.charttime,\n    LOWER(di.label) AS lbl,\n    LOWER(REPLACE(COALESCE(ce.valueuom,\'\'),\' \',\'\')) AS uom_nospace,\n    LOWER(COALESCE(ce.value,\'\')) AS valstr,\n    COALESCE(\n      CAST(ce.valuenum AS FLOAT64),\n      SAFE_CAST(REGEXP_EXTRACT(LOWER(ce.value), r\'(-?\\d+(?:\\.\\d+)?)\') AS FLOAT64)\n    ) AS val\n  FROM `{PHYS}.{ICU}.chartevents` ce\n  JOIN `{PHYS}.{ICU}.d_items`  di ON di.itemid = ce.itemid\n  JOIN `{PHYS}.{ICU}.icustays` ie ON ie.stay_id = ce.stay_id\n  JOIN hadms h ON h.hadm_id = ie.hadm_id\n),\nicu_cand AS (\n  SELECT\n    hadm_id, stay_id, charttime, lbl, uom_nospace, valstr, val,\n    CASE\n      WHEN REGEXP_CONTAINS(lbl, r\'(^|[^a-z])ph([^a-z]|$)\') OR (val BETWEEN 6.3 AND 7.8) THEN \'ph\'\n      WHEN (\n             REGEXP_CONTAINS(lbl, r\'(^|[^a-z])p(?:a)?\\s*co\\s*(?:2|₂)([^a-z]|$)\')\n             OR uom_nospace IN (\'mmhg\',\'kpa\')\n             OR REGEXP_CONTAINS(valstr, r\'\\b(mm\\s*hg|kpa)\\b\')\n           )\n           AND NOT REGEXP_CONTAINS(lbl, r\'(et\\s*co2|end[- ]?tidal|t\\s*co2|tco2|total\\s*co2|hco3|bicar|v\\s*co2|vco2|co2\\s*(prod|elimin|production|elimination))\')\n      THEN \'pco2\'\n      ELSE NULL\n    END AS analyte,\n    CASE\n      WHEN REGEXP_CONTAINS(lbl, r\'\\b(abg|art|arterial|a[- ]?line)\\b\') THEN \'arterial\'\n      WHEN REGEXP_CONTAINS(lbl, r\'\\b(vbg|ven|venous|mixed|central)\\b\') THEN \'venous\'\n      ELSE NULL\n    END AS site\n  FROM icu_raw\n  WHERE val IS NOT NULL\n    AND (\n      REGEXP_CONTAINS(lbl, r\'(^|[^a-z])ph([^a-z]|$)\') OR\n      REGEXP_CONTAINS(lbl, r\'(^|[^a-z])p(?:a)?\\s*co\\s*(?:2|₂)([^a-z]|$)\') OR\n      uom_nospace IN (\'mmhg\',\'kpa\') OR\n      REGEXP_CONTAINS(valstr, r\'\\b(mm\\s*hg|kpa)\\b\')\n    )\n    AND NOT REGEXP_CONTAINS(lbl, r\'(et\\s*co2|end[- ]?tidal|t\\s*co2|tco2|total\\s*co2|hco3|bicar|v\\s*co2|vco2|co2\\s*(prod|elimin|production|elimination))\')\n),\nicu_ph AS (\n  SELECT hadm_id, stay_id, charttime, val AS ph, site AS site_ph\n  FROM icu_cand WHERE analyte=\'ph\'\n),\nicu_co2 AS (\n  SELECT hadm_id, stay_id, charttime, val AS pco2_raw, uom_nospace, valstr, site AS site_co2\n  FROM icu_cand WHERE analyte=\'pco2\'\n),\nicu_pair_win AS (\n  SELECT\n    p.hadm_id, p.stay_id,\n    COALESCE(p.site_ph, c.site_co2) AS site,\n    p.charttime AS ph_time, c.charttime AS co2_time,\n    p.ph,\n    CASE\n      WHEN c.uom_nospace=\'kpa\' OR REGEXP_CONTAINS(c.valstr, r\'\\bkpa\\b\') THEN \'kpa\'\n      WHEN c.uom_nospace=\'mmhg\' OR REGEXP_CONTAINS(c.valstr, r\'mm\\s*hg\') THEN \'mmhg\'\n      ELSE c.uom_nospace\n    END AS pco2_uom_norm_raw,\n    c.pco2_raw,\n    ABS(TIMESTAMP_DIFF(c.charttime, p.charttime, SECOND)) AS dt_sec\n  FROM icu_ph p\n  JOIN icu_co2 c\n    ON c.hadm_id = p.hadm_id\n   AND c.stay_id = p.stay_id\n   AND (COALESCE(p.site_ph, c.site_co2) IN (\'arterial\',\'venous\'))\n   AND ABS(TIMESTAMP_DIFF(c.charttime, p.charttime, MINUTE)) <= 10\n  QUALIFY ROW_NUMBER() OVER (\n    PARTITION BY p.hadm_id, p.stay_id, p.charttime\n    ORDER BY dt_sec\n  ) = 1\n),\nicu_pairs_std AS (\n  SELECT\n    hadm_id, stay_id, site,\n    LEAST(ph_time, co2_time) AS sample_time,\n    ph,\n    CAST(NULL AS STRING) AS ph_uom,              -- POC pH is unitless/null\n    CASE WHEN pco2_uom_norm_raw=\'kpa\' THEN pco2_raw*7.50062 ELSE pco2_raw END AS pco2_mmHg,\n    \'mmhg\' AS pco2_uom_norm\n  FROM icu_pair_win\n  WHERE (ph BETWEEN 6.3 AND 7.8 OR ph IS NULL)\n    AND (CASE WHEN pco2_uom_norm_raw=\'kpa\' THEN pco2_raw*7.50062 ELSE pco2_raw END) BETWEEN 5 AND 200\n),\nicu_solo_pco2_std AS (\n  SELECT\n    hadm_id, stay_id, site_co2 AS site,\n    charttime AS sample_time,\n    CAST(NULL AS FLOAT64) AS ph,\n    CAST(NULL AS STRING)  AS ph_uom,            -- no pH here\n    CASE WHEN uom_nospace=\'kpa\' OR REGEXP_CONTAINS(valstr, r\'\\bkpa\\b\') THEN pco2_raw*7.50062 ELSE pco2_raw END AS pco2_mmHg,\n    \'mmhg\' AS pco2_uom_norm\n  FROM icu_co2\n  WHERE site_co2 IN (\'arterial\',\'venous\')\n    AND pco2_raw BETWEEN 5 AND 200\n),\nicu_all AS (\n  SELECT * FROM icu_pairs_std\n  UNION ALL\n  SELECT * FROM icu_solo_pco2_std\n),\n\npoc_abg AS (\n  SELECT hadm_id,\n         ph            AS poc_abg_ph,\n         ph_uom        AS poc_abg_ph_uom,\n         pco2_mmHg     AS poc_abg_paco2,\n         \'mmhg\'        AS poc_abg_paco2_uom,\n         sample_time   AS poc_abg_time\n  FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY hadm_id ORDER BY sample_time) rn\n        FROM icu_all WHERE site=\'arterial\') WHERE rn=1\n),\npoc_vbg AS (\n  SELECT hadm_id,\n         ph            AS poc_vbg_ph,\n         ph_uom        AS poc_vbg_ph_uom,\n         pco2_mmHg     AS poc_vbg_paco2,\n         \'mmhg\'        AS poc_vbg_paco2_uom,\n         sample_time   AS poc_vbg_time\n  FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY hadm_id ORDER BY sample_time) rn\n        FROM icu_all WHERE site=\'venous\') WHERE rn=1\n)\n\n/* ---------------- Final one row per hadm ---------------- */\nSELECT\n  h.hadm_id,\n  -- LAB-ABG / LAB-VBG\n  la.lab_abg_ph, la.lab_abg_ph_uom, la.lab_abg_paco2, la.lab_abg_paco2_uom, la.lab_abg_time,\n  lv.lab_vbg_ph, lv.lab_vbg_ph_uom, lv.lab_vbg_paco2, lv.lab_vbg_paco2_uom, lv.lab_vbg_time,\n  -- POC-ABG / POC-VBG\n  pa.poc_abg_ph, pa.poc_abg_ph_uom, pa.poc_abg_paco2, pa.poc_abg_paco2_uom, pa.poc_abg_time,\n  pv.poc_vbg_ph, pv.poc_vbg_ph_uom, pv.poc_vbg_paco2, pv.poc_vbg_paco2_uom, pv.poc_vbg_time,\n  -- First ABG across LAB+POC\n  (SELECT AS STRUCT src, t, ph, pco2\n   FROM (SELECT \'LAB\' AS src, la.lab_abg_time AS t, la.lab_abg_ph AS ph, la.lab_abg_paco2 AS pco2\n         UNION ALL\n         SELECT \'POC\', pa.poc_abg_time, pa.poc_abg_ph, pa.poc_abg_paco2)\n   WHERE t IS NOT NULL\n   ORDER BY t LIMIT 1) AS first_abg,\n  -- First VBG across LAB+POC\n  (SELECT AS STRUCT src, t, ph, pco2\n   FROM (SELECT \'LAB\' AS src, lv.lab_vbg_time AS t, lv.lab_vbg_ph AS ph, lv.lab_vbg_paco2 AS pco2\n         UNION ALL\n         SELECT \'POC\', pv.poc_vbg_time, pv.poc_vbg_ph, pv.poc_vbg_paco2)\n   WHERE t IS NOT NULL\n   ORDER BY t LIMIT 1) AS first_vbg\nFROM hadms h\nLEFT JOIN lab_abg la USING (hadm_id)\nLEFT JOIN lab_vbg lv USING (hadm_id)\nLEFT JOIN poc_abg pa USING (hadm_id)\nLEFT JOIN poc_vbg pv USING (hadm_id)\n"""\n\nbg_pairs = run_sql_bq(bg_pairs_sql, params)\n\n# Flatten STRUCTs for first_abg and first_vbg\nfor col in ["first_abg","first_vbg"]:\n    if col in bg_pairs.columns:\n        bg_pairs[f"{col}_src"]  = bg_pairs[col].apply(lambda x: x.get("src") if isinstance(x, dict) else None)\n        bg_pairs[f"{col}_time"] = bg_pairs[col].apply(lambda x: x.get("t")   if isinstance(x, dict) else None)\n        bg_pairs[f"{col}_ph"]   = bg_pairs[col].apply(lambda x: x.get("ph")  if isinstance(x, dict) else None)\n        bg_pairs[f"{col}_pco2"] = bg_pairs[col].apply(lambda x: x.get("pco2")if isinstance(x, dict) else None)\n        bg_pairs = bg_pairs.drop(columns=[col])\n\nbg_pairs.head(3)\n', 'execution_count': 8}
[NbConvertApp] msg_type: execute_result
[NbConvertApp] content: {'data': {'text/plain': '    hadm_id  lab_abg_ph lab_abg_ph_uom  lab_abg_paco2 lab_abg_paco2_uom  \\\n0  20000094         NaN           None            NaN              None   \n1  20000694         NaN           None            NaN              None   \n2  20001305         NaN           None            NaN              None   \n\n  lab_abg_time  lab_vbg_ph lab_vbg_ph_uom  lab_vbg_paco2 lab_vbg_paco2_uom  \\\n0          NaT        7.41          units           47.0              mmhg   \n1          NaT        7.29          units           43.0              mmhg   \n2          NaT        7.29          units           77.0              mmhg   \n\n   ... poc_vbg_paco2_uom        poc_vbg_time first_abg_src  first_abg_time  \\\n0  ...              mmhg 2150-03-02 22:05:00          None             NaT   \n1  ...              None                 NaT          None             NaT   \n2  ...              mmhg 2178-03-25 04:28:00          None             NaT   \n\n  first_abg_ph first_abg_pco2  first_vbg_src      first_vbg_time  \\\n0          NaN            NaN            LAB 2150-03-02 15:53:00   \n1          NaN            NaN            LAB 2144-09-01 05:15:00   \n2          NaN            NaN            LAB 2178-03-25 04:28:00   \n\n   first_vbg_ph first_vbg_pco2  \n0          7.41           47.0  \n1          7.29           43.0  \n2          7.29           77.0  \n\n[3 rows x 29 columns]', 'text/html': '<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>hadm_id</th>\n      <th>lab_abg_ph</th>\n      <th>lab_abg_ph_uom</th>\n      <th>lab_abg_paco2</th>\n      <th>lab_abg_paco2_uom</th>\n      <th>lab_abg_time</th>\n      <th>lab_vbg_ph</th>\n      <th>lab_vbg_ph_uom</th>\n      <th>lab_vbg_paco2</th>\n      <th>lab_vbg_paco2_uom</th>\n      <th>...</th>\n      <th>poc_vbg_paco2_uom</th>\n      <th>poc_vbg_time</th>\n      <th>first_abg_src</th>\n      <th>first_abg_time</th>\n      <th>first_abg_ph</th>\n      <th>first_abg_pco2</th>\n      <th>first_vbg_src</th>\n      <th>first_vbg_time</th>\n      <th>first_vbg_ph</th>\n      <th>first_vbg_pco2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20000094</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>NaT</td>\n      <td>7.41</td>\n      <td>units</td>\n      <td>47.0</td>\n      <td>mmhg</td>\n      <td>...</td>\n      <td>mmhg</td>\n      <td>2150-03-02 22:05:00</td>\n      <td>None</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>LAB</td>\n      <td>2150-03-02 15:53:00</td>\n      <td>7.41</td>\n      <td>47.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20000694</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>NaT</td>\n      <td>7.29</td>\n      <td>units</td>\n      <td>43.0</td>\n      <td>mmhg</td>\n      <td>...</td>\n      <td>None</td>\n      <td>NaT</td>\n      <td>None</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>LAB</td>\n      <td>2144-09-01 05:15:00</td>\n      <td>7.29</td>\n      <td>43.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20001305</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>NaT</td>\n      <td>7.29</td>\n      <td>units</td>\n      <td>77.0</td>\n      <td>mmhg</td>\n      <td>...</td>\n      <td>mmhg</td>\n      <td>2178-03-25 04:28:00</td>\n      <td>None</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>LAB</td>\n      <td>2178-03-25 04:28:00</td>\n      <td>7.29</td>\n      <td>77.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 29 columns</p>\n</div>'}, 'metadata': {}, 'execution_count': 8}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 25
[NbConvertApp] Skipping non-executing cell 26
[NbConvertApp] Executing cell:

SQL["demo_sql"] = f"""
WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x)
SELECT
  a.hadm_id,
  a.subject_id,
  a.admittime,
  a.dischtime,
  a.deathtime,
  a.admission_type,
  a.admission_location,
  a.discharge_location,
  a.insurance,
  -- LOS (days)
  TIMESTAMP_DIFF(a.dischtime, a.admittime, HOUR) / 24.0 AS hosp_los_days,
  -- in-hospital death
  IF(a.deathtime IS NOT NULL, 1, 0) AS death_in_hosp,
  -- demographics
  p.gender,
  SAFE_CAST(ROUND(p.anchor_age + (EXTRACT(YEAR FROM a.admittime) - p.anchor_year), 1) AS FLOAT64) AS age_at_admit,
  -- 30-day all-cause mortality from admission
  IF(p.dod IS NOT NULL AND DATE_DIFF(DATE(p.dod), DATE(a.admittime), DAY) BETWEEN 0 AND 30, 1, 0) AS death_30d
FROM `{PHYS}.{HOSP}.admissions` a
JOIN hadms h USING (hadm_id)
JOIN `{PHYS}.{HOSP}.patients` p USING (subject_id)
"""
demo = run_sql_bq(sql("demo_sql"), {"hadms": hadm_list})
print("Demo rows:", len(demo))
demo.head(3)


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '\nSQL["demo_sql"] = f"""\nWITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x)\nSELECT\n  a.hadm_id,\n  a.subject_id,\n  a.admittime,\n  a.dischtime,\n  a.deathtime,\n  a.admission_type,\n  a.admission_location,\n  a.discharge_location,\n  a.insurance,\n  -- LOS (days)\n  TIMESTAMP_DIFF(a.dischtime, a.admittime, HOUR) / 24.0 AS hosp_los_days,\n  -- in-hospital death\n  IF(a.deathtime IS NOT NULL, 1, 0) AS death_in_hosp,\n  -- demographics\n  p.gender,\n  SAFE_CAST(ROUND(p.anchor_age + (EXTRACT(YEAR FROM a.admittime) - p.anchor_year), 1) AS FLOAT64) AS age_at_admit,\n  -- 30-day all-cause mortality from admission\n  IF(p.dod IS NOT NULL AND DATE_DIFF(DATE(p.dod), DATE(a.admittime), DAY) BETWEEN 0 AND 30, 1, 0) AS death_30d\nFROM `{PHYS}.{HOSP}.admissions` a\nJOIN hadms h USING (hadm_id)\nJOIN `{PHYS}.{HOSP}.patients` p USING (subject_id)\n"""\ndemo = run_sql_bq(sql("demo_sql"), {"hadms": hadm_list})\nprint("Demo rows:", len(demo))\ndemo.head(3)\n\n', 'execution_count': 9}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Demo rows: 40151\n'}
[NbConvertApp] msg_type: execute_result
[NbConvertApp] content: {'data': {'text/plain': '    hadm_id  subject_id           admittime           dischtime  \\\n0  23485217    10584718 2165-02-12 15:41:00 2165-03-06 08:20:00   \n1  22661627    10032409 2130-01-12 18:42:00 2130-01-21 14:32:00   \n2  22447711    13352386 2188-03-25 01:17:00 2188-03-26 16:00:00   \n\n            deathtime admission_type                      admission_location  \\\n0 2165-03-06 08:20:00       EW EMER.  TRANSFER FROM SKILLED NURSING FACILITY   \n1                 NaT       EW EMER.                          EMERGENCY ROOM   \n2                 NaT       EW EMER.                          EMERGENCY ROOM   \n\n         discharge_location insurance  hosp_los_days  death_in_hosp gender  \\\n0                      DIED  Medicare      21.708333              1      M   \n1  SKILLED NURSING FACILITY  Medicare       8.833333              0      F   \n2                      HOME   Private       1.625000              0      F   \n\n   age_at_admit  death_30d  \n0          78.0          1  \n1          77.0          0  \n2          64.0          0  ', 'text/html': '<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>hadm_id</th>\n      <th>subject_id</th>\n      <th>admittime</th>\n      <th>dischtime</th>\n      <th>deathtime</th>\n      <th>admission_type</th>\n      <th>admission_location</th>\n      <th>discharge_location</th>\n      <th>insurance</th>\n      <th>hosp_los_days</th>\n      <th>death_in_hosp</th>\n      <th>gender</th>\n      <th>age_at_admit</th>\n      <th>death_30d</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>23485217</td>\n      <td>10584718</td>\n      <td>2165-02-12 15:41:00</td>\n      <td>2165-03-06 08:20:00</td>\n      <td>2165-03-06 08:20:00</td>\n      <td>EW EMER.</td>\n      <td>TRANSFER FROM SKILLED NURSING FACILITY</td>\n      <td>DIED</td>\n      <td>Medicare</td>\n      <td>21.708333</td>\n      <td>1</td>\n      <td>M</td>\n      <td>78.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>22661627</td>\n      <td>10032409</td>\n      <td>2130-01-12 18:42:00</td>\n      <td>2130-01-21 14:32:00</td>\n      <td>NaT</td>\n      <td>EW EMER.</td>\n      <td>EMERGENCY ROOM</td>\n      <td>SKILLED NURSING FACILITY</td>\n      <td>Medicare</td>\n      <td>8.833333</td>\n      <td>0</td>\n      <td>F</td>\n      <td>77.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>22447711</td>\n      <td>13352386</td>\n      <td>2188-03-25 01:17:00</td>\n      <td>2188-03-26 16:00:00</td>\n      <td>NaT</td>\n      <td>EW EMER.</td>\n      <td>EMERGENCY ROOM</td>\n      <td>HOME</td>\n      <td>Private</td>\n      <td>1.625000</td>\n      <td>0</td>\n      <td>F</td>\n      <td>64.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>'}, 'metadata': {}, 'execution_count': 9}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
# ==== Drop-in: safe merge utilities (one cell, run once) ====
import pandas as pd
from typing import Iterable, Optional, Literal

def _ensure_Int64(s: pd.Series) -> pd.Series:
    """Coerce to pandas nullable Int64 (preserves NA)."""
    return pd.to_numeric(s, errors="coerce").astype("Int64")

def strip_subject_cols(fr: pd.DataFrame) -> pd.DataFrame:
    """Remove any subject_id-like columns from a frame (e.g., 'subject_id', 'Subject_ID')."""
    return fr.drop(columns=[c for c in fr.columns if c.lower().startswith("subject_id")],
                   errors="ignore")

def safe_merge_on_hadm(
    left: pd.DataFrame,
    right: pd.DataFrame,
    *,
    right_name: str,
    take: Optional[Iterable[str]] = None,
    order_by: Optional[Iterable[str]] = None,
    check_subject: Literal[False, "warn", "raise"] = False,
) -> pd.DataFrame:
    """
    Left-merge 'right' into 'left' on hadm_id, returning a copy of left with right's columns.
    - Dedupes right on hadm_id (optionally using order_by to pick the first row).
    - Optionally restricts right columns via `take`.
    - Optionally audits subject_id agreement before dropping subject_id from right.
    - Always strips subject_id-like columns from the right to prevent *_x/_y suffixes.
    - Raises if any *_x/_y suffixes still appear (indicates overlapping names besides hadm_id).
    """
    if "hadm_id" not in left.columns:
        raise KeyError(f"left frame lacks hadm_id before merging {right_name}")
    if "hadm_id" not in right.columns:
        raise KeyError(f"{right_name} lacks hadm_id")

    L = left.copy()
    R = right.copy()

    # Standardize dtypes of keys
    L["hadm_id"] = _ensure_Int64(L["hadm_id"])
    R["hadm_id"] = _ensure_Int64(R["hadm_id"])
    if "subject_id" in L.columns:
        L["subject_id"] = _ensure_Int64(L["subject_id"])
    if "subject_id" in R.columns:
        R["subject_id"] = _ensure_Int64(R["subject_id"])

    # Dedupe RIGHT by hadm_id (optionally order_by first)
    if order_by:
        R = (R.sort_values(list(order_by))
               .drop_duplicates(subset=["hadm_id"], keep="first"))
    else:
        R = R.drop_duplicates(subset=["hadm_id"], keep="first")

    # Optional subject_id consistency audit (before stripping)
    if check_subject and ("subject_id" in L.columns) and ("subject_id" in R.columns):
        # Join only on hadm_id where both sides have subject_id
        tmp = (L[["hadm_id", "subject_id"]]
                 .merge(R[["hadm_id", "subject_id"]],
                        on="hadm_id", how="inner", suffixes=("_L","_R")))
        mism = (tmp["subject_id_L"].notna() & tmp["subject_id_R"].notna() &
                (tmp["subject_id_L"] != tmp["subject_id_R"]))
        n_mism = int(mism.sum())
        if n_mism > 0:
            sample_ids = tmp.loc[mism, "hadm_id"].head(10).tolist()
            msg = (f"[{right_name}] subject_id mismatch on {n_mism} hadm_id(s). "
                   f"Examples: {sample_ids}")
            if check_subject == "raise":
                raise ValueError(msg)
            else:
                print("WARNING:", msg)

    # Limit right columns (avoid accidental overlaps)
    if take is not None:
        keep = ["hadm_id"] + [c for c in take if c != "hadm_id"]
        R = R[[c for c in keep if c in R.columns]]

    # Always strip subject_id-like columns from right to prevent *_x/_y
    R = strip_subject_cols(R)

    # Final merge
    out = L.merge(R, on="hadm_id", how="left", suffixes=("", ""))

    # Guard: no suffixes should be present
    bad = [c for c in out.columns if c.endswith("_x") or c.endswith("_y")]
    if bad:
        raise RuntimeError(
            f"Merge with {right_name} produced suffixed columns {bad}. "
            "You likely have overlapping column names other than hadm_id."
        )
    return out

print("Safe merge helpers loaded.")

[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# ==== Drop-in: safe merge utilities (one cell, run once) ====\nimport pandas as pd\nfrom typing import Iterable, Optional, Literal\n\ndef _ensure_Int64(s: pd.Series) -> pd.Series:\n    """Coerce to pandas nullable Int64 (preserves NA)."""\n    return pd.to_numeric(s, errors="coerce").astype("Int64")\n\ndef strip_subject_cols(fr: pd.DataFrame) -> pd.DataFrame:\n    """Remove any subject_id-like columns from a frame (e.g., \'subject_id\', \'Subject_ID\')."""\n    return fr.drop(columns=[c for c in fr.columns if c.lower().startswith("subject_id")],\n                   errors="ignore")\n\ndef safe_merge_on_hadm(\n    left: pd.DataFrame,\n    right: pd.DataFrame,\n    *,\n    right_name: str,\n    take: Optional[Iterable[str]] = None,\n    order_by: Optional[Iterable[str]] = None,\n    check_subject: Literal[False, "warn", "raise"] = False,\n) -> pd.DataFrame:\n    """\n    Left-merge \'right\' into \'left\' on hadm_id, returning a copy of left with right\'s columns.\n    - Dedupes right on hadm_id (optionally using order_by to pick the first row).\n    - Optionally restricts right columns via `take`.\n    - Optionally audits subject_id agreement before dropping subject_id from right.\n    - Always strips subject_id-like columns from the right to prevent *_x/_y suffixes.\n    - Raises if any *_x/_y suffixes still appear (indicates overlapping names besides hadm_id).\n    """\n    if "hadm_id" not in left.columns:\n        raise KeyError(f"left frame lacks hadm_id before merging {right_name}")\n    if "hadm_id" not in right.columns:\n        raise KeyError(f"{right_name} lacks hadm_id")\n\n    L = left.copy()\n    R = right.copy()\n\n    # Standardize dtypes of keys\n    L["hadm_id"] = _ensure_Int64(L["hadm_id"])\n    R["hadm_id"] = _ensure_Int64(R["hadm_id"])\n    if "subject_id" in L.columns:\n        L["subject_id"] = _ensure_Int64(L["subject_id"])\n    if "subject_id" in R.columns:\n        R["subject_id"] = _ensure_Int64(R["subject_id"])\n\n    # Dedupe RIGHT by hadm_id (optionally order_by first)\n    if order_by:\n        R = (R.sort_values(list(order_by))\n               .drop_duplicates(subset=["hadm_id"], keep="first"))\n    else:\n        R = R.drop_duplicates(subset=["hadm_id"], keep="first")\n\n    # Optional subject_id consistency audit (before stripping)\n    if check_subject and ("subject_id" in L.columns) and ("subject_id" in R.columns):\n        # Join only on hadm_id where both sides have subject_id\n        tmp = (L[["hadm_id", "subject_id"]]\n                 .merge(R[["hadm_id", "subject_id"]],\n                        on="hadm_id", how="inner", suffixes=("_L","_R")))\n        mism = (tmp["subject_id_L"].notna() & tmp["subject_id_R"].notna() &\n                (tmp["subject_id_L"] != tmp["subject_id_R"]))\n        n_mism = int(mism.sum())\n        if n_mism > 0:\n            sample_ids = tmp.loc[mism, "hadm_id"].head(10).tolist()\n            msg = (f"[{right_name}] subject_id mismatch on {n_mism} hadm_id(s). "\n                   f"Examples: {sample_ids}")\n            if check_subject == "raise":\n                raise ValueError(msg)\n            else:\n                print("WARNING:", msg)\n\n    # Limit right columns (avoid accidental overlaps)\n    if take is not None:\n        keep = ["hadm_id"] + [c for c in take if c != "hadm_id"]\n        R = R[[c for c in keep if c in R.columns]]\n\n    # Always strip subject_id-like columns from right to prevent *_x/_y\n    R = strip_subject_cols(R)\n\n    # Final merge\n    out = L.merge(R, on="hadm_id", how="left", suffixes=("", ""))\n\n    # Guard: no suffixes should be present\n    bad = [c for c in out.columns if c.endswith("_x") or c.endswith("_y")]\n    if bad:\n        raise RuntimeError(\n            f"Merge with {right_name} produced suffixed columns {bad}. "\n            "You likely have overlapping column names other than hadm_id."\n        )\n    return out\n\nprint("Safe merge helpers loaded.")\n', 'execution_count': 10}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Safe merge helpers loaded.\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 29
[NbConvertApp] Skipping non-executing cell 30
[NbConvertApp] Executing cell:
race_eth_sql = rf"""
WITH hadms AS (
  SELECT x AS hadm_id
  FROM UNNEST(@hadms) AS x
),

-- Hospital admission "race" text
hosp AS (
  SELECT a.hadm_id, LOWER(TRIM(a.race)) AS race_hosp_raw
  FROM `{PHYS}.{HOSP}.admissions` a
  JOIN hadms hm USING (hadm_id)
),

-- Earliest ED stay leading to the admission; take its "race" text if present
ed_first AS (
  SELECT
    e.hadm_id,
    (ARRAY_AGG(STRUCT(e.intime AS intime, LOWER(TRIM(e.race)) AS race_ed_raw)
               ORDER BY e.intime ASC LIMIT 1))[OFFSET(0)] AS pick
  FROM `{PHYS}.{ED}.edstays` e
  JOIN hadms hm USING (hadm_id)
  GROUP BY e.hadm_id
),
ed AS (
  SELECT hadm_id, pick.race_ed_raw
  FROM ed_first
),

-- Combine ED + Hospital for maximum coverage
comb AS (
  SELECT
    hm.hadm_id,
    ho.race_hosp_raw,
    ed.race_ed_raw,
    TRIM(REGEXP_REPLACE(CONCAT(COALESCE(ho.race_hosp_raw,''), ' ', COALESCE(ed.race_ed_raw,'')), r'\s+', ' ')) AS race_text_any
  FROM hadms hm
  LEFT JOIN hosp ho USING (hadm_id)
  LEFT JOIN ed   ed USING (hadm_id)
),

-- Tokenization to OMB families + Hispanic ethnicity
tok AS (
  SELECT
    hadm_id, race_hosp_raw, race_ed_raw, race_text_any,

    -- Ethnicity (Hispanic)
    REGEXP_CONTAINS(race_text_any, r'\b(hispanic|latinx|latino|latina)\b') AS is_hisp,

    -- Race families (use boundaries to reduce false positives)
    REGEXP_CONTAINS(race_text_any, r'american\s+indian|\balaska\b') AS is_aian,
    REGEXP_CONTAINS(race_text_any, r'\basian\b') AS is_asian,
    REGEXP_CONTAINS(race_text_any, r'\b(black|african\s+american)\b') AS is_black,
    REGEXP_CONTAINS(race_text_any, r'hawaiian|pacific\s+islander') AS is_nhopi,
    REGEXP_CONTAINS(race_text_any, r'\bwhite\b|caucasian') AS is_white,

    -- Unknown/other indicators
    REGEXP_CONTAINS(race_text_any, r'unknown|other|declined|unable|not\s+reported|missing|null') AS is_unknown_any,

    -- Multi-race hints
    REGEXP_CONTAINS(race_text_any, r'(two|2)\s+or\s+more|multi|biracial|multiracial') AS is_multi_hint
  FROM comb
),

-- Decide ethnicity per NIH
ethn AS (
  SELECT
    hadm_id, race_hosp_raw, race_ed_raw, race_text_any,
    CASE
      WHEN is_hisp THEN 'Hispanic or Latino'
      WHEN (race_text_any IS NULL OR race_text_any = '' OR is_unknown_any) THEN 'Unknown or Not Reported'
      ELSE 'Not Hispanic or Latino'
    END AS nih_ethnicity,
    (CAST(is_aian AS INT64) + CAST(is_asian AS INT64) + CAST(is_black AS INT64)
     + CAST(is_nhopi AS INT64) + CAST(is_white AS INT64)) AS race_hits,
    is_aian, is_asian, is_black, is_nhopi, is_white, is_multi_hint, is_unknown_any
  FROM tok
),

-- Decide race per NIH/OMB (1997)
race_assign AS (
  SELECT
    hadm_id, race_hosp_raw, race_ed_raw, race_text_any, nih_ethnicity,
    CASE
      WHEN race_hits >= 2 OR is_multi_hint THEN 'More than one race'
      WHEN is_aian THEN 'American Indian or Alaska Native'
      WHEN is_asian THEN 'Asian'
      WHEN is_black THEN 'Black or African American'
      WHEN is_nhopi THEN 'Native Hawaiian or Other Pacific Islander'
      WHEN is_white THEN 'White'
      WHEN is_unknown_any OR race_text_any IS NULL OR race_text_any = '' THEN 'Unknown or Not Reported'
      ELSE 'Unknown or Not Reported'
    END AS nih_race
  FROM ethn
)

SELECT hadm_id, race_hosp_raw, race_ed_raw, nih_race, nih_ethnicity
FROM race_assign
"""
race_eth = run_sql_bq(race_eth_sql, {"hadms": hadm_list})
print("Race/Eth rows:", len(race_eth))
race_eth.head(3)


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': 'race_eth_sql = rf"""\nWITH hadms AS (\n  SELECT x AS hadm_id\n  FROM UNNEST(@hadms) AS x\n),\n\n-- Hospital admission "race" text\nhosp AS (\n  SELECT a.hadm_id, LOWER(TRIM(a.race)) AS race_hosp_raw\n  FROM `{PHYS}.{HOSP}.admissions` a\n  JOIN hadms hm USING (hadm_id)\n),\n\n-- Earliest ED stay leading to the admission; take its "race" text if present\ned_first AS (\n  SELECT\n    e.hadm_id,\n    (ARRAY_AGG(STRUCT(e.intime AS intime, LOWER(TRIM(e.race)) AS race_ed_raw)\n               ORDER BY e.intime ASC LIMIT 1))[OFFSET(0)] AS pick\n  FROM `{PHYS}.{ED}.edstays` e\n  JOIN hadms hm USING (hadm_id)\n  GROUP BY e.hadm_id\n),\ned AS (\n  SELECT hadm_id, pick.race_ed_raw\n  FROM ed_first\n),\n\n-- Combine ED + Hospital for maximum coverage\ncomb AS (\n  SELECT\n    hm.hadm_id,\n    ho.race_hosp_raw,\n    ed.race_ed_raw,\n    TRIM(REGEXP_REPLACE(CONCAT(COALESCE(ho.race_hosp_raw,\'\'), \' \', COALESCE(ed.race_ed_raw,\'\')), r\'\\s+\', \' \')) AS race_text_any\n  FROM hadms hm\n  LEFT JOIN hosp ho USING (hadm_id)\n  LEFT JOIN ed   ed USING (hadm_id)\n),\n\n-- Tokenization to OMB families + Hispanic ethnicity\ntok AS (\n  SELECT\n    hadm_id, race_hosp_raw, race_ed_raw, race_text_any,\n\n    -- Ethnicity (Hispanic)\n    REGEXP_CONTAINS(race_text_any, r\'\\b(hispanic|latinx|latino|latina)\\b\') AS is_hisp,\n\n    -- Race families (use boundaries to reduce false positives)\n    REGEXP_CONTAINS(race_text_any, r\'american\\s+indian|\\balaska\\b\') AS is_aian,\n    REGEXP_CONTAINS(race_text_any, r\'\\basian\\b\') AS is_asian,\n    REGEXP_CONTAINS(race_text_any, r\'\\b(black|african\\s+american)\\b\') AS is_black,\n    REGEXP_CONTAINS(race_text_any, r\'hawaiian|pacific\\s+islander\') AS is_nhopi,\n    REGEXP_CONTAINS(race_text_any, r\'\\bwhite\\b|caucasian\') AS is_white,\n\n    -- Unknown/other indicators\n    REGEXP_CONTAINS(race_text_any, r\'unknown|other|declined|unable|not\\s+reported|missing|null\') AS is_unknown_any,\n\n    -- Multi-race hints\n    REGEXP_CONTAINS(race_text_any, r\'(two|2)\\s+or\\s+more|multi|biracial|multiracial\') AS is_multi_hint\n  FROM comb\n),\n\n-- Decide ethnicity per NIH\nethn AS (\n  SELECT\n    hadm_id, race_hosp_raw, race_ed_raw, race_text_any,\n    CASE\n      WHEN is_hisp THEN \'Hispanic or Latino\'\n      WHEN (race_text_any IS NULL OR race_text_any = \'\' OR is_unknown_any) THEN \'Unknown or Not Reported\'\n      ELSE \'Not Hispanic or Latino\'\n    END AS nih_ethnicity,\n    (CAST(is_aian AS INT64) + CAST(is_asian AS INT64) + CAST(is_black AS INT64)\n     + CAST(is_nhopi AS INT64) + CAST(is_white AS INT64)) AS race_hits,\n    is_aian, is_asian, is_black, is_nhopi, is_white, is_multi_hint, is_unknown_any\n  FROM tok\n),\n\n-- Decide race per NIH/OMB (1997)\nrace_assign AS (\n  SELECT\n    hadm_id, race_hosp_raw, race_ed_raw, race_text_any, nih_ethnicity,\n    CASE\n      WHEN race_hits >= 2 OR is_multi_hint THEN \'More than one race\'\n      WHEN is_aian THEN \'American Indian or Alaska Native\'\n      WHEN is_asian THEN \'Asian\'\n      WHEN is_black THEN \'Black or African American\'\n      WHEN is_nhopi THEN \'Native Hawaiian or Other Pacific Islander\'\n      WHEN is_white THEN \'White\'\n      WHEN is_unknown_any OR race_text_any IS NULL OR race_text_any = \'\' THEN \'Unknown or Not Reported\'\n      ELSE \'Unknown or Not Reported\'\n    END AS nih_race\n  FROM ethn\n)\n\nSELECT hadm_id, race_hosp_raw, race_ed_raw, nih_race, nih_ethnicity\nFROM race_assign\n"""\nrace_eth = run_sql_bq(race_eth_sql, {"hadms": hadm_list})\nprint("Race/Eth rows:", len(race_eth))\nrace_eth.head(3)\n\n', 'execution_count': 11}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Race/Eth rows: 40151\n'}
[NbConvertApp] msg_type: execute_result
[NbConvertApp] content: {'data': {'text/plain': '    hadm_id race_hosp_raw race_ed_raw nih_race           nih_ethnicity\n0  20000094         white        None    White  Not Hispanic or Latino\n1  20000694         white       white    White  Not Hispanic or Latino\n2  20001305         white       white    White  Not Hispanic or Latino', 'text/html': '<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>hadm_id</th>\n      <th>race_hosp_raw</th>\n      <th>race_ed_raw</th>\n      <th>nih_race</th>\n      <th>nih_ethnicity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20000094</td>\n      <td>white</td>\n      <td>None</td>\n      <td>White</td>\n      <td>Not Hispanic or Latino</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20000694</td>\n      <td>white</td>\n      <td>white</td>\n      <td>White</td>\n      <td>Not Hispanic or Latino</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20001305</td>\n      <td>white</td>\n      <td>white</td>\n      <td>White</td>\n      <td>Not Hispanic or Latino</td>\n    </tr>\n  </tbody>\n</table>\n</div>'}, 'metadata': {}, 'execution_count': 11}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 32
[NbConvertApp] Skipping non-executing cell 33
[NbConvertApp] Executing cell:
# ED triage
SQL["ed_triage_sql"] = f"""
WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x),
edmap AS (
  SELECT stay_id, hadm_id, intime
  FROM `{PHYS}.{ED}.edstays`
  WHERE hadm_id IN (SELECT hadm_id FROM hadms)
),
tri AS (
  SELECT stay_id, temperature, heartrate, resprate, o2sat, sbp, dbp, pain, acuity, chiefcomplaint
  FROM `{PHYS}.{ED}.triage`
),
tri_by_stay AS (
  SELECT m.hadm_id, m.intime, t.*
  FROM edmap m
  JOIN tri t USING (stay_id)
),
tri_by_hadm AS (
  SELECT
    hadm_id,
    (ARRAY_AGG(STRUCT(intime, temperature, heartrate, resprate, o2sat, sbp, dbp, pain, acuity, chiefcomplaint)
               ORDER BY intime LIMIT 1))[OFFSET(0)] AS pick
  FROM tri_by_stay
  GROUP BY hadm_id
)
SELECT
  hadm_id,
  pick.temperature    AS ed_triage_temp,
  pick.heartrate      AS ed_triage_hr,
  pick.resprate       AS ed_triage_rr,
  pick.o2sat          AS ed_triage_o2sat,
  pick.sbp            AS ed_triage_sbp,
  pick.dbp            AS ed_triage_dbp,
  pick.pain           AS ed_triage_pain,
  pick.acuity         AS ed_triage_acuity,
  pick.chiefcomplaint AS ed_triage_cc
FROM tri_by_hadm
"""
ed_triage = run_sql_bq(sql("ed_triage_sql"), {"hadms": hadm_list})
print("ED triage rows:", len(ed_triage))

# First ED vitals
SQL["ed_first_vitals_sql"] = f"""
WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x),
edmap AS (
  SELECT stay_id, hadm_id
  FROM `{PHYS}.{ED}.edstays`
  WHERE hadm_id IN (SELECT hadm_id FROM hadms)
),
vs AS (
  SELECT stay_id, charttime, temperature, heartrate, resprate, o2sat, sbp, dbp, rhythm, pain
  FROM `{PHYS}.{ED}.vitalsign`
),
first_vs AS (
  SELECT
    m.hadm_id,
    (ARRAY_AGG(STRUCT(v.charttime, v.temperature, v.heartrate, v.resprate, v.o2sat, v.sbp, v.dbp, v.rhythm, v.pain)
               ORDER BY v.charttime LIMIT 1))[OFFSET(0)] AS pick
  FROM edmap m JOIN vs v USING (stay_id)
  GROUP BY m.hadm_id
)
SELECT
  hadm_id,
  pick.charttime   AS ed_first_vitals_time,
  pick.temperature AS ed_first_temp,
  pick.heartrate   AS ed_first_hr,
  pick.resprate    AS ed_first_rr,
  pick.o2sat       AS ed_first_o2sat,
  pick.sbp         AS ed_first_sbp,
  pick.dbp         AS ed_first_dbp,
  pick.rhythm      AS ed_first_rhythm,
  pick.pain        AS ed_first_pain
FROM first_vs
"""
ed_first = run_sql_bq(sql("ed_first_vitals_sql"), {"hadms": hadm_list})
print("ED first vitals rows:", len(ed_first))


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# ED triage\nSQL["ed_triage_sql"] = f"""\nWITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x),\nedmap AS (\n  SELECT stay_id, hadm_id, intime\n  FROM `{PHYS}.{ED}.edstays`\n  WHERE hadm_id IN (SELECT hadm_id FROM hadms)\n),\ntri AS (\n  SELECT stay_id, temperature, heartrate, resprate, o2sat, sbp, dbp, pain, acuity, chiefcomplaint\n  FROM `{PHYS}.{ED}.triage`\n),\ntri_by_stay AS (\n  SELECT m.hadm_id, m.intime, t.*\n  FROM edmap m\n  JOIN tri t USING (stay_id)\n),\ntri_by_hadm AS (\n  SELECT\n    hadm_id,\n    (ARRAY_AGG(STRUCT(intime, temperature, heartrate, resprate, o2sat, sbp, dbp, pain, acuity, chiefcomplaint)\n               ORDER BY intime LIMIT 1))[OFFSET(0)] AS pick\n  FROM tri_by_stay\n  GROUP BY hadm_id\n)\nSELECT\n  hadm_id,\n  pick.temperature    AS ed_triage_temp,\n  pick.heartrate      AS ed_triage_hr,\n  pick.resprate       AS ed_triage_rr,\n  pick.o2sat          AS ed_triage_o2sat,\n  pick.sbp            AS ed_triage_sbp,\n  pick.dbp            AS ed_triage_dbp,\n  pick.pain           AS ed_triage_pain,\n  pick.acuity         AS ed_triage_acuity,\n  pick.chiefcomplaint AS ed_triage_cc\nFROM tri_by_hadm\n"""\ned_triage = run_sql_bq(sql("ed_triage_sql"), {"hadms": hadm_list})\nprint("ED triage rows:", len(ed_triage))\n\n# First ED vitals\nSQL["ed_first_vitals_sql"] = f"""\nWITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x),\nedmap AS (\n  SELECT stay_id, hadm_id\n  FROM `{PHYS}.{ED}.edstays`\n  WHERE hadm_id IN (SELECT hadm_id FROM hadms)\n),\nvs AS (\n  SELECT stay_id, charttime, temperature, heartrate, resprate, o2sat, sbp, dbp, rhythm, pain\n  FROM `{PHYS}.{ED}.vitalsign`\n),\nfirst_vs AS (\n  SELECT\n    m.hadm_id,\n    (ARRAY_AGG(STRUCT(v.charttime, v.temperature, v.heartrate, v.resprate, v.o2sat, v.sbp, v.dbp, v.rhythm, v.pain)\n               ORDER BY v.charttime LIMIT 1))[OFFSET(0)] AS pick\n  FROM edmap m JOIN vs v USING (stay_id)\n  GROUP BY m.hadm_id\n)\nSELECT\n  hadm_id,\n  pick.charttime   AS ed_first_vitals_time,\n  pick.temperature AS ed_first_temp,\n  pick.heartrate   AS ed_first_hr,\n  pick.resprate    AS ed_first_rr,\n  pick.o2sat       AS ed_first_o2sat,\n  pick.sbp         AS ed_first_sbp,\n  pick.dbp         AS ed_first_dbp,\n  pick.rhythm      AS ed_first_rhythm,\n  pick.pain        AS ed_first_pain\nFROM first_vs\n"""\ned_first = run_sql_bq(sql("ed_first_vitals_sql"), {"hadms": hadm_list})\nprint("ED first vitals rows:", len(ed_first))\n\n', 'execution_count': 12}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ED triage rows: 17424\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ED first vitals rows: 16746\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 35
[NbConvertApp] Skipping non-executing cell 36
[NbConvertApp] Executing cell:
SQL["icu_meta_sql"] = f"""
WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x),
first_icu AS (
  SELECT
    hadm_id,
    (ARRAY_AGG(STRUCT(stay_id, intime, outtime) ORDER BY intime LIMIT 1))[OFFSET(0)] AS s
  FROM `{PHYS}.{ICU}.icustays`
  WHERE hadm_id IN (SELECT hadm_id FROM hadms)
  GROUP BY hadm_id
)
SELECT
  hadm_id,
  s.stay_id AS first_icu_stay_id,
  s.intime  AS icu_intime,
  s.outtime AS icu_outtime,
  TIMESTAMP_DIFF(s.outtime, s.intime, HOUR)/24.0 AS icu_los_days
FROM first_icu
"""
icu_meta = run_sql_bq(sql("icu_meta_sql"), {"hadms": hadm_list})
print("ICU meta rows:", len(icu_meta))


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': 'SQL["icu_meta_sql"] = f"""\nWITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x),\nfirst_icu AS (\n  SELECT\n    hadm_id,\n    (ARRAY_AGG(STRUCT(stay_id, intime, outtime) ORDER BY intime LIMIT 1))[OFFSET(0)] AS s\n  FROM `{PHYS}.{ICU}.icustays`\n  WHERE hadm_id IN (SELECT hadm_id FROM hadms)\n  GROUP BY hadm_id\n)\nSELECT\n  hadm_id,\n  s.stay_id AS first_icu_stay_id,\n  s.intime  AS icu_intime,\n  s.outtime AS icu_outtime,\n  TIMESTAMP_DIFF(s.outtime, s.intime, HOUR)/24.0 AS icu_los_days\nFROM first_icu\n"""\nicu_meta = run_sql_bq(sql("icu_meta_sql"), {"hadms": hadm_list})\nprint("ICU meta rows:", len(icu_meta))\n\n', 'execution_count': 13}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ICU meta rows: 26059\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 38
[NbConvertApp] Skipping non-executing cell 39
[NbConvertApp] Executing cell:
# Cheaper ventilation flags: yes/no for IMV and NIV (ICD-9/ICD-10), plus any_vent_flag
SQL["vent_sql"] = f"""
WITH hadms AS (
  SELECT x AS hadm_id
  FROM UNNEST(@hadms) AS x
),

-- Restrict to admissions of interest early to minimize CPU
proc AS (
  SELECT hadm_id, icd_version, icd_code
  FROM `{PHYS}.{HOSP}.procedures_icd`
  JOIN hadms USING (hadm_id)
),

-- ICU Invasive Mechanical Ventilation (IMV)
-- ICD-10-PCS (usually stored without a dot)
imv10 AS (
  SELECT DISTINCT hadm_id
  FROM proc
  WHERE icd_version = 10
    AND icd_code IN ('5A1935Z','5A1945Z','5A1955Z','0BH17EZ','0BH18EZ')
),

-- ICD-9-CM procedures (stored sometimes with dot, sometimes without)
imv9 AS (
  SELECT DISTINCT hadm_id
  FROM proc
  WHERE icd_version = 9
    AND (
          icd_code IN ('96.70','96.71','96.72','96.04')  -- dotted forms
          OR REPLACE(icd_code, '.', '') IN ('9670','9671','9672','9604') -- dotless match
        )
),

-- Noninvasive Ventilation (NIV)
-- ICD-10-PCS (no dot)
niv10 AS (
  SELECT DISTINCT hadm_id
  FROM proc
  WHERE icd_version = 10
    AND icd_code IN ('5A09357','5A09457','5A09557')
),

-- ICD-9-CM (with/without dot)
niv9 AS (
  SELECT DISTINCT hadm_id
  FROM proc
  WHERE icd_version = 9
    AND (
          icd_code IN ('93.90','93.91','93.99')            -- dotted
          OR REPLACE(icd_code, '.', '') IN ('9390','9391','9399')  -- dotless
        )
)

SELECT
  h.hadm_id,
  IF(i10.hadm_id IS NOT NULL OR i9.hadm_id IS NOT NULL, 1, 0) AS imv_flag,
  IF(n10.hadm_id IS NOT NULL OR n9.hadm_id IS NOT NULL, 1, 0) AS niv_flag,
  -- any_vent = either IMV or NIV
  IF(
    (i10.hadm_id IS NOT NULL OR i9.hadm_id IS NOT NULL)
    OR (n10.hadm_id IS NOT NULL OR n9.hadm_id IS NOT NULL),
    1, 0
  ) AS any_vent_flag
FROM hadms h
LEFT JOIN imv10 i10 ON h.hadm_id = i10.hadm_id
LEFT JOIN imv9  i9  ON h.hadm_id = i9.hadm_id
LEFT JOIN niv10 n10 ON h.hadm_id = n10.hadm_id
LEFT JOIN niv9  n9  ON h.hadm_id = n9.hadm_id
"""

vent = run_sql_bq(sql("vent_sql"), {"hadms": hadm_list})
print("Vent rows:", len(vent))

[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Cheaper ventilation flags: yes/no for IMV and NIV (ICD-9/ICD-10), plus any_vent_flag\nSQL["vent_sql"] = f"""\nWITH hadms AS (\n  SELECT x AS hadm_id\n  FROM UNNEST(@hadms) AS x\n),\n\n-- Restrict to admissions of interest early to minimize CPU\nproc AS (\n  SELECT hadm_id, icd_version, icd_code\n  FROM `{PHYS}.{HOSP}.procedures_icd`\n  JOIN hadms USING (hadm_id)\n),\n\n-- ICU Invasive Mechanical Ventilation (IMV)\n-- ICD-10-PCS (usually stored without a dot)\nimv10 AS (\n  SELECT DISTINCT hadm_id\n  FROM proc\n  WHERE icd_version = 10\n    AND icd_code IN (\'5A1935Z\',\'5A1945Z\',\'5A1955Z\',\'0BH17EZ\',\'0BH18EZ\')\n),\n\n-- ICD-9-CM procedures (stored sometimes with dot, sometimes without)\nimv9 AS (\n  SELECT DISTINCT hadm_id\n  FROM proc\n  WHERE icd_version = 9\n    AND (\n          icd_code IN (\'96.70\',\'96.71\',\'96.72\',\'96.04\')  -- dotted forms\n          OR REPLACE(icd_code, \'.\', \'\') IN (\'9670\',\'9671\',\'9672\',\'9604\') -- dotless match\n        )\n),\n\n-- Noninvasive Ventilation (NIV)\n-- ICD-10-PCS (no dot)\nniv10 AS (\n  SELECT DISTINCT hadm_id\n  FROM proc\n  WHERE icd_version = 10\n    AND icd_code IN (\'5A09357\',\'5A09457\',\'5A09557\')\n),\n\n-- ICD-9-CM (with/without dot)\nniv9 AS (\n  SELECT DISTINCT hadm_id\n  FROM proc\n  WHERE icd_version = 9\n    AND (\n          icd_code IN (\'93.90\',\'93.91\',\'93.99\')            -- dotted\n          OR REPLACE(icd_code, \'.\', \'\') IN (\'9390\',\'9391\',\'9399\')  -- dotless\n        )\n)\n\nSELECT\n  h.hadm_id,\n  IF(i10.hadm_id IS NOT NULL OR i9.hadm_id IS NOT NULL, 1, 0) AS imv_flag,\n  IF(n10.hadm_id IS NOT NULL OR n9.hadm_id IS NOT NULL, 1, 0) AS niv_flag,\n  -- any_vent = either IMV or NIV\n  IF(\n    (i10.hadm_id IS NOT NULL OR i9.hadm_id IS NOT NULL)\n    OR (n10.hadm_id IS NOT NULL OR n9.hadm_id IS NOT NULL),\n    1, 0\n  ) AS any_vent_flag\nFROM hadms h\nLEFT JOIN imv10 i10 ON h.hadm_id = i10.hadm_id\nLEFT JOIN imv9  i9  ON h.hadm_id = i9.hadm_id\nLEFT JOIN niv10 n10 ON h.hadm_id = n10.hadm_id\nLEFT JOIN niv9  n9  ON h.hadm_id = n9.hadm_id\n"""\n\nvent = run_sql_bq(sql("vent_sql"), {"hadms": hadm_list})\nprint("Vent rows:", len(vent))\n', 'execution_count': 14}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Vent rows: 40151\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 41
[NbConvertApp] Executing cell:
SQL["vent_chart_sql"] = f"""
WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x),

stays AS (
  SELECT hadm_id, stay_id
  FROM `{PHYS}.{ICU}.icustays`
  WHERE hadm_id IN (SELECT hadm_id FROM hadms)
),

cand AS (
  SELECT
    s.hadm_id,
    ce.stay_id,
    ce.charttime,
    LOWER(di.label) AS lbl,
    LOWER(COALESCE(ce.value,'')) AS valstr
  FROM `{PHYS}.{ICU}.chartevents` ce
  JOIN `{PHYS}.{ICU}.d_items`    di ON di.itemid = ce.itemid
  JOIN stays s ON s.stay_id = ce.stay_id
  WHERE
    (
      REGEXP_CONTAINS(LOWER(di.label), r'(vent|ventilator|mode|bipap|bi[- ]?pap|cpap|nippv|niv|mask|ett|endotracheal)')
      OR REGEXP_CONTAINS(LOWER(ce.value), r'(bipap|bi[- ]?pap|cpap|nippv|niv|endotracheal|ett|ac/|simv|prvc|aprv|pcv|vcv|assist\s*control)')
    )
),

flags AS (
  SELECT
    hadm_id,
    MIN(IF(
          REGEXP_CONTAINS(lbl,   r'(non[- ]?invasive|niv|nippv|bipap|bi[- ]?pap|cpap)')
          OR REGEXP_CONTAINS(valstr,r'(non[- ]?invasive|niv|nippv|bipap|bi[- ]?pap|cpap)'),
          charttime, NULL)) AS first_niv_time,
    MIN(IF(
          REGEXP_CONTAINS(lbl,   r'(invasive ventilation|endotracheal|ett|mech(|anical)? vent|ventilator mode|ac/|simv|prvc|aprv|pcv|vcv|assist\s*control)')
          OR REGEXP_CONTAINS(valstr,r'(invasive ventilation|endotracheal|ett|ac/|simv|prvc|aprv|pcv|vcv|assist\s*control)'),
          charttime, NULL)) AS first_imv_time,
    MAX( CASE
            WHEN REGEXP_CONTAINS(lbl,   r'(non[- ]?invasive|niv|nippv|bipap|bi[- ]?pap|cpap)')
              OR REGEXP_CONTAINS(valstr,r'(non[- ]?invasive|niv|nippv|bipap|bi[- ]?pap|cpap)')
            THEN 1 ELSE 0
        END ) AS niv_chart_flag,
    MAX( CASE
            WHEN REGEXP_CONTAINS(lbl,   r'(invasive ventilation|endotracheal|ett|mech(|anical)? vent|ventilator mode|ac/|simv|prvc|aprv|pcv|vcv|assist\s*control)')
              OR REGEXP_CONTAINS(valstr,r'(invasive ventilation|endotracheal|ett|ac/|simv|prvc|aprv|pcv|vcv|assist\s*control)')
            THEN 1 ELSE 0
        END ) AS imv_chart_flag
  FROM cand
  GROUP BY hadm_id
)

SELECT hadm_id, niv_chart_flag, imv_chart_flag, first_niv_time, first_imv_time
FROM flags
"""
vent_chart = run_sql_bq(sql("vent_chart_sql"), {"hadms": hadm_list})


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': 'SQL["vent_chart_sql"] = f"""\nWITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x),\n\nstays AS (\n  SELECT hadm_id, stay_id\n  FROM `{PHYS}.{ICU}.icustays`\n  WHERE hadm_id IN (SELECT hadm_id FROM hadms)\n),\n\ncand AS (\n  SELECT\n    s.hadm_id,\n    ce.stay_id,\n    ce.charttime,\n    LOWER(di.label) AS lbl,\n    LOWER(COALESCE(ce.value,\'\')) AS valstr\n  FROM `{PHYS}.{ICU}.chartevents` ce\n  JOIN `{PHYS}.{ICU}.d_items`    di ON di.itemid = ce.itemid\n  JOIN stays s ON s.stay_id = ce.stay_id\n  WHERE\n    (\n      REGEXP_CONTAINS(LOWER(di.label), r\'(vent|ventilator|mode|bipap|bi[- ]?pap|cpap|nippv|niv|mask|ett|endotracheal)\')\n      OR REGEXP_CONTAINS(LOWER(ce.value), r\'(bipap|bi[- ]?pap|cpap|nippv|niv|endotracheal|ett|ac/|simv|prvc|aprv|pcv|vcv|assist\\s*control)\')\n    )\n),\n\nflags AS (\n  SELECT\n    hadm_id,\n    MIN(IF(\n          REGEXP_CONTAINS(lbl,   r\'(non[- ]?invasive|niv|nippv|bipap|bi[- ]?pap|cpap)\')\n          OR REGEXP_CONTAINS(valstr,r\'(non[- ]?invasive|niv|nippv|bipap|bi[- ]?pap|cpap)\'),\n          charttime, NULL)) AS first_niv_time,\n    MIN(IF(\n          REGEXP_CONTAINS(lbl,   r\'(invasive ventilation|endotracheal|ett|mech(|anical)? vent|ventilator mode|ac/|simv|prvc|aprv|pcv|vcv|assist\\s*control)\')\n          OR REGEXP_CONTAINS(valstr,r\'(invasive ventilation|endotracheal|ett|ac/|simv|prvc|aprv|pcv|vcv|assist\\s*control)\'),\n          charttime, NULL)) AS first_imv_time,\n    MAX( CASE\n            WHEN REGEXP_CONTAINS(lbl,   r\'(non[- ]?invasive|niv|nippv|bipap|bi[- ]?pap|cpap)\')\n              OR REGEXP_CONTAINS(valstr,r\'(non[- ]?invasive|niv|nippv|bipap|bi[- ]?pap|cpap)\')\n            THEN 1 ELSE 0\n        END ) AS niv_chart_flag,\n    MAX( CASE\n            WHEN REGEXP_CONTAINS(lbl,   r\'(invasive ventilation|endotracheal|ett|mech(|anical)? vent|ventilator mode|ac/|simv|prvc|aprv|pcv|vcv|assist\\s*control)\')\n              OR REGEXP_CONTAINS(valstr,r\'(invasive ventilation|endotracheal|ett|ac/|simv|prvc|aprv|pcv|vcv|assist\\s*control)\')\n            THEN 1 ELSE 0\n        END ) AS imv_chart_flag\n  FROM cand\n  GROUP BY hadm_id\n)\n\nSELECT hadm_id, niv_chart_flag, imv_chart_flag, first_niv_time, first_imv_time\nFROM flags\n"""\nvent_chart = run_sql_bq(sql("vent_chart_sql"), {"hadms": hadm_list})\n\n', 'execution_count': 15}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
# If your existing ICD-only result is called `vent`, rename for clarity:
vent_proc = vent.copy()

# Outer merge so we keep hadm_ids that appear in only one source
vent_combined = vent_proc.merge(vent_chart, on="hadm_id", how="outer")

# Fill missing with 0 before taking maxima
for c in ["imv_flag","niv_flag","any_vent_flag","imv_chart_flag","niv_chart_flag"]:
    if c in vent_combined.columns:
        vent_combined[c] = vent_combined[c].fillna(0).astype("Int64")

# Final "any-source" flags
vent_combined["imv_flag"]       = vent_combined[["imv_flag","imv_chart_flag"]].max(axis=1).astype("Int64")
vent_combined["niv_flag"]       = vent_combined[["niv_flag","niv_chart_flag"]].max(axis=1).astype("Int64")
vent_combined["any_vent_flag"]  = vent_combined[["imv_flag","niv_flag"]].max(axis=1).astype("Int64")

vent_combined = vent_combined[["hadm_id","imv_flag","niv_flag","any_vent_flag","first_imv_time","first_niv_time"]]
print("After combining ICD + chart signals:",
      "\nIMV=1:", int((vent_combined["imv_flag"]==1).sum()),
      "\nNIV=1:", int((vent_combined["niv_flag"]==1).sum()))


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# If your existing ICD-only result is called `vent`, rename for clarity:\nvent_proc = vent.copy()\n\n# Outer merge so we keep hadm_ids that appear in only one source\nvent_combined = vent_proc.merge(vent_chart, on="hadm_id", how="outer")\n\n# Fill missing with 0 before taking maxima\nfor c in ["imv_flag","niv_flag","any_vent_flag","imv_chart_flag","niv_chart_flag"]:\n    if c in vent_combined.columns:\n        vent_combined[c] = vent_combined[c].fillna(0).astype("Int64")\n\n# Final "any-source" flags\nvent_combined["imv_flag"]       = vent_combined[["imv_flag","imv_chart_flag"]].max(axis=1).astype("Int64")\nvent_combined["niv_flag"]       = vent_combined[["niv_flag","niv_chart_flag"]].max(axis=1).astype("Int64")\nvent_combined["any_vent_flag"]  = vent_combined[["imv_flag","niv_flag"]].max(axis=1).astype("Int64")\n\nvent_combined = vent_combined[["hadm_id","imv_flag","niv_flag","any_vent_flag","first_imv_time","first_niv_time"]]\nprint("After combining ICD + chart signals:",\n      "\\nIMV=1:", int((vent_combined["imv_flag"]==1).sum()),\n      "\\nNIV=1:", int((vent_combined["niv_flag"]==1).sum()))\n\n', 'execution_count': 16}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'After combining ICD + chart signals: \nIMV=1: 23655 \nNIV=1: 10540\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 44
[NbConvertApp] Skipping non-executing cell 45
[NbConvertApp] Executing cell:
# Canonical base (carries authoritative subject_id)
df = demo.copy()

# Cohort flags / thresholds / labs / etc.
df = safe_merge_on_hadm(df, cohort_any, right_name="cohort_any", check_subject="warn")
df = safe_merge_on_hadm(df, bg_pairs,   right_name="bg_pairs")
df = safe_merge_on_hadm(df, race_eth,   right_name="race_eth")
df = safe_merge_on_hadm(df, ed_triage,  right_name="ed_triage")
df = safe_merge_on_hadm(df, ed_first,   right_name="ed_first")
df = safe_merge_on_hadm(df, icu_meta,   right_name="icu_meta")
df = safe_merge_on_hadm(df, vent_combined,       right_name="vent_combined")

# Anchor to first ED presentation (per admission)
if "ed_intime_first" in globals():
    df = safe_merge_on_hadm(df, ed_intime_first, right_name="ed_intime_first")

# Derived timing: first NIV/IMV relative to ED presentation
if "ed_intime_first" in df.columns and "first_imv_time" in df.columns:
    df["dt_first_imv_hours"] = (df["first_imv_time"] - df["ed_intime_first"]).dt.total_seconds() / 3600.0
if "ed_intime_first" in df.columns and "first_niv_time" in df.columns:
    df["dt_first_niv_hours"] = (df["first_niv_time"] - df["ed_intime_first"]).dt.total_seconds() / 3600.0

# ABG/VBG before IMV (hadm-level)
if {"first_abg_time", "first_imv_time"}.issubset(df.columns):
    df["abg_before_imv"] = (
        df["first_abg_time"].notna() & df["first_imv_time"].notna() &
        (df["first_abg_time"] < df["first_imv_time"])
    ).astype("Int64")
if {"first_vbg_time", "first_imv_time"}.issubset(df.columns):
    df["vbg_before_imv"] = (
        df["first_vbg_time"].notna() & df["first_imv_time"].notna() &
        (df["first_vbg_time"] < df["first_imv_time"])
    ).astype("Int64")

print("Final df rows:", len(df), "cols:", len(df.columns))

# Safety checks
assert "subject_id" in df.columns, "subject_id missing from final df"
assert not any(c.endswith("_x") or c.endswith("_y") for c in df.columns), "Found suffixed columns"
print("Final df rows:", len(df), "cols:", len(df.columns))
df.head(3)


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Canonical base (carries authoritative subject_id)\ndf = demo.copy()\n\n# Cohort flags / thresholds / labs / etc.\ndf = safe_merge_on_hadm(df, cohort_any, right_name="cohort_any", check_subject="warn")\ndf = safe_merge_on_hadm(df, bg_pairs,   right_name="bg_pairs")\ndf = safe_merge_on_hadm(df, race_eth,   right_name="race_eth")\ndf = safe_merge_on_hadm(df, ed_triage,  right_name="ed_triage")\ndf = safe_merge_on_hadm(df, ed_first,   right_name="ed_first")\ndf = safe_merge_on_hadm(df, icu_meta,   right_name="icu_meta")\ndf = safe_merge_on_hadm(df, vent_combined,       right_name="vent_combined")\n\n# Anchor to first ED presentation (per admission)\nif "ed_intime_first" in globals():\n    df = safe_merge_on_hadm(df, ed_intime_first, right_name="ed_intime_first")\n\n# Derived timing: first NIV/IMV relative to ED presentation\nif "ed_intime_first" in df.columns and "first_imv_time" in df.columns:\n    df["dt_first_imv_hours"] = (df["first_imv_time"] - df["ed_intime_first"]).dt.total_seconds() / 3600.0\nif "ed_intime_first" in df.columns and "first_niv_time" in df.columns:\n    df["dt_first_niv_hours"] = (df["first_niv_time"] - df["ed_intime_first"]).dt.total_seconds() / 3600.0\n\n# ABG/VBG before IMV (hadm-level)\nif {"first_abg_time", "first_imv_time"}.issubset(df.columns):\n    df["abg_before_imv"] = (\n        df["first_abg_time"].notna() & df["first_imv_time"].notna() &\n        (df["first_abg_time"] < df["first_imv_time"])\n    ).astype("Int64")\nif {"first_vbg_time", "first_imv_time"}.issubset(df.columns):\n    df["vbg_before_imv"] = (\n        df["first_vbg_time"].notna() & df["first_imv_time"].notna() &\n        (df["first_vbg_time"] < df["first_imv_time"])\n    ).astype("Int64")\n\nprint("Final df rows:", len(df), "cols:", len(df.columns))\n\n# Safety checks\nassert "subject_id" in df.columns, "subject_id missing from final df"\nassert not any(c.endswith("_x") or c.endswith("_y") for c in df.columns), "Found suffixed columns"\nprint("Final df rows:", len(df), "cols:", len(df.columns))\ndf.head(3)\n\n', 'execution_count': 17}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Final df rows: 40151 cols: 89\nFinal df rows: 40151 cols: 89\n'}
[NbConvertApp] msg_type: execute_result
[NbConvertApp] content: {'data': {'text/plain': '    hadm_id  subject_id           admittime           dischtime  \\\n0  23485217    10584718 2165-02-12 15:41:00 2165-03-06 08:20:00   \n1  22661627    10032409 2130-01-12 18:42:00 2130-01-21 14:32:00   \n2  22447711    13352386 2188-03-25 01:17:00 2188-03-26 16:00:00   \n\n            deathtime admission_type                      admission_location  \\\n0 2165-03-06 08:20:00       EW EMER.  TRANSFER FROM SKILLED NURSING FACILITY   \n1                 NaT       EW EMER.                          EMERGENCY ROOM   \n2                 NaT       EW EMER.                          EMERGENCY ROOM   \n\n         discharge_location insurance  hosp_los_days  ...          icu_intime  \\\n0                      DIED  Medicare      21.708333  ... 2165-02-27 21:41:10   \n1  SKILLED NURSING FACILITY  Medicare       8.833333  ...                 NaT   \n2                      HOME   Private       1.625000  ...                 NaT   \n\n          icu_outtime  icu_los_days  imv_flag  niv_flag  any_vent_flag  \\\n0 2165-03-06 11:09:58      6.583333         1         0              1   \n1                 NaT           NaN         0         0              0   \n2                 NaT           NaN         0         0              0   \n\n       first_imv_time  first_niv_time  abg_before_imv  vbg_before_imv  \n0 2165-02-27 22:00:00             NaT               0               1  \n1                 NaT             NaT               0               0  \n2                 NaT             NaT               0               0  \n\n[3 rows x 89 columns]', 'text/html': '<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>hadm_id</th>\n      <th>subject_id</th>\n      <th>admittime</th>\n      <th>dischtime</th>\n      <th>deathtime</th>\n      <th>admission_type</th>\n      <th>admission_location</th>\n      <th>discharge_location</th>\n      <th>insurance</th>\n      <th>hosp_los_days</th>\n      <th>...</th>\n      <th>icu_intime</th>\n      <th>icu_outtime</th>\n      <th>icu_los_days</th>\n      <th>imv_flag</th>\n      <th>niv_flag</th>\n      <th>any_vent_flag</th>\n      <th>first_imv_time</th>\n      <th>first_niv_time</th>\n      <th>abg_before_imv</th>\n      <th>vbg_before_imv</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>23485217</td>\n      <td>10584718</td>\n      <td>2165-02-12 15:41:00</td>\n      <td>2165-03-06 08:20:00</td>\n      <td>2165-03-06 08:20:00</td>\n      <td>EW EMER.</td>\n      <td>TRANSFER FROM SKILLED NURSING FACILITY</td>\n      <td>DIED</td>\n      <td>Medicare</td>\n      <td>21.708333</td>\n      <td>...</td>\n      <td>2165-02-27 21:41:10</td>\n      <td>2165-03-06 11:09:58</td>\n      <td>6.583333</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2165-02-27 22:00:00</td>\n      <td>NaT</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>22661627</td>\n      <td>10032409</td>\n      <td>2130-01-12 18:42:00</td>\n      <td>2130-01-21 14:32:00</td>\n      <td>NaT</td>\n      <td>EW EMER.</td>\n      <td>EMERGENCY ROOM</td>\n      <td>SKILLED NURSING FACILITY</td>\n      <td>Medicare</td>\n      <td>8.833333</td>\n      <td>...</td>\n      <td>NaT</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaT</td>\n      <td>NaT</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>22447711</td>\n      <td>13352386</td>\n      <td>2188-03-25 01:17:00</td>\n      <td>2188-03-26 16:00:00</td>\n      <td>NaT</td>\n      <td>EW EMER.</td>\n      <td>EMERGENCY ROOM</td>\n      <td>HOME</td>\n      <td>Private</td>\n      <td>1.625000</td>\n      <td>...</td>\n      <td>NaT</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaT</td>\n      <td>NaT</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 89 columns</p>\n</div>'}, 'metadata': {}, 'execution_count': 17}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
# --- Cohort flow counts (ED / ICU / blood gas / hypercapnia / CC) ---

# 1) Dataset-level ED counts
SQL["ed_counts_sql"] = f"""
SELECT
  COUNT(*) AS total_ed_encounters,
  COUNTIF(hadm_id IS NOT NULL) AS ed_encounters_with_hadm
FROM `{PHYS}.{ED}.edstays`
"""

SQL["ed_to_icu_sql"] = f"""
SELECT
  COUNT(DISTINCT e.hadm_id) AS ed_to_icu_hadm,
  COUNT(DISTINCT e.stay_id) AS ed_to_icu_edstays
FROM `{PHYS}.{ED}.edstays` e
JOIN `{PHYS}.{ICU}.icustays` i USING (hadm_id)
WHERE e.hadm_id IS NOT NULL
"""

try:
    ed_counts = run_sql_bq(sql("ed_counts_sql"))
    ed_to_icu = run_sql_bq(sql("ed_to_icu_sql"))
except Exception as e:
    print("Warning: ED/ICU counts query failed:", e)
    ed_counts = None
    ed_to_icu = None

# 2) Cohort-level counts (admission-level)
cohort_union = int((cohort_any["enrolled_any"] == 1).sum()) if "cohort_any" in globals() and "enrolled_any" in cohort_any.columns else len(hadm_list)
cohort_df_n = len(df)

# Any blood gas present (ABG/VBG, LAB/POC)
co2_cols = [c for c in [
    "lab_abg_paco2", "lab_vbg_paco2", "poc_abg_paco2", "poc_vbg_paco2"
] if c in df.columns]
any_bg = int(df[co2_cols].notna().any(axis=1).sum()) if co2_cols else None

# Hypercapnia thresholds and ICD
icd_count = int((df["any_hypercap_icd"] == 1).sum()) if "any_hypercap_icd" in df.columns else None
threshold_count = int((df["pco2_threshold_any"] == 1).sum()) if "pco2_threshold_any" in df.columns else None

# ED chief complaint missing / present (within cohort)
if "ed_triage_cc" in df.columns:
    mask_cc_present = df["ed_triage_cc"].notna() & (df["ed_triage_cc"].astype(str).str.strip() != "")
    cc_present = int(mask_cc_present.sum())
    cc_missing = int((~mask_cc_present).sum())
else:
    cc_present = None
    cc_missing = None

# ED→ICU within cohort (admissions with ED triage data and ICU stay)
if "first_icu_stay_id" in df.columns and "ed_triage_cc" in df.columns:
    cohort_ed_to_icu = int((df["first_icu_stay_id"].notna() & mask_cc_present).sum())
else:
    cohort_ed_to_icu = None

rows = []
if ed_counts is not None:
    rows.append({"step": "Total ED encounters (edstays)", "count": int(ed_counts.loc[0, "total_ed_encounters"]), "scope": "All ED dataset"})
    rows.append({"step": "ED encounters with hadm_id", "count": int(ed_counts.loc[0, "ed_encounters_with_hadm"]), "scope": "All ED dataset"})
if ed_to_icu is not None:
    rows.append({"step": "ED→ICU admissions (distinct hadm_id)", "count": int(ed_to_icu.loc[0, "ed_to_icu_hadm"]), "scope": "All ED+ICU"})
    rows.append({"step": "ED→ICU ED-stays (distinct stay_id)", "count": int(ed_to_icu.loc[0, "ed_to_icu_edstays"]), "scope": "All ED+ICU"})

rows.append({"step": "Cohort admissions (union ICD ∪ thresholds)", "count": cohort_union, "scope": "Cohort"})
rows.append({"step": "Cohort admissions after merges (df rows)", "count": cohort_df_n, "scope": "Cohort"})
if any_bg is not None:
    rows.append({"step": "Cohort with any ABG/VBG (LAB or POC)", "count": any_bg, "scope": "Cohort"})
if threshold_count is not None:
    rows.append({"step": "Cohort meeting hypercapnia thresholds", "count": threshold_count, "scope": "Cohort"})
if icd_count is not None:
    rows.append({"step": "Cohort meeting ICD code criteria", "count": icd_count, "scope": "Cohort"})
if cc_present is not None:
    rows.append({"step": "Cohort with ED chief complaint present", "count": cc_present, "scope": "Cohort"})
if cc_missing is not None:
    rows.append({"step": "Cohort excluded for missing ED chief complaint", "count": cc_missing, "scope": "Cohort"})
if cohort_ed_to_icu is not None:
    rows.append({"step": "Cohort ED→ICU (ED CC present + ICU stay)", "count": cohort_ed_to_icu, "scope": "Cohort"})

flow_counts = pd.DataFrame(rows)
flow_counts


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# --- Cohort flow counts (ED / ICU / blood gas / hypercapnia / CC) ---\n\n# 1) Dataset-level ED counts\nSQL["ed_counts_sql"] = f"""\nSELECT\n  COUNT(*) AS total_ed_encounters,\n  COUNTIF(hadm_id IS NOT NULL) AS ed_encounters_with_hadm\nFROM `{PHYS}.{ED}.edstays`\n"""\n\nSQL["ed_to_icu_sql"] = f"""\nSELECT\n  COUNT(DISTINCT e.hadm_id) AS ed_to_icu_hadm,\n  COUNT(DISTINCT e.stay_id) AS ed_to_icu_edstays\nFROM `{PHYS}.{ED}.edstays` e\nJOIN `{PHYS}.{ICU}.icustays` i USING (hadm_id)\nWHERE e.hadm_id IS NOT NULL\n"""\n\ntry:\n    ed_counts = run_sql_bq(sql("ed_counts_sql"))\n    ed_to_icu = run_sql_bq(sql("ed_to_icu_sql"))\nexcept Exception as e:\n    print("Warning: ED/ICU counts query failed:", e)\n    ed_counts = None\n    ed_to_icu = None\n\n# 2) Cohort-level counts (admission-level)\ncohort_union = int((cohort_any["enrolled_any"] == 1).sum()) if "cohort_any" in globals() and "enrolled_any" in cohort_any.columns else len(hadm_list)\ncohort_df_n = len(df)\n\n# Any blood gas present (ABG/VBG, LAB/POC)\nco2_cols = [c for c in [\n    "lab_abg_paco2", "lab_vbg_paco2", "poc_abg_paco2", "poc_vbg_paco2"\n] if c in df.columns]\nany_bg = int(df[co2_cols].notna().any(axis=1).sum()) if co2_cols else None\n\n# Hypercapnia thresholds and ICD\nicd_count = int((df["any_hypercap_icd"] == 1).sum()) if "any_hypercap_icd" in df.columns else None\nthreshold_count = int((df["pco2_threshold_any"] == 1).sum()) if "pco2_threshold_any" in df.columns else None\n\n# ED chief complaint missing / present (within cohort)\nif "ed_triage_cc" in df.columns:\n    mask_cc_present = df["ed_triage_cc"].notna() & (df["ed_triage_cc"].astype(str).str.strip() != "")\n    cc_present = int(mask_cc_present.sum())\n    cc_missing = int((~mask_cc_present).sum())\nelse:\n    cc_present = None\n    cc_missing = None\n\n# ED→ICU within cohort (admissions with ED triage data and ICU stay)\nif "first_icu_stay_id" in df.columns and "ed_triage_cc" in df.columns:\n    cohort_ed_to_icu = int((df["first_icu_stay_id"].notna() & mask_cc_present).sum())\nelse:\n    cohort_ed_to_icu = None\n\nrows = []\nif ed_counts is not None:\n    rows.append({"step": "Total ED encounters (edstays)", "count": int(ed_counts.loc[0, "total_ed_encounters"]), "scope": "All ED dataset"})\n    rows.append({"step": "ED encounters with hadm_id", "count": int(ed_counts.loc[0, "ed_encounters_with_hadm"]), "scope": "All ED dataset"})\nif ed_to_icu is not None:\n    rows.append({"step": "ED→ICU admissions (distinct hadm_id)", "count": int(ed_to_icu.loc[0, "ed_to_icu_hadm"]), "scope": "All ED+ICU"})\n    rows.append({"step": "ED→ICU ED-stays (distinct stay_id)", "count": int(ed_to_icu.loc[0, "ed_to_icu_edstays"]), "scope": "All ED+ICU"})\n\nrows.append({"step": "Cohort admissions (union ICD ∪ thresholds)", "count": cohort_union, "scope": "Cohort"})\nrows.append({"step": "Cohort admissions after merges (df rows)", "count": cohort_df_n, "scope": "Cohort"})\nif any_bg is not None:\n    rows.append({"step": "Cohort with any ABG/VBG (LAB or POC)", "count": any_bg, "scope": "Cohort"})\nif threshold_count is not None:\n    rows.append({"step": "Cohort meeting hypercapnia thresholds", "count": threshold_count, "scope": "Cohort"})\nif icd_count is not None:\n    rows.append({"step": "Cohort meeting ICD code criteria", "count": icd_count, "scope": "Cohort"})\nif cc_present is not None:\n    rows.append({"step": "Cohort with ED chief complaint present", "count": cc_present, "scope": "Cohort"})\nif cc_missing is not None:\n    rows.append({"step": "Cohort excluded for missing ED chief complaint", "count": cc_missing, "scope": "Cohort"})\nif cohort_ed_to_icu is not None:\n    rows.append({"step": "Cohort ED→ICU (ED CC present + ICU stay)", "count": cohort_ed_to_icu, "scope": "Cohort"})\n\nflow_counts = pd.DataFrame(rows)\nflow_counts\n\n', 'execution_count': 18}
[NbConvertApp] msg_type: execute_result
[NbConvertApp] content: {'data': {'text/plain': '                                              step   count           scope\n0                    Total ED encounters (edstays)  425087  All ED dataset\n1                       ED encounters with hadm_id  203016  All ED dataset\n2             ED→ICU admissions (distinct hadm_id)   31862      All ED+ICU\n3               ED→ICU ED-stays (distinct stay_id)   31916      All ED+ICU\n4       Cohort admissions (union ICD ∪ thresholds)   40152          Cohort\n5         Cohort admissions after merges (df rows)   40151          Cohort\n6             Cohort with any ABG/VBG (LAB or POC)   37887          Cohort\n7            Cohort meeting hypercapnia thresholds   39016          Cohort\n8                 Cohort meeting ICD code criteria    4237          Cohort\n9           Cohort with ED chief complaint present   17424          Cohort\n10  Cohort excluded for missing ED chief complaint   22727          Cohort\n11        Cohort ED→ICU (ED CC present + ICU stay)   11542          Cohort', 'text/html': '<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>step</th>\n      <th>count</th>\n      <th>scope</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Total ED encounters (edstays)</td>\n      <td>425087</td>\n      <td>All ED dataset</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ED encounters with hadm_id</td>\n      <td>203016</td>\n      <td>All ED dataset</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ED→ICU admissions (distinct hadm_id)</td>\n      <td>31862</td>\n      <td>All ED+ICU</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ED→ICU ED-stays (distinct stay_id)</td>\n      <td>31916</td>\n      <td>All ED+ICU</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Cohort admissions (union ICD ∪ thresholds)</td>\n      <td>40152</td>\n      <td>Cohort</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Cohort admissions after merges (df rows)</td>\n      <td>40151</td>\n      <td>Cohort</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Cohort with any ABG/VBG (LAB or POC)</td>\n      <td>37887</td>\n      <td>Cohort</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Cohort meeting hypercapnia thresholds</td>\n      <td>39016</td>\n      <td>Cohort</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Cohort meeting ICD code criteria</td>\n      <td>4237</td>\n      <td>Cohort</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Cohort with ED chief complaint present</td>\n      <td>17424</td>\n      <td>Cohort</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Cohort excluded for missing ED chief complaint</td>\n      <td>22727</td>\n      <td>Cohort</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Cohort ED→ICU (ED CC present + ICU stay)</td>\n      <td>11542</td>\n      <td>Cohort</td>\n    </tr>\n  </tbody>\n</table>\n</div>'}, 'metadata': {}, 'execution_count': 18}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
# --- Ascertainment overlap counts (ABG/VBG/ICD) ---

required = ["abg_hypercap_threshold", "vbg_hypercap_threshold", "any_hypercap_icd"]
missing = [c for c in required if c not in df.columns]
if missing:
    raise KeyError(f"Missing required columns for overlap counts: {missing}")

abg = pd.to_numeric(df["abg_hypercap_threshold"], errors="coerce").fillna(0).astype(int)
vbg = pd.to_numeric(df["vbg_hypercap_threshold"], errors="coerce").fillna(0).astype(int)

gas_any = (
    pd.to_numeric(df.get("pco2_threshold_any", None), errors="coerce")
    if "pco2_threshold_any" in df.columns else (abg | vbg)
)
if hasattr(gas_any, "fillna"):
    gas_any = gas_any.fillna(0).astype(int)
else:
    gas_any = gas_any.astype(int)

icd = pd.to_numeric(df["any_hypercap_icd"], errors="coerce").fillna(0).astype(int)

total_n = len(df)
ngas = int((gas_any == 1).sum())

abg_vbg_overlap = pd.DataFrame([
    {"group": "ABG-only", "count": int(((abg==1) & (vbg==0)).sum())},
    {"group": "VBG-only", "count": int(((vbg==1) & (abg==0)).sum())},
    {"group": "ABG+VBG", "count": int(((abg==1) & (vbg==1)).sum())},
])
if ngas > 0:
    abg_vbg_overlap["pct_of_gas"] = (abg_vbg_overlap["count"] / ngas * 100).round(1)
else:
    abg_vbg_overlap["pct_of_gas"] = 0.0
abg_vbg_overlap["pct_of_cohort"] = (abg_vbg_overlap["count"] / max(total_n,1) * 100).round(1)

icd_gas_overlap = pd.DataFrame([
    {"group": "ICD+Gas", "count": int(((icd==1) & (gas_any==1)).sum())},
    {"group": "ICD-only", "count": int(((icd==1) & (gas_any==0)).sum())},
    {"group": "Gas-only", "count": int(((icd==0) & (gas_any==1)).sum())},
    {"group": "Neither", "count": int(((icd==0) & (gas_any==0)).sum())},
])
icd_gas_overlap["pct_of_cohort"] = (icd_gas_overlap["count"] / max(total_n,1) * 100).round(1)

print("ABG/VBG overlap (among gas-positive):")
print(abg_vbg_overlap.to_string(index=False))
print("ICD vs Gas overlap (cohort-level):")
print(icd_gas_overlap.to_string(index=False))


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# --- Ascertainment overlap counts (ABG/VBG/ICD) ---\n\nrequired = ["abg_hypercap_threshold", "vbg_hypercap_threshold", "any_hypercap_icd"]\nmissing = [c for c in required if c not in df.columns]\nif missing:\n    raise KeyError(f"Missing required columns for overlap counts: {missing}")\n\nabg = pd.to_numeric(df["abg_hypercap_threshold"], errors="coerce").fillna(0).astype(int)\nvbg = pd.to_numeric(df["vbg_hypercap_threshold"], errors="coerce").fillna(0).astype(int)\n\ngas_any = (\n    pd.to_numeric(df.get("pco2_threshold_any", None), errors="coerce")\n    if "pco2_threshold_any" in df.columns else (abg | vbg)\n)\nif hasattr(gas_any, "fillna"):\n    gas_any = gas_any.fillna(0).astype(int)\nelse:\n    gas_any = gas_any.astype(int)\n\nicd = pd.to_numeric(df["any_hypercap_icd"], errors="coerce").fillna(0).astype(int)\n\ntotal_n = len(df)\nngas = int((gas_any == 1).sum())\n\nabg_vbg_overlap = pd.DataFrame([\n    {"group": "ABG-only", "count": int(((abg==1) & (vbg==0)).sum())},\n    {"group": "VBG-only", "count": int(((vbg==1) & (abg==0)).sum())},\n    {"group": "ABG+VBG", "count": int(((abg==1) & (vbg==1)).sum())},\n])\nif ngas > 0:\n    abg_vbg_overlap["pct_of_gas"] = (abg_vbg_overlap["count"] / ngas * 100).round(1)\nelse:\n    abg_vbg_overlap["pct_of_gas"] = 0.0\nabg_vbg_overlap["pct_of_cohort"] = (abg_vbg_overlap["count"] / max(total_n,1) * 100).round(1)\n\nicd_gas_overlap = pd.DataFrame([\n    {"group": "ICD+Gas", "count": int(((icd==1) & (gas_any==1)).sum())},\n    {"group": "ICD-only", "count": int(((icd==1) & (gas_any==0)).sum())},\n    {"group": "Gas-only", "count": int(((icd==0) & (gas_any==1)).sum())},\n    {"group": "Neither", "count": int(((icd==0) & (gas_any==0)).sum())},\n])\nicd_gas_overlap["pct_of_cohort"] = (icd_gas_overlap["count"] / max(total_n,1) * 100).round(1)\n\nprint("ABG/VBG overlap (among gas-positive):")\nprint(abg_vbg_overlap.to_string(index=False))\nprint("ICD vs Gas overlap (cohort-level):")\nprint(icd_gas_overlap.to_string(index=False))\n\n', 'execution_count': 19}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ABG/VBG overlap (among gas-positive):\n   group  count  pct_of_gas  pct_of_cohort\nABG-only      0         0.0            0.0\nVBG-only  39016       100.0           97.2\n ABG+VBG      0         0.0            0.0\nICD vs Gas overlap (cohort-level):\n   group  count  pct_of_cohort\n ICD+Gas   3102            7.7\nICD-only   1135            2.8\nGas-only  35914           89.4\n Neither      0            0.0\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
# --- Missingness summary (chief complaint, race/ethnicity, ED triage/vitals) ---

import numpy as np

summary_rows = []

# Chief complaint missingness (ED triage CC)
if "ed_triage_cc" in df.columns:
    cc_present = df["ed_triage_cc"].notna() & (df["ed_triage_cc"].astype(str).str.strip() != "")
    summary_rows.append({
        "variable": "ed_triage_cc_present",
        "missing_n": int((~cc_present).sum()),
        "missing_pct": float((~cc_present).mean())
    })

# Race/Ethnicity missingness (NIH categories + raw sources)
unknown_tokens = {
    "unknown or not reported",
    "unknown",
    "not reported",
    "missing",
    "declined",
    "unable"
}

def _missing_rate(series):
    if series is None:
        return None, None
    s = series.astype(str).str.strip()
    is_missing = series.isna() | (s == "") | s.str.lower().isin(unknown_tokens)
    return int(is_missing.sum()), float(is_missing.mean())

for col in ["nih_race", "nih_ethnicity", "race_hosp_raw", "race_ed_raw"]:
    if col in df.columns:
        m_n, m_p = _missing_rate(df[col])
        summary_rows.append({
            "variable": col,
            "missing_n": m_n,
            "missing_pct": m_p
        })

missing_summary = pd.DataFrame(summary_rows)
print("Missingness summary (key variables):")
missing_summary

# ED triage + first ED vitals missingness
triage_cols = [c for c in df.columns if c.startswith("ed_triage_")]
first_cols  = [c for c in df.columns if c.startswith("ed_first_")]

vital_cols = triage_cols + first_cols
if vital_cols:
    miss_tbl = (
        pd.DataFrame({"variable": vital_cols})
        .assign(
            missing_n=lambda d: [int(df[c].isna().sum()) for c in d["variable"]],
            missing_pct=lambda d: [float(df[c].isna().mean()) for c in d["variable"]]
        )
        .sort_values("missing_pct", ascending=False)
    )
    print("Missingness summary (ED triage + first ED vitals):")
    miss_tbl
else:
    miss_tbl = pd.DataFrame(columns=["variable", "missing_n", "missing_pct"])


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# --- Missingness summary (chief complaint, race/ethnicity, ED triage/vitals) ---\n\nimport numpy as np\n\nsummary_rows = []\n\n# Chief complaint missingness (ED triage CC)\nif "ed_triage_cc" in df.columns:\n    cc_present = df["ed_triage_cc"].notna() & (df["ed_triage_cc"].astype(str).str.strip() != "")\n    summary_rows.append({\n        "variable": "ed_triage_cc_present",\n        "missing_n": int((~cc_present).sum()),\n        "missing_pct": float((~cc_present).mean())\n    })\n\n# Race/Ethnicity missingness (NIH categories + raw sources)\nunknown_tokens = {\n    "unknown or not reported",\n    "unknown",\n    "not reported",\n    "missing",\n    "declined",\n    "unable"\n}\n\ndef _missing_rate(series):\n    if series is None:\n        return None, None\n    s = series.astype(str).str.strip()\n    is_missing = series.isna() | (s == "") | s.str.lower().isin(unknown_tokens)\n    return int(is_missing.sum()), float(is_missing.mean())\n\nfor col in ["nih_race", "nih_ethnicity", "race_hosp_raw", "race_ed_raw"]:\n    if col in df.columns:\n        m_n, m_p = _missing_rate(df[col])\n        summary_rows.append({\n            "variable": col,\n            "missing_n": m_n,\n            "missing_pct": m_p\n        })\n\nmissing_summary = pd.DataFrame(summary_rows)\nprint("Missingness summary (key variables):")\nmissing_summary\n\n# ED triage + first ED vitals missingness\ntriage_cols = [c for c in df.columns if c.startswith("ed_triage_")]\nfirst_cols  = [c for c in df.columns if c.startswith("ed_first_")]\n\nvital_cols = triage_cols + first_cols\nif vital_cols:\n    miss_tbl = (\n        pd.DataFrame({"variable": vital_cols})\n        .assign(\n            missing_n=lambda d: [int(df[c].isna().sum()) for c in d["variable"]],\n            missing_pct=lambda d: [float(df[c].isna().mean()) for c in d["variable"]]\n        )\n        .sort_values("missing_pct", ascending=False)\n    )\n    print("Missingness summary (ED triage + first ED vitals):")\n    miss_tbl\nelse:\n    miss_tbl = pd.DataFrame(columns=["variable", "missing_n", "missing_pct"])\n\n', 'execution_count': 20}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Missingness summary (key variables):\nMissingness summary (ED triage + first ED vitals):\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 50
[NbConvertApp] Skipping non-executing cell 51
[NbConvertApp] Executing cell:

# ICU LOS negative?
if {"icu_los_days","first_icu_stay_id"}.issubset(df.columns):
    neg_los = int((df["icu_los_days"] < 0).fillna(False).sum())
    print("Negative ICU LOS rows:", neg_los)

# Vent flags consistency
vent_cols = {"imv_flag","niv_flag","any_vent_flag"}
if vent_cols.issubset(df.columns):
    any_calc = ((df["imv_flag"]==1) | (df["niv_flag"]==1)).fillna(False).astype(int)
    any_flag = pd.to_numeric(df["any_vent_flag"], errors="coerce").fillna(0).astype(int)
    mism = int((any_calc != any_flag).sum())
    print("any_vent_flag mismatches vs (imv|niv):", mism)

# UOMs: expect mmhg only
uom_cols = [c for c in df.columns if c.endswith("_paco2_uom")]
for c in uom_cols:
    vals = sorted(pd.Series(df[c]).dropna().astype(str).str.lower().str.strip().unique().tolist())
    print(c, vals)

# ABG/VBG coverage QC
def qc_pair(df, ph_col, co2_col, label, ph_lo=6.3, ph_hi=7.8, co2_lo=5, co2_hi=200):
    ph  = pd.to_numeric(df.get(ph_col), errors="coerce")
    co2 = pd.to_numeric(df.get(co2_col), errors="coerce")
    return {
        "pair": label,
        "present_any":  int(((ph.notna()) | (co2.notna())).sum()),
        "present_both": int(((ph.notna()) & (co2.notna())).sum()),
        "only_ph":      int(((ph.notna()) & (~co2.notna())).sum()),
        "only_pco2":    int(((co2.notna()) & (~ph.notna())).sum()),
        "ph_oob":       int((((ph  < ph_lo)  | (ph  > ph_hi))  & ph.notna()).sum()),
        "pco2_oob":     int((((co2 < co2_lo) | (co2 > co2_hi)) & co2.notna()).sum()),
    }

qc = pd.DataFrame([
    qc_pair(df, "lab_abg_ph","lab_abg_paco2","LAB ABG"),
    qc_pair(df, "lab_vbg_ph","lab_vbg_paco2","LAB VBG"),
    qc_pair(df, "poc_abg_ph","poc_abg_paco2","POC ABG"),
    qc_pair(df, "poc_vbg_ph","poc_vbg_paco2","POC VBG"),
])
qc

[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '\n# ICU LOS negative?\nif {"icu_los_days","first_icu_stay_id"}.issubset(df.columns):\n    neg_los = int((df["icu_los_days"] < 0).fillna(False).sum())\n    print("Negative ICU LOS rows:", neg_los)\n\n# Vent flags consistency\nvent_cols = {"imv_flag","niv_flag","any_vent_flag"}\nif vent_cols.issubset(df.columns):\n    any_calc = ((df["imv_flag"]==1) | (df["niv_flag"]==1)).fillna(False).astype(int)\n    any_flag = pd.to_numeric(df["any_vent_flag"], errors="coerce").fillna(0).astype(int)\n    mism = int((any_calc != any_flag).sum())\n    print("any_vent_flag mismatches vs (imv|niv):", mism)\n\n# UOMs: expect mmhg only\nuom_cols = [c for c in df.columns if c.endswith("_paco2_uom")]\nfor c in uom_cols:\n    vals = sorted(pd.Series(df[c]).dropna().astype(str).str.lower().str.strip().unique().tolist())\n    print(c, vals)\n\n# ABG/VBG coverage QC\ndef qc_pair(df, ph_col, co2_col, label, ph_lo=6.3, ph_hi=7.8, co2_lo=5, co2_hi=200):\n    ph  = pd.to_numeric(df.get(ph_col), errors="coerce")\n    co2 = pd.to_numeric(df.get(co2_col), errors="coerce")\n    return {\n        "pair": label,\n        "present_any":  int(((ph.notna()) | (co2.notna())).sum()),\n        "present_both": int(((ph.notna()) & (co2.notna())).sum()),\n        "only_ph":      int(((ph.notna()) & (~co2.notna())).sum()),\n        "only_pco2":    int(((co2.notna()) & (~ph.notna())).sum()),\n        "ph_oob":       int((((ph  < ph_lo)  | (ph  > ph_hi))  & ph.notna()).sum()),\n        "pco2_oob":     int((((co2 < co2_lo) | (co2 > co2_hi)) & co2.notna()).sum()),\n    }\n\nqc = pd.DataFrame([\n    qc_pair(df, "lab_abg_ph","lab_abg_paco2","LAB ABG"),\n    qc_pair(df, "lab_vbg_ph","lab_vbg_paco2","LAB VBG"),\n    qc_pair(df, "poc_abg_ph","poc_abg_paco2","POC ABG"),\n    qc_pair(df, "poc_vbg_ph","poc_vbg_paco2","POC VBG"),\n])\nqc\n', 'execution_count': 21}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': "Negative ICU LOS rows: 0\nany_vent_flag mismatches vs (imv|niv): 0\nlab_abg_paco2_uom ['mmhg']\nlab_vbg_paco2_uom ['mmhg']\npoc_abg_paco2_uom ['mmhg']\npoc_vbg_paco2_uom ['mmhg']\n"}
[NbConvertApp] msg_type: execute_result
[NbConvertApp] content: {'data': {'text/plain': '      pair  present_any  present_both  only_ph  only_pco2  ph_oob  pco2_oob\n0  LAB ABG        17368         17168      195          5       0         0\n1  LAB VBG        36980         33600     3372          8       0         0\n2  POC ABG        15411          7046        0       8365       0         0\n3  POC VBG        22413         16476        0       5937       0         0', 'text/html': '<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>pair</th>\n      <th>present_any</th>\n      <th>present_both</th>\n      <th>only_ph</th>\n      <th>only_pco2</th>\n      <th>ph_oob</th>\n      <th>pco2_oob</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>LAB ABG</td>\n      <td>17368</td>\n      <td>17168</td>\n      <td>195</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>LAB VBG</td>\n      <td>36980</td>\n      <td>33600</td>\n      <td>3372</td>\n      <td>8</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>POC ABG</td>\n      <td>15411</td>\n      <td>7046</td>\n      <td>0</td>\n      <td>8365</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>POC VBG</td>\n      <td>22413</td>\n      <td>16476</td>\n      <td>0</td>\n      <td>5937</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>'}, 'metadata': {}, 'execution_count': 21}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 53
[NbConvertApp] Skipping non-executing cell 54
[NbConvertApp] Executing cell:
from datetime import datetime
out_path = DATA_DIR / f"mimic_hypercap_EXT_bq_abg_vbg_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
with pd.ExcelWriter(out_path, engine="openpyxl") as xw:
    df.to_excel(xw, sheet_name="cohort", index=False)
    try:
        qc.to_excel(xw, sheet_name="qc_abg_vbg", index=False)
    except Exception:
        pass
out_path


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': 'from datetime import datetime\nout_path = DATA_DIR / f"mimic_hypercap_EXT_bq_abg_vbg_{datetime.now().strftime(\'%Y%m%d_%H%M%S\')}.xlsx"\nwith pd.ExcelWriter(out_path, engine="openpyxl") as xw:\n    df.to_excel(xw, sheet_name="cohort", index=False)\n    try:\n        qc.to_excel(xw, sheet_name="qc_abg_vbg", index=False)\n    except Exception:\n        pass\nout_path\n\n', 'execution_count': 22}
[NbConvertApp] msg_type: execute_result
[NbConvertApp] content: {'data': {'text/plain': "PosixPath('/Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/MIMIC tabular data/mimic_hypercap_EXT_bq_abg_vbg_20260204_162606.xlsx')"}, 'metadata': {}, 'execution_count': 22}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 56
[NbConvertApp] Skipping non-executing cell 57
[NbConvertApp] Executing cell:
# ---- Extra exports: (1) ED chief-complaint only; (2) random sample of 160 patients ----
from datetime import datetime

# Where to write
out_dir = DATA_DIR
out_dir.mkdir(parents=True, exist_ok=True)
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

# 1) Filter to rows with a non-empty ED chief complaint
if "ed_triage_cc" not in df.columns:
    raise KeyError(
        "Column 'ed_triage_cc' not found in df. "
        "Ensure the ED triage merge cell ran earlier."
    )

mask_cc = df["ed_triage_cc"].notna() & (df["ed_triage_cc"].astype(str).str.strip() != "")
df_cc = df.loc[mask_cc].copy()

print(f"ED-CC present rows: {len(df_cc)} of {len(df)} "
      f"({(len(df_cc) / max(len(df),1)):.1%} of cohort).")

# Save ED-CC-only cohort
out_path_cc = out_dir / f"mimic_hypercap_EXT_EDcc_only_bq_abg_vbg_{timestamp}.xlsx"
with pd.ExcelWriter(out_path_cc, engine="openpyxl") as xw:
    df_cc.to_excel(xw, sheet_name="cohort_cc_only", index=False)
    try:
        qc.to_excel(xw, sheet_name="qc_abg_vbg", index=False)
    except Exception:
        pass
print("Saved:", out_path_cc)

# 2) Random sample of n = 160 patients (distinct subject_id), one row per patient
if "subject_id" not in df_cc.columns:
    raise KeyError("Column 'subject_id' missing; cannot sample by patient.")

# Make a one-row-per-patient frame by earliest admission
if "admittime" in df_cc.columns:
    df_cc_one = (
        df_cc.sort_values(["subject_id", "admittime"])
             .groupby("subject_id", as_index=False)
             .head(1)
    )
else:
    # Fallback if admittime not present: choose the smallest hadm_id per patient
    df_cc_one = (
        df_cc.sort_values(["subject_id", "hadm_id"])
             .groupby("subject_id", as_index=False)
             .head(1)
    )

N = 160
n_avail = len(df_cc_one)
n_take = min(N, n_avail)
if n_avail < N:
    print(f"Warning: only {n_avail} unique patients with ED chief complaint; sampling all of them.")

RANDOM_SEED = 42
df_cc_sample = df_cc_one.sample(n=n_take, random_state=RANDOM_SEED)

# Save the sample
out_path_cc_sample = out_dir / f"mimic_hypercap_EXT_EDcc_sample{n_take}_bq_abg_vbg_{timestamp}.xlsx"
with pd.ExcelWriter(out_path_cc_sample, engine="openpyxl") as xw:
    df_cc_sample.to_excel(xw, sheet_name="cohort_cc_sample", index=False)
    try:
        qc.to_excel(xw, sheet_name="qc_abg_vbg", index=False)
    except Exception:
        pass
print("Saved:", out_path_cc_sample)


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# ---- Extra exports: (1) ED chief-complaint only; (2) random sample of 160 patients ----\nfrom datetime import datetime\n\n# Where to write\nout_dir = DATA_DIR\nout_dir.mkdir(parents=True, exist_ok=True)\ntimestamp = datetime.now().strftime(\'%Y%m%d_%H%M%S\')\n\n# 1) Filter to rows with a non-empty ED chief complaint\nif "ed_triage_cc" not in df.columns:\n    raise KeyError(\n        "Column \'ed_triage_cc\' not found in df. "\n        "Ensure the ED triage merge cell ran earlier."\n    )\n\nmask_cc = df["ed_triage_cc"].notna() & (df["ed_triage_cc"].astype(str).str.strip() != "")\ndf_cc = df.loc[mask_cc].copy()\n\nprint(f"ED-CC present rows: {len(df_cc)} of {len(df)} "\n      f"({(len(df_cc) / max(len(df),1)):.1%} of cohort).")\n\n# Save ED-CC-only cohort\nout_path_cc = out_dir / f"mimic_hypercap_EXT_EDcc_only_bq_abg_vbg_{timestamp}.xlsx"\nwith pd.ExcelWriter(out_path_cc, engine="openpyxl") as xw:\n    df_cc.to_excel(xw, sheet_name="cohort_cc_only", index=False)\n    try:\n        qc.to_excel(xw, sheet_name="qc_abg_vbg", index=False)\n    except Exception:\n        pass\nprint("Saved:", out_path_cc)\n\n# 2) Random sample of n = 160 patients (distinct subject_id), one row per patient\nif "subject_id" not in df_cc.columns:\n    raise KeyError("Column \'subject_id\' missing; cannot sample by patient.")\n\n# Make a one-row-per-patient frame by earliest admission\nif "admittime" in df_cc.columns:\n    df_cc_one = (\n        df_cc.sort_values(["subject_id", "admittime"])\n             .groupby("subject_id", as_index=False)\n             .head(1)\n    )\nelse:\n    # Fallback if admittime not present: choose the smallest hadm_id per patient\n    df_cc_one = (\n        df_cc.sort_values(["subject_id", "hadm_id"])\n             .groupby("subject_id", as_index=False)\n             .head(1)\n    )\n\nN = 160\nn_avail = len(df_cc_one)\nn_take = min(N, n_avail)\nif n_avail < N:\n    print(f"Warning: only {n_avail} unique patients with ED chief complaint; sampling all of them.")\n\nRANDOM_SEED = 42\ndf_cc_sample = df_cc_one.sample(n=n_take, random_state=RANDOM_SEED)\n\n# Save the sample\nout_path_cc_sample = out_dir / f"mimic_hypercap_EXT_EDcc_sample{n_take}_bq_abg_vbg_{timestamp}.xlsx"\nwith pd.ExcelWriter(out_path_cc_sample, engine="openpyxl") as xw:\n    df_cc_sample.to_excel(xw, sheet_name="cohort_cc_sample", index=False)\n    try:\n        qc.to_excel(xw, sheet_name="qc_abg_vbg", index=False)\n    except Exception:\n        pass\nprint("Saved:", out_path_cc_sample)\n\n', 'execution_count': 23}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ED-CC present rows: 17424 of 40151 (43.4% of cohort).\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Saved: /Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/MIMIC tabular data/mimic_hypercap_EXT_EDcc_only_bq_abg_vbg_20260204_162638.xlsx\nSaved: /Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/MIMIC tabular data/mimic_hypercap_EXT_EDcc_sample160_bq_abg_vbg_20260204_162638.xlsx\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 59
[NbConvertApp] Skipping non-executing cell 60
[NbConvertApp] Executing cell:
from pathlib import Path
import json
import pandas as pd

# Identify current cohort dataframe (admission-level)
if 'df' not in globals():
    raise NameError("Expected admission-level df to exist before inventory step.")

ED_KEY = "ed_stay_id"  # target key for ED-level cohort

print("Current admission-level df:", df.shape)
print("Current columns count:", len(df.columns))
if ED_KEY in df.columns:
    print("ED stay unique count:", int(df[ED_KEY].nunique()))
else:
    print("ED stay unique count: ED_KEY not in columns")

# Persist columns snapshot
cols_out = WORK_DIR / "current_columns.json"
cols_out.write_text(json.dumps(sorted(df.columns), indent=2))
print("Wrote:", cols_out)



# Persist ED-stay columns snapshot (if ed_df exists)
if "ed_df" in globals():
    ed_cols_out = WORK_DIR / "ed_columns.json"
    ed_cols_out.write_text(json.dumps(sorted(ed_df.columns), indent=2))
    print("Wrote:", ed_cols_out)

[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': 'from pathlib import Path\nimport json\nimport pandas as pd\n\n# Identify current cohort dataframe (admission-level)\nif \'df\' not in globals():\n    raise NameError("Expected admission-level df to exist before inventory step.")\n\nED_KEY = "ed_stay_id"  # target key for ED-level cohort\n\nprint("Current admission-level df:", df.shape)\nprint("Current columns count:", len(df.columns))\nif ED_KEY in df.columns:\n    print("ED stay unique count:", int(df[ED_KEY].nunique()))\nelse:\n    print("ED stay unique count: ED_KEY not in columns")\n\n# Persist columns snapshot\ncols_out = WORK_DIR / "current_columns.json"\ncols_out.write_text(json.dumps(sorted(df.columns), indent=2))\nprint("Wrote:", cols_out)\n\n\n\n# Persist ED-stay columns snapshot (if ed_df exists)\nif "ed_df" in globals():\n    ed_cols_out = WORK_DIR / "ed_columns.json"\n    ed_cols_out.write_text(json.dumps(sorted(ed_df.columns), indent=2))\n    print("Wrote:", ed_cols_out)\n', 'execution_count': 24}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Current admission-level df: (40151, 89)\nCurrent columns count: 89\nED stay unique count: ED_KEY not in columns\nWrote: /Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/current_columns.json\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
# Target field registry
TARGET_RAW_FIELDS = {
    "ed_edstays": [
        "ed_stay_id","subject_id","hadm_id","ed_intime","ed_outtime","ed_intime_first",
        "arrival_transport","disposition","ed_gender","ed_race",
    ],
    "ed_triage": [
        "ed_triage_temp","ed_triage_hr","ed_triage_rr","ed_triage_o2sat",
        "ed_triage_sbp","ed_triage_dbp","ed_triage_pain","ed_triage_acuity","ed_triage_cc",
    ],
    "ed_vitals_first": [
        "ed_first_vitals_time","ed_first_temp","ed_first_hr","ed_first_rr",
        "ed_first_o2sat","ed_first_sbp","ed_first_dbp","ed_first_rhythm","ed_first_pain",
    ],
    "admissions": [
        "admittime","dischtime","deathtime","hospital_expire_flag",
        "admission_type","admission_location","discharge_location",
        "insurance","language","marital_status","hosp_race",
    ],
    "icu": [
        "icu_stay_id","icu_intime_first","icu_outtime_last","icu_los_total","n_icu_stays",
        "first_careunit","last_careunit",
    ],
    "labs_gas": [
        "first_gas_time","first_pco2","first_ph","first_hco3","first_lactate",
        "max_pco2_0_6h","min_ph_0_6h","max_pco2_0_24h","min_ph_0_24h",
        "flag_abg_hypercapnia","flag_vbg_hypercapnia","flag_any_gas_hypercapnia",
        "gas_source_unknown_rate",
        "dt_first_imv_hours","dt_first_niv_hours",
    ],
    "omr": [
        "bmi_closest_pre_ed","height_closest_pre_ed","weight_closest_pre_ed",
    ],
    "dx_flags": [
        "flag_copd","flag_osa_ohs","flag_chf","flag_neuromuscular",
        "flag_opioid_substance","flag_pneumonia",
    ],
    "timing": [
        "dt_first_qualifying_gas_hours","presenting_hypercapnia","late_hypercapnia",
        "dt_first_imv_hours","dt_first_niv_hours","abg_before_imv","vbg_before_imv",
        "ph_band","hco3_band","lactate_band",
    ],
}

TARGET_DERIVED_FIELDS = [
    "hospital_los_hours","in_hospital_death",
]

TARGET_FIELDS = sorted({c for v in TARGET_RAW_FIELDS.values() for c in v} | set(TARGET_DERIVED_FIELDS))

# Grouped missing report
missing_by_group = {}
for group, cols in TARGET_RAW_FIELDS.items():
    missing_by_group[group] = [c for c in cols if c not in df.columns]

missing_derived = [c for c in TARGET_DERIVED_FIELDS if c not in df.columns]
missing_by_group["derived"] = missing_derived

missing_flat = [c for cols in missing_by_group.values() for c in cols]
print("Missing fields total:", len(missing_flat))
for group, cols in missing_by_group.items():
    if cols:
        print(f"- {group}: {cols}")




[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Target field registry\nTARGET_RAW_FIELDS = {\n    "ed_edstays": [\n        "ed_stay_id","subject_id","hadm_id","ed_intime","ed_outtime","ed_intime_first",\n        "arrival_transport","disposition","ed_gender","ed_race",\n    ],\n    "ed_triage": [\n        "ed_triage_temp","ed_triage_hr","ed_triage_rr","ed_triage_o2sat",\n        "ed_triage_sbp","ed_triage_dbp","ed_triage_pain","ed_triage_acuity","ed_triage_cc",\n    ],\n    "ed_vitals_first": [\n        "ed_first_vitals_time","ed_first_temp","ed_first_hr","ed_first_rr",\n        "ed_first_o2sat","ed_first_sbp","ed_first_dbp","ed_first_rhythm","ed_first_pain",\n    ],\n    "admissions": [\n        "admittime","dischtime","deathtime","hospital_expire_flag",\n        "admission_type","admission_location","discharge_location",\n        "insurance","language","marital_status","hosp_race",\n    ],\n    "icu": [\n        "icu_stay_id","icu_intime_first","icu_outtime_last","icu_los_total","n_icu_stays",\n        "first_careunit","last_careunit",\n    ],\n    "labs_gas": [\n        "first_gas_time","first_pco2","first_ph","first_hco3","first_lactate",\n        "max_pco2_0_6h","min_ph_0_6h","max_pco2_0_24h","min_ph_0_24h",\n        "flag_abg_hypercapnia","flag_vbg_hypercapnia","flag_any_gas_hypercapnia",\n        "gas_source_unknown_rate",\n        "dt_first_imv_hours","dt_first_niv_hours",\n    ],\n    "omr": [\n        "bmi_closest_pre_ed","height_closest_pre_ed","weight_closest_pre_ed",\n    ],\n    "dx_flags": [\n        "flag_copd","flag_osa_ohs","flag_chf","flag_neuromuscular",\n        "flag_opioid_substance","flag_pneumonia",\n    ],\n    "timing": [\n        "dt_first_qualifying_gas_hours","presenting_hypercapnia","late_hypercapnia",\n        "dt_first_imv_hours","dt_first_niv_hours","abg_before_imv","vbg_before_imv",\n        "ph_band","hco3_band","lactate_band",\n    ],\n}\n\nTARGET_DERIVED_FIELDS = [\n    "hospital_los_hours","in_hospital_death",\n]\n\nTARGET_FIELDS = sorted({c for v in TARGET_RAW_FIELDS.values() for c in v} | set(TARGET_DERIVED_FIELDS))\n\n# Grouped missing report\nmissing_by_group = {}\nfor group, cols in TARGET_RAW_FIELDS.items():\n    missing_by_group[group] = [c for c in cols if c not in df.columns]\n\nmissing_derived = [c for c in TARGET_DERIVED_FIELDS if c not in df.columns]\nmissing_by_group["derived"] = missing_derived\n\nmissing_flat = [c for cols in missing_by_group.values() for c in cols]\nprint("Missing fields total:", len(missing_flat))\nfor group, cols in missing_by_group.items():\n    if cols:\n        print(f"- {group}: {cols}")\n\n\n\n', 'execution_count': 25}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': "Missing fields total: 53\n- ed_edstays: ['ed_stay_id', 'ed_intime', 'ed_outtime', 'ed_intime_first', 'arrival_transport', 'disposition', 'ed_gender', 'ed_race']\n- admissions: ['hospital_expire_flag', 'language', 'marital_status', 'hosp_race']\n- icu: ['icu_stay_id', 'icu_intime_first', 'icu_outtime_last', 'icu_los_total', 'n_icu_stays', 'first_careunit', 'last_careunit']\n- labs_gas: ['first_gas_time', 'first_pco2', 'first_ph', 'first_hco3', 'first_lactate', 'max_pco2_0_6h', 'min_ph_0_6h', 'max_pco2_0_24h', 'min_ph_0_24h', 'flag_abg_hypercapnia', 'flag_vbg_hypercapnia', 'flag_any_gas_hypercapnia', 'gas_source_unknown_rate', 'dt_first_imv_hours', 'dt_first_niv_hours']\n- omr: ['bmi_closest_pre_ed', 'height_closest_pre_ed', 'weight_closest_pre_ed']\n- dx_flags: ['flag_copd', 'flag_osa_ohs', 'flag_chf', 'flag_neuromuscular', 'flag_opioid_substance', 'flag_pneumonia']\n- timing: ['dt_first_qualifying_gas_hours', 'presenting_hypercapnia', 'late_hypercapnia', 'dt_first_imv_hours', 'dt_first_niv_hours', 'ph_band', 'hco3_band', 'lactate_band']\n- derived: ['hospital_los_hours', 'in_hospital_death']\n"}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 63
[NbConvertApp] Executing cell:
# ED stay spine (rename stay_id to ed_stay_id)

SQL["ed_spine_sql"] = f"""
WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x)
SELECT
  s.stay_id AS ed_stay_id,
  s.subject_id,
  s.hadm_id,
  s.intime AS ed_intime,
  s.outtime AS ed_outtime,
  s.arrival_transport,
  s.disposition,
  s.gender AS ed_gender,
  s.race   AS ed_race
FROM `{PHYS}.{ED}.edstays` s
JOIN hadms h ON h.hadm_id = s.hadm_id
"""

ed_spine = run_sql_bq(sql("ed_spine_sql"), {"hadms": hadm_list})
print("ED spine rows:", len(ed_spine), "unique ed_stay_id:", ed_spine["ed_stay_id"].nunique())

# ensure uniqueness
if ed_spine["ed_stay_id"].nunique() != len(ed_spine):
    raise ValueError("ed_stay_id not unique in ED spine")

# First ED presentation time per admission
ed_intime_first = (
    ed_spine.groupby("hadm_id", as_index=False)["ed_intime"]
    .min()
    .rename(columns={"ed_intime": "ed_intime_first"})
)

# Start ED-level df
ed_df = ed_spine.copy()
ed_df = ed_df.merge(ed_intime_first, on="hadm_id", how="left")


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# ED stay spine (rename stay_id to ed_stay_id)\n\nSQL["ed_spine_sql"] = f"""\nWITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x)\nSELECT\n  s.stay_id AS ed_stay_id,\n  s.subject_id,\n  s.hadm_id,\n  s.intime AS ed_intime,\n  s.outtime AS ed_outtime,\n  s.arrival_transport,\n  s.disposition,\n  s.gender AS ed_gender,\n  s.race   AS ed_race\nFROM `{PHYS}.{ED}.edstays` s\nJOIN hadms h ON h.hadm_id = s.hadm_id\n"""\n\ned_spine = run_sql_bq(sql("ed_spine_sql"), {"hadms": hadm_list})\nprint("ED spine rows:", len(ed_spine), "unique ed_stay_id:", ed_spine["ed_stay_id"].nunique())\n\n# ensure uniqueness\nif ed_spine["ed_stay_id"].nunique() != len(ed_spine):\n    raise ValueError("ed_stay_id not unique in ED spine")\n\n# First ED presentation time per admission\ned_intime_first = (\n    ed_spine.groupby("hadm_id", as_index=False)["ed_intime"]\n    .min()\n    .rename(columns={"ed_intime": "ed_intime_first"})\n)\n\n# Start ED-level df\ned_df = ed_spine.copy()\ned_df = ed_df.merge(ed_intime_first, on="hadm_id", how="left")\n\n', 'execution_count': 26}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ED spine rows: 17449 unique ed_stay_id: 17449\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
# ED triage and first ED vitals (reuse existing logic if present; otherwise join)

def _needs_cols(df, cols):
    return (df is None) or any(c not in df.columns for c in cols)

# Use existing ed_triage / ed_first if already in memory from earlier cells
try:
    _ = ed_triage
except NameError:
    ed_triage = None

try:
    _ = ed_first
except NameError:
    ed_first = None

# Force re-query if required keys are missing
if _needs_cols(locals().get('ed_triage', None), ['ed_stay_id', 'hadm_id']):
    ed_triage = None
if _needs_cols(locals().get('ed_first', None), ['ed_stay_id']):
    ed_first = None

# If missing, re-run ED triage / first vitals queries with ed_stay_id output
if ed_triage is None:
    ed_triage_sql = f"""
    WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x)
    SELECT
      s.stay_id AS ed_stay_id,
      s.hadm_id,
      s.intime,
      t.temperature    AS ed_triage_temp,
      t.heartrate      AS ed_triage_hr,
      t.resprate       AS ed_triage_rr,
      t.o2sat          AS ed_triage_o2sat,
      t.sbp            AS ed_triage_sbp,
      t.dbp            AS ed_triage_dbp,
      t.pain           AS ed_triage_pain,
      t.acuity         AS ed_triage_acuity,
      t.chiefcomplaint AS ed_triage_cc
    FROM `{PHYS}.{ED}.edstays` s
    JOIN hadms h ON h.hadm_id = s.hadm_id
    LEFT JOIN `{PHYS}.{ED}.triage` t
      ON t.stay_id = s.stay_id
    """
    ed_triage = run_sql_bq(sql("ed_triage_sql"), {'hadms': hadm_list})
    print('ED triage rows:', len(ed_triage))

if ed_first is None:
    ed_first_vitals_sql = f"""
    WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x),
    edmap AS (
      SELECT stay_id, hadm_id
      FROM `{PHYS}.{ED}.edstays`
      WHERE hadm_id IN (SELECT hadm_id FROM hadms)
    ),
    vs AS (
      SELECT stay_id, charttime, temperature, heartrate, resprate, o2sat, sbp, dbp, rhythm, pain
      FROM `{PHYS}.{ED}.vitalsign`
    ),
    first_vs AS (
      SELECT
        v.stay_id,
        (ARRAY_AGG(STRUCT(v.charttime, v.temperature, v.heartrate, v.resprate, v.o2sat, v.sbp, v.dbp, v.rhythm, v.pain)
                   ORDER BY v.charttime LIMIT 1))[OFFSET(0)] AS pick
      FROM vs v
      JOIN edmap m USING (stay_id)
      GROUP BY v.stay_id
    )
    SELECT
      f.stay_id AS ed_stay_id,
      pick.charttime AS ed_first_vitals_time,
      pick.temperature AS ed_first_temp,
      pick.heartrate AS ed_first_hr,
      pick.resprate AS ed_first_rr,
      pick.o2sat AS ed_first_o2sat,
      pick.sbp AS ed_first_sbp,
      pick.dbp AS ed_first_dbp,
      pick.rhythm AS ed_first_rhythm,
      pick.pain AS ed_first_pain
    FROM first_vs f
    """
    ed_first = run_sql_bq(sql("ed_first_vitals_sql"), {'hadms': hadm_list})
    print('ED first vitals rows:', len(ed_first))

# Debug columns before merge
print('ed_triage cols:', list(ed_triage.columns))
print('ed_first cols:', list(ed_first.columns))
print('ed_df cols:', list(ed_df.columns))

if 'ed_stay_id' not in ed_df.columns:
    raise KeyError('ed_df missing ed_stay_id; ensure ED spine cell ran.')

# Merge ED triage + vitals onto ed_df with available keys
merge_keys_triage = [k for k in ["ed_stay_id", "hadm_id"] if k in ed_df.columns and k in ed_triage.columns]
if not merge_keys_triage:
    raise KeyError("No common keys between ed_df and ed_triage")
ed_df = ed_df.merge(ed_triage, on=merge_keys_triage, how="left")
merge_keys_first = [k for k in ["ed_stay_id", "hadm_id"] if k in ed_df.columns and k in ed_first.columns]
if not merge_keys_first:
    raise KeyError("No common keys between ed_df and ed_first")
ed_df = ed_df.merge(ed_first, on=merge_keys_first, how="left")





[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# ED triage and first ED vitals (reuse existing logic if present; otherwise join)\n\ndef _needs_cols(df, cols):\n    return (df is None) or any(c not in df.columns for c in cols)\n\n# Use existing ed_triage / ed_first if already in memory from earlier cells\ntry:\n    _ = ed_triage\nexcept NameError:\n    ed_triage = None\n\ntry:\n    _ = ed_first\nexcept NameError:\n    ed_first = None\n\n# Force re-query if required keys are missing\nif _needs_cols(locals().get(\'ed_triage\', None), [\'ed_stay_id\', \'hadm_id\']):\n    ed_triage = None\nif _needs_cols(locals().get(\'ed_first\', None), [\'ed_stay_id\']):\n    ed_first = None\n\n# If missing, re-run ED triage / first vitals queries with ed_stay_id output\nif ed_triage is None:\n    ed_triage_sql = f"""\n    WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x)\n    SELECT\n      s.stay_id AS ed_stay_id,\n      s.hadm_id,\n      s.intime,\n      t.temperature    AS ed_triage_temp,\n      t.heartrate      AS ed_triage_hr,\n      t.resprate       AS ed_triage_rr,\n      t.o2sat          AS ed_triage_o2sat,\n      t.sbp            AS ed_triage_sbp,\n      t.dbp            AS ed_triage_dbp,\n      t.pain           AS ed_triage_pain,\n      t.acuity         AS ed_triage_acuity,\n      t.chiefcomplaint AS ed_triage_cc\n    FROM `{PHYS}.{ED}.edstays` s\n    JOIN hadms h ON h.hadm_id = s.hadm_id\n    LEFT JOIN `{PHYS}.{ED}.triage` t\n      ON t.stay_id = s.stay_id\n    """\n    ed_triage = run_sql_bq(sql("ed_triage_sql"), {\'hadms\': hadm_list})\n    print(\'ED triage rows:\', len(ed_triage))\n\nif ed_first is None:\n    ed_first_vitals_sql = f"""\n    WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x),\n    edmap AS (\n      SELECT stay_id, hadm_id\n      FROM `{PHYS}.{ED}.edstays`\n      WHERE hadm_id IN (SELECT hadm_id FROM hadms)\n    ),\n    vs AS (\n      SELECT stay_id, charttime, temperature, heartrate, resprate, o2sat, sbp, dbp, rhythm, pain\n      FROM `{PHYS}.{ED}.vitalsign`\n    ),\n    first_vs AS (\n      SELECT\n        v.stay_id,\n        (ARRAY_AGG(STRUCT(v.charttime, v.temperature, v.heartrate, v.resprate, v.o2sat, v.sbp, v.dbp, v.rhythm, v.pain)\n                   ORDER BY v.charttime LIMIT 1))[OFFSET(0)] AS pick\n      FROM vs v\n      JOIN edmap m USING (stay_id)\n      GROUP BY v.stay_id\n    )\n    SELECT\n      f.stay_id AS ed_stay_id,\n      pick.charttime AS ed_first_vitals_time,\n      pick.temperature AS ed_first_temp,\n      pick.heartrate AS ed_first_hr,\n      pick.resprate AS ed_first_rr,\n      pick.o2sat AS ed_first_o2sat,\n      pick.sbp AS ed_first_sbp,\n      pick.dbp AS ed_first_dbp,\n      pick.rhythm AS ed_first_rhythm,\n      pick.pain AS ed_first_pain\n    FROM first_vs f\n    """\n    ed_first = run_sql_bq(sql("ed_first_vitals_sql"), {\'hadms\': hadm_list})\n    print(\'ED first vitals rows:\', len(ed_first))\n\n# Debug columns before merge\nprint(\'ed_triage cols:\', list(ed_triage.columns))\nprint(\'ed_first cols:\', list(ed_first.columns))\nprint(\'ed_df cols:\', list(ed_df.columns))\n\nif \'ed_stay_id\' not in ed_df.columns:\n    raise KeyError(\'ed_df missing ed_stay_id; ensure ED spine cell ran.\')\n\n# Merge ED triage + vitals onto ed_df with available keys\nmerge_keys_triage = [k for k in ["ed_stay_id", "hadm_id"] if k in ed_df.columns and k in ed_triage.columns]\nif not merge_keys_triage:\n    raise KeyError("No common keys between ed_df and ed_triage")\ned_df = ed_df.merge(ed_triage, on=merge_keys_triage, how="left")\nmerge_keys_first = [k for k in ["ed_stay_id", "hadm_id"] if k in ed_df.columns and k in ed_first.columns]\nif not merge_keys_first:\n    raise KeyError("No common keys between ed_df and ed_first")\ned_df = ed_df.merge(ed_first, on=merge_keys_first, how="left")\n\n\n\n\n', 'execution_count': 27}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ED triage rows: 17424\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': "ED first vitals rows: 16746\ned_triage cols: ['hadm_id', 'ed_triage_temp', 'ed_triage_hr', 'ed_triage_rr', 'ed_triage_o2sat', 'ed_triage_sbp', 'ed_triage_dbp', 'ed_triage_pain', 'ed_triage_acuity', 'ed_triage_cc']\ned_first cols: ['hadm_id', 'ed_first_vitals_time', 'ed_first_temp', 'ed_first_hr', 'ed_first_rr', 'ed_first_o2sat', 'ed_first_sbp', 'ed_first_dbp', 'ed_first_rhythm', 'ed_first_pain']\ned_df cols: ['ed_stay_id', 'subject_id', 'hadm_id', 'ed_intime', 'ed_outtime', 'arrival_transport', 'disposition', 'ed_gender', 'ed_race', 'ed_intime_first']\n"}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 66
[NbConvertApp] Executing cell:
# Admissions fields
SQL["admit_sql"] = f"""
WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x)
SELECT
  a.hadm_id,
  a.admittime,
  a.dischtime,
  a.deathtime,
  a.hospital_expire_flag,
  a.admission_type,
  a.admission_location,
  a.discharge_location,
  a.insurance,
  a.language,
  a.marital_status,
  a.race AS hosp_race
FROM `{PHYS}.{HOSP}.admissions` a
JOIN hadms h USING (hadm_id)
"""

admit = run_sql_bq(sql("admit_sql"), {"hadms": hadm_list})
print("Admissions rows:", len(admit))

ed_df = ed_df.merge(admit, on="hadm_id", how="left")

# Merge ventilation flags/times (hadm-level)
if "vent_combined" in globals():
    ed_df = ed_df.merge(vent_combined, on="hadm_id", how="left")

# Derived outcomes
ed_df["hospital_los_hours"] = (ed_df["dischtime"] - ed_df["admittime"]).dt.total_seconds() / 3600.0
ed_df["in_hospital_death"] = ((ed_df["hospital_expire_flag"] == 1) | ed_df["deathtime"].notna()).astype("int64")

# Concordance check
discord = (
    ((ed_df["hospital_expire_flag"] == 1) & ed_df["deathtime"].isna()) |
    ((ed_df["hospital_expire_flag"] == 0) & ed_df["deathtime"].notna())
)
print("Admissions discordance (expire_flag vs deathtime):", int(discord.sum()))


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Admissions fields\nSQL["admit_sql"] = f"""\nWITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x)\nSELECT\n  a.hadm_id,\n  a.admittime,\n  a.dischtime,\n  a.deathtime,\n  a.hospital_expire_flag,\n  a.admission_type,\n  a.admission_location,\n  a.discharge_location,\n  a.insurance,\n  a.language,\n  a.marital_status,\n  a.race AS hosp_race\nFROM `{PHYS}.{HOSP}.admissions` a\nJOIN hadms h USING (hadm_id)\n"""\n\nadmit = run_sql_bq(sql("admit_sql"), {"hadms": hadm_list})\nprint("Admissions rows:", len(admit))\n\ned_df = ed_df.merge(admit, on="hadm_id", how="left")\n\n# Merge ventilation flags/times (hadm-level)\nif "vent_combined" in globals():\n    ed_df = ed_df.merge(vent_combined, on="hadm_id", how="left")\n\n# Derived outcomes\ned_df["hospital_los_hours"] = (ed_df["dischtime"] - ed_df["admittime"]).dt.total_seconds() / 3600.0\ned_df["in_hospital_death"] = ((ed_df["hospital_expire_flag"] == 1) | ed_df["deathtime"].notna()).astype("int64")\n\n# Concordance check\ndiscord = (\n    ((ed_df["hospital_expire_flag"] == 1) & ed_df["deathtime"].isna()) |\n    ((ed_df["hospital_expire_flag"] == 0) & ed_df["deathtime"].notna())\n)\nprint("Admissions discordance (expire_flag vs deathtime):", int(discord.sum()))\n\n', 'execution_count': 28}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Admissions rows: 40151\nAdmissions discordance (expire_flag vs deathtime): 3\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 68
[NbConvertApp] Executing cell:
# ICU stays (aggregate per hadm)
SQL["icu_sql"] = f"""
WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x)
SELECT
  i.hadm_id,
  i.stay_id AS icu_stay_id,
  i.intime,
  i.outtime,
  i.los,
  i.first_careunit,
  i.last_careunit
FROM `{PHYS}.{ICU}.icustays` i
JOIN hadms h USING (hadm_id)
"""

icu = run_sql_bq(sql("icu_sql"), {"hadms": hadm_list})
print("ICU stay rows:", len(icu))

if len(icu) > 0:
    icu_agg = (
        icu.sort_values(["hadm_id", "intime"]).groupby("hadm_id", as_index=False)
        .agg(
            icu_intime_first=("intime", "min"),
            icu_outtime_last=("outtime", "max"),
            icu_los_total=("los", "sum"),
            n_icu_stays=("icu_stay_id", "nunique"),
            first_careunit=("first_careunit", "first"),
            last_careunit=("last_careunit", "last"),
        )
    )
    ed_df = ed_df.merge(icu_agg, on="hadm_id", how="left")
else:
    print("No ICU stays found for cohort.")


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# ICU stays (aggregate per hadm)\nSQL["icu_sql"] = f"""\nWITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x)\nSELECT\n  i.hadm_id,\n  i.stay_id AS icu_stay_id,\n  i.intime,\n  i.outtime,\n  i.los,\n  i.first_careunit,\n  i.last_careunit\nFROM `{PHYS}.{ICU}.icustays` i\nJOIN hadms h USING (hadm_id)\n"""\n\nicu = run_sql_bq(sql("icu_sql"), {"hadms": hadm_list})\nprint("ICU stay rows:", len(icu))\n\nif len(icu) > 0:\n    icu_agg = (\n        icu.sort_values(["hadm_id", "intime"]).groupby("hadm_id", as_index=False)\n        .agg(\n            icu_intime_first=("intime", "min"),\n            icu_outtime_last=("outtime", "max"),\n            icu_los_total=("los", "sum"),\n            n_icu_stays=("icu_stay_id", "nunique"),\n            first_careunit=("first_careunit", "first"),\n            last_careunit=("last_careunit", "last"),\n        )\n    )\n    ed_df = ed_df.merge(icu_agg, on="hadm_id", how="left")\nelse:\n    print("No ICU stays found for cohort.")\n\n', 'execution_count': 29}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ICU stay rows: 31393\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 70
[NbConvertApp] Executing cell:
# ED vitals long + aggregates (0-6h)

SQL["ed_vitals_sql"] = f"""
WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x),
edmap AS (
  SELECT stay_id AS ed_stay_id, hadm_id, intime AS ed_intime
  FROM `{PHYS}.{ED}.edstays`
  WHERE hadm_id IN (SELECT hadm_id FROM hadms)
)
SELECT
  v.stay_id AS ed_stay_id,
  m.hadm_id,
  m.ed_intime,
  v.charttime,
  v.temperature,
  v.heartrate,
  v.resprate,
  v.o2sat,
  v.sbp,
  v.dbp,
  v.rhythm,
  v.pain
FROM `{PHYS}.{ED}.vitalsign` v
JOIN edmap m ON m.ed_stay_id = v.stay_id
"""

ed_vitals_long = run_sql_bq(sql("ed_vitals_sql"), {"hadms": hadm_list})
print("ED vitals long rows:", len(ed_vitals_long))

# Window filter: 0–6h from ED intime
ed_vitals_long["dt_hours"] = (ed_vitals_long["charttime"] - ed_vitals_long["ed_intime"]).dt.total_seconds() / 3600.0
in_6h = ed_vitals_long["dt_hours"].between(0, 6, inclusive="both")

agg = (
    ed_vitals_long.loc[in_6h]
    .groupby("ed_stay_id", as_index=False)
    .agg(
        max_heartrate_0_6h=("heartrate", "max"),
        max_resprate_0_6h=("resprate", "max"),
        min_o2sat_0_6h=("o2sat", "min"),
        min_sbp_0_6h=("sbp", "min"),
        n_vitals_0_6h=("charttime", "count"),
    )
)

ed_df = ed_df.merge(agg, on="ed_stay_id", how="left")

# Range warnings (do not drop)
range_checks = {
    "heartrate": (0, 300),
    "resprate": (0, 80),
    "o2sat": (0, 100),
    "sbp": (0, 300),
}
for col, (lo, hi) in range_checks.items():
    bad = ed_vitals_long[col].notna() & (~ed_vitals_long[col].between(lo, hi))
    if bad.any():
        print(f"Warning: {col} out of range count:", int(bad.sum()))


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# ED vitals long + aggregates (0-6h)\n\nSQL["ed_vitals_sql"] = f"""\nWITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x),\nedmap AS (\n  SELECT stay_id AS ed_stay_id, hadm_id, intime AS ed_intime\n  FROM `{PHYS}.{ED}.edstays`\n  WHERE hadm_id IN (SELECT hadm_id FROM hadms)\n)\nSELECT\n  v.stay_id AS ed_stay_id,\n  m.hadm_id,\n  m.ed_intime,\n  v.charttime,\n  v.temperature,\n  v.heartrate,\n  v.resprate,\n  v.o2sat,\n  v.sbp,\n  v.dbp,\n  v.rhythm,\n  v.pain\nFROM `{PHYS}.{ED}.vitalsign` v\nJOIN edmap m ON m.ed_stay_id = v.stay_id\n"""\n\ned_vitals_long = run_sql_bq(sql("ed_vitals_sql"), {"hadms": hadm_list})\nprint("ED vitals long rows:", len(ed_vitals_long))\n\n# Window filter: 0–6h from ED intime\ned_vitals_long["dt_hours"] = (ed_vitals_long["charttime"] - ed_vitals_long["ed_intime"]).dt.total_seconds() / 3600.0\nin_6h = ed_vitals_long["dt_hours"].between(0, 6, inclusive="both")\n\nagg = (\n    ed_vitals_long.loc[in_6h]\n    .groupby("ed_stay_id", as_index=False)\n    .agg(\n        max_heartrate_0_6h=("heartrate", "max"),\n        max_resprate_0_6h=("resprate", "max"),\n        min_o2sat_0_6h=("o2sat", "min"),\n        min_sbp_0_6h=("sbp", "min"),\n        n_vitals_0_6h=("charttime", "count"),\n    )\n)\n\ned_df = ed_df.merge(agg, on="ed_stay_id", how="left")\n\n# Range warnings (do not drop)\nrange_checks = {\n    "heartrate": (0, 300),\n    "resprate": (0, 80),\n    "o2sat": (0, 100),\n    "sbp": (0, 300),\n}\nfor col, (lo, hi) in range_checks.items():\n    bad = ed_vitals_long[col].notna() & (~ed_vitals_long[col].between(lo, hi))\n    if bad.any():\n        print(f"Warning: {col} out of range count:", int(bad.sum()))\n\n', 'execution_count': 30}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ED vitals long rows: 120310\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Warning: heartrate out of range count: 1\nWarning: resprate out of range count: 3\nWarning: o2sat out of range count: 2\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 72
[NbConvertApp] Executing cell:
import re
import json

# Discover itemids from d_labitems
SQL["labitems_sql"] = f"""
SELECT itemid, label, fluid, category
FROM `{PHYS}.{HOSP}.d_labitems`
"""

labitems = run_sql_bq(sql("labitems_sql"))

patterns = {
    "gas_pco2": re.compile(r"\bp\s*co2\b|pco2|pco₂", re.I),
    "gas_ph": re.compile(r"\bph\b", re.I),
    "gas_hco3": re.compile(r"hco3|bicarbonate", re.I),
    "gas_lactate": re.compile(r"lactate", re.I),
    "gas_specimen": re.compile(r"specimen|source|type", re.I),
    "chem_creatinine": re.compile(r"creatinine", re.I),
    "chem_sodium": re.compile(r"\bsodium\b", re.I),
    "chem_chloride": re.compile(r"\bchloride\b", re.I),
    "chem_total_co2": re.compile(r"carbon dioxide|total co2|\bco2\b", re.I),
    "cbc_hemoglobin": re.compile(r"hemoglobin", re.I),
}

# category filters
cat_gas = re.compile(r"blood\s*gas|blood gas|arterial|venous", re.I)
cat_chem = re.compile(r"chemistry|chem|blood", re.I)
cat_cbc = re.compile(r"hematology|cbc", re.I)

matches = {}
for name, pat in patterns.items():
    dfm = labitems.copy()
    dfm = dfm[dfm["label"].str.contains(pat, na=False)]
    if name.startswith("gas_"):
        dfm = dfm[dfm["category"].str.contains(cat_gas, na=False)]
    elif name.startswith("chem_"):
        dfm = dfm[dfm["category"].str.contains(cat_chem, na=False)]
    elif name.startswith("cbc_"):
        dfm = dfm[dfm["category"].str.contains(cat_cbc, na=False)]
    matches[name] = dfm[["itemid","label","category"]]

# Build lab_item_map with counts in cohort
itemids_all = sorted({int(i) for dfm in matches.values() for i in dfm["itemid"].tolist()})

SQL["counts_sql"] = f"""
WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x)
SELECT itemid, COUNT(*) AS n
FROM `{PHYS}.{HOSP}.labevents`
WHERE hadm_id IN (SELECT hadm_id FROM hadms)
  AND itemid IN UNNEST(@itemids)
GROUP BY itemid
"""

counts = run_sql_bq(sql("counts_sql"), {"hadms": hadm_list, "itemids": itemids_all}) if itemids_all else pd.DataFrame(columns=["itemid","n"])

lab_item_map = {}
for name, dfm in matches.items():
    tmp = dfm.merge(counts, on="itemid", how="left").fillna({"n":0})
    lab_item_map[name] = {
        "pattern": patterns[name].pattern,
        "items": tmp.sort_values("n", ascending=False).to_dict(orient="records"),
    }

lab_item_map_path = WORK_DIR / "lab_item_map.json"
lab_item_map_path.write_text(json.dumps(lab_item_map, indent=2))
print("Wrote:", lab_item_map_path)



[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': 'import re\nimport json\n\n# Discover itemids from d_labitems\nSQL["labitems_sql"] = f"""\nSELECT itemid, label, fluid, category\nFROM `{PHYS}.{HOSP}.d_labitems`\n"""\n\nlabitems = run_sql_bq(sql("labitems_sql"))\n\npatterns = {\n    "gas_pco2": re.compile(r"\\bp\\s*co2\\b|pco2|pco₂", re.I),\n    "gas_ph": re.compile(r"\\bph\\b", re.I),\n    "gas_hco3": re.compile(r"hco3|bicarbonate", re.I),\n    "gas_lactate": re.compile(r"lactate", re.I),\n    "gas_specimen": re.compile(r"specimen|source|type", re.I),\n    "chem_creatinine": re.compile(r"creatinine", re.I),\n    "chem_sodium": re.compile(r"\\bsodium\\b", re.I),\n    "chem_chloride": re.compile(r"\\bchloride\\b", re.I),\n    "chem_total_co2": re.compile(r"carbon dioxide|total co2|\\bco2\\b", re.I),\n    "cbc_hemoglobin": re.compile(r"hemoglobin", re.I),\n}\n\n# category filters\ncat_gas = re.compile(r"blood\\s*gas|blood gas|arterial|venous", re.I)\ncat_chem = re.compile(r"chemistry|chem|blood", re.I)\ncat_cbc = re.compile(r"hematology|cbc", re.I)\n\nmatches = {}\nfor name, pat in patterns.items():\n    dfm = labitems.copy()\n    dfm = dfm[dfm["label"].str.contains(pat, na=False)]\n    if name.startswith("gas_"):\n        dfm = dfm[dfm["category"].str.contains(cat_gas, na=False)]\n    elif name.startswith("chem_"):\n        dfm = dfm[dfm["category"].str.contains(cat_chem, na=False)]\n    elif name.startswith("cbc_"):\n        dfm = dfm[dfm["category"].str.contains(cat_cbc, na=False)]\n    matches[name] = dfm[["itemid","label","category"]]\n\n# Build lab_item_map with counts in cohort\nitemids_all = sorted({int(i) for dfm in matches.values() for i in dfm["itemid"].tolist()})\n\nSQL["counts_sql"] = f"""\nWITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x)\nSELECT itemid, COUNT(*) AS n\nFROM `{PHYS}.{HOSP}.labevents`\nWHERE hadm_id IN (SELECT hadm_id FROM hadms)\n  AND itemid IN UNNEST(@itemids)\nGROUP BY itemid\n"""\n\ncounts = run_sql_bq(sql("counts_sql"), {"hadms": hadm_list, "itemids": itemids_all}) if itemids_all else pd.DataFrame(columns=["itemid","n"])\n\nlab_item_map = {}\nfor name, dfm in matches.items():\n    tmp = dfm.merge(counts, on="itemid", how="left").fillna({"n":0})\n    lab_item_map[name] = {\n        "pattern": patterns[name].pattern,\n        "items": tmp.sort_values("n", ascending=False).to_dict(orient="records"),\n    }\n\nlab_item_map_path = WORK_DIR / "lab_item_map.json"\nlab_item_map_path.write_text(json.dumps(lab_item_map, indent=2))\nprint("Wrote:", lab_item_map_path)\n\n\n', 'execution_count': 31}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Wrote: /Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/lab_item_map.json\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
# Extract labevents within ED windows

# assemble itemid lists
itemid_sets = {k: [int(x["itemid"]) for x in v["items"]] for k, v in lab_item_map.items()}
# flatten
all_itemids = sorted({i for v in itemid_sets.values() for i in v})

SQL["labs_sql"] = f"""
WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x),
eds AS (
  SELECT stay_id AS ed_stay_id, hadm_id, intime AS ed_intime
  FROM `{PHYS}.{ED}.edstays`
  WHERE hadm_id IN (SELECT hadm_id FROM hadms)
)
SELECT
  e.ed_stay_id,
  l.subject_id,
  l.hadm_id,
  l.itemid,
  l.charttime,
  l.specimen_id,
  l.valuenum,
  l.valueuom
FROM `{PHYS}.{HOSP}.labevents` l
JOIN eds e ON e.hadm_id = l.hadm_id
WHERE l.itemid IN UNNEST(@itemids)
  AND l.charttime BETWEEN e.ed_intime AND TIMESTAMP_ADD(e.ed_intime, INTERVAL 24 HOUR)
"""

labs_long = run_sql_bq(sql("labs_sql"), {"hadms": hadm_list, "itemids": all_itemids})
print("Labs long rows:", len(labs_long))

# Unit audit for pCO2
pco2_ids = itemid_sets.get("gas_pco2", [])
unit_audit = (
    labs_long.loc[labs_long["itemid"].isin(pco2_ids)]
    .groupby("valueuom", dropna=False)
    .size().reset_index(name="n")
)
unit_audit_path = WORK_DIR / "lab_unit_audit.csv"
unit_audit.to_csv(unit_audit_path, index=False)
print("Wrote:", unit_audit_path)


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Extract labevents within ED windows\n\n# assemble itemid lists\nitemid_sets = {k: [int(x["itemid"]) for x in v["items"]] for k, v in lab_item_map.items()}\n# flatten\nall_itemids = sorted({i for v in itemid_sets.values() for i in v})\n\nSQL["labs_sql"] = f"""\nWITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x),\neds AS (\n  SELECT stay_id AS ed_stay_id, hadm_id, intime AS ed_intime\n  FROM `{PHYS}.{ED}.edstays`\n  WHERE hadm_id IN (SELECT hadm_id FROM hadms)\n)\nSELECT\n  e.ed_stay_id,\n  l.subject_id,\n  l.hadm_id,\n  l.itemid,\n  l.charttime,\n  l.specimen_id,\n  l.valuenum,\n  l.valueuom\nFROM `{PHYS}.{HOSP}.labevents` l\nJOIN eds e ON e.hadm_id = l.hadm_id\nWHERE l.itemid IN UNNEST(@itemids)\n  AND l.charttime BETWEEN e.ed_intime AND TIMESTAMP_ADD(e.ed_intime, INTERVAL 24 HOUR)\n"""\n\nlabs_long = run_sql_bq(sql("labs_sql"), {"hadms": hadm_list, "itemids": all_itemids})\nprint("Labs long rows:", len(labs_long))\n\n# Unit audit for pCO2\npco2_ids = itemid_sets.get("gas_pco2", [])\nunit_audit = (\n    labs_long.loc[labs_long["itemid"].isin(pco2_ids)]\n    .groupby("valueuom", dropna=False)\n    .size().reset_index(name="n")\n)\nunit_audit_path = WORK_DIR / "lab_unit_audit.csv"\nunit_audit.to_csv(unit_audit_path, index=False)\nprint("Wrote:", unit_audit_path)\n\n', 'execution_count': 32}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Labs long rows: 277691\nWrote: /Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/lab_unit_audit.csv\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
# Reconstruct gas panels by specimen_id within ED stay

pco2_ids = set(itemid_sets.get("gas_pco2", []))
ph_ids = set(itemid_sets.get("gas_ph", []))
hco3_ids = set(itemid_sets.get("gas_hco3", []))
lact_ids = set(itemid_sets.get("gas_lactate", []))

labs = labs_long.copy()

# Convert pCO2 kPa to mmHg when needed
is_kpa = labs["valueuom"].astype(str).str.lower().str.contains("kpa", na=False)
mask_pco2 = labs["itemid"].isin(pco2_ids)
if mask_pco2.any() and is_kpa.any():
    labs.loc[mask_pco2 & is_kpa, "valuenum"] = labs.loc[mask_pco2 & is_kpa, "valuenum"] * 7.50062
    labs.loc[mask_pco2 & is_kpa, "valueuom"] = "mmHg"

# panel by specimen_id
panel = (
    labs.groupby(["ed_stay_id","specimen_id"], as_index=False)
    .agg(panel_time=("charttime","min"))
)

# attach analytes

def pick_analyte(df, ids, name):
    tmp = df.loc[df["itemid"].isin(ids), ["ed_stay_id","specimen_id","valuenum"]]
    tmp = tmp.rename(columns={"valuenum": name})
    return tmp.groupby(["ed_stay_id","specimen_id"], as_index=False).first()

if pco2_ids:
    panel = panel.merge(pick_analyte(labs, pco2_ids, "pco2"), on=["ed_stay_id","specimen_id"], how="left")
if ph_ids:
    panel = panel.merge(pick_analyte(labs, ph_ids, "ph"), on=["ed_stay_id","specimen_id"], how="left")
if hco3_ids:
    panel = panel.merge(pick_analyte(labs, hco3_ids, "hco3"), on=["ed_stay_id","specimen_id"], how="left")
if lact_ids:
    panel = panel.merge(pick_analyte(labs, lact_ids, "lactate"), on=["ed_stay_id","specimen_id"], how="left")

# First panel per ED stay
first_panel = (
    panel.sort_values(["ed_stay_id","panel_time"]).groupby("ed_stay_id", as_index=False).first()
)

# 0–6h and 0–24h extrema
panel = panel.merge(ed_df[["ed_stay_id","ed_intime"]], on="ed_stay_id", how="left")
panel["dt_hours"] = (panel["panel_time"] - panel["ed_intime"]).dt.total_seconds() / 3600.0

# Ensure expected panel columns exist even if analyte is absent
for col in ["pco2", "ph", "hco3", "lactate"]:
    if col not in panel.columns:
        panel[col] = pd.NA

p06 = panel.loc[panel["dt_hours"].between(0,6, inclusive="both")]
p24 = panel.loc[panel["dt_hours"].between(0,24, inclusive="both")]

agg06 = p06.groupby("ed_stay_id", as_index=False).agg(max_pco2_0_6h=("pco2","max"), min_ph_0_6h=("ph","min"))
agg24 = p24.groupby("ed_stay_id", as_index=False).agg(max_pco2_0_24h=("pco2","max"), min_ph_0_24h=("ph","min"))

ed_df = ed_df.merge(first_panel[["ed_stay_id","panel_time","pco2","ph","hco3","lactate"]].rename(
    columns={"panel_time":"first_gas_time","pco2":"first_pco2","ph":"first_ph","hco3":"first_hco3","lactate":"first_lactate"}
), on="ed_stay_id", how="left")

ed_df = ed_df.merge(agg06, on="ed_stay_id", how="left")
ed_df = ed_df.merge(agg24, on="ed_stay_id", how="left")



[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Reconstruct gas panels by specimen_id within ED stay\n\npco2_ids = set(itemid_sets.get("gas_pco2", []))\nph_ids = set(itemid_sets.get("gas_ph", []))\nhco3_ids = set(itemid_sets.get("gas_hco3", []))\nlact_ids = set(itemid_sets.get("gas_lactate", []))\n\nlabs = labs_long.copy()\n\n# Convert pCO2 kPa to mmHg when needed\nis_kpa = labs["valueuom"].astype(str).str.lower().str.contains("kpa", na=False)\nmask_pco2 = labs["itemid"].isin(pco2_ids)\nif mask_pco2.any() and is_kpa.any():\n    labs.loc[mask_pco2 & is_kpa, "valuenum"] = labs.loc[mask_pco2 & is_kpa, "valuenum"] * 7.50062\n    labs.loc[mask_pco2 & is_kpa, "valueuom"] = "mmHg"\n\n# panel by specimen_id\npanel = (\n    labs.groupby(["ed_stay_id","specimen_id"], as_index=False)\n    .agg(panel_time=("charttime","min"))\n)\n\n# attach analytes\n\ndef pick_analyte(df, ids, name):\n    tmp = df.loc[df["itemid"].isin(ids), ["ed_stay_id","specimen_id","valuenum"]]\n    tmp = tmp.rename(columns={"valuenum": name})\n    return tmp.groupby(["ed_stay_id","specimen_id"], as_index=False).first()\n\nif pco2_ids:\n    panel = panel.merge(pick_analyte(labs, pco2_ids, "pco2"), on=["ed_stay_id","specimen_id"], how="left")\nif ph_ids:\n    panel = panel.merge(pick_analyte(labs, ph_ids, "ph"), on=["ed_stay_id","specimen_id"], how="left")\nif hco3_ids:\n    panel = panel.merge(pick_analyte(labs, hco3_ids, "hco3"), on=["ed_stay_id","specimen_id"], how="left")\nif lact_ids:\n    panel = panel.merge(pick_analyte(labs, lact_ids, "lactate"), on=["ed_stay_id","specimen_id"], how="left")\n\n# First panel per ED stay\nfirst_panel = (\n    panel.sort_values(["ed_stay_id","panel_time"]).groupby("ed_stay_id", as_index=False).first()\n)\n\n# 0–6h and 0–24h extrema\npanel = panel.merge(ed_df[["ed_stay_id","ed_intime"]], on="ed_stay_id", how="left")\npanel["dt_hours"] = (panel["panel_time"] - panel["ed_intime"]).dt.total_seconds() / 3600.0\n\n# Ensure expected panel columns exist even if analyte is absent\nfor col in ["pco2", "ph", "hco3", "lactate"]:\n    if col not in panel.columns:\n        panel[col] = pd.NA\n\np06 = panel.loc[panel["dt_hours"].between(0,6, inclusive="both")]\np24 = panel.loc[panel["dt_hours"].between(0,24, inclusive="both")]\n\nagg06 = p06.groupby("ed_stay_id", as_index=False).agg(max_pco2_0_6h=("pco2","max"), min_ph_0_6h=("ph","min"))\nagg24 = p24.groupby("ed_stay_id", as_index=False).agg(max_pco2_0_24h=("pco2","max"), min_ph_0_24h=("ph","min"))\n\ned_df = ed_df.merge(first_panel[["ed_stay_id","panel_time","pco2","ph","hco3","lactate"]].rename(\n    columns={"panel_time":"first_gas_time","pco2":"first_pco2","ph":"first_ph","hco3":"first_hco3","lactate":"first_lactate"}\n), on="ed_stay_id", how="left")\n\ned_df = ed_df.merge(agg06, on="ed_stay_id", how="left")\ned_df = ed_df.merge(agg24, on="ed_stay_id", how="left")\n\n\n', 'execution_count': 33}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
# ABG vs VBG classification (label-based fallback only; specimen source may be unavailable)

# Use label heuristics from d_labitems if available (arterial/venous hints)
labitems_map = labitems.set_index("itemid")["label"].to_dict()

def infer_source(itemid):
    label = str(labitems_map.get(itemid, "")).lower()
    if re.search(r"arterial|abg|a[- ]?line", label):
        return "arterial"
    if re.search(r"venous|vbg|central|mixed", label):
        return "venous"
    return "unknown"

# assign source for panels based on pco2 item label (fallback)
if pco2_ids:
    pco2_itemid = labs.loc[labs["itemid"].isin(pco2_ids)].groupby(["ed_stay_id","specimen_id"], as_index=False)["itemid"].first()
    pco2_itemid["source"] = pco2_itemid["itemid"].map(infer_source)
    panel = panel.merge(pco2_itemid[["ed_stay_id","specimen_id","source"]], on=["ed_stay_id","specimen_id"], how="left")
else:
    panel["source"] = "unknown"

# flags
panel["flag_abg_hypercapnia"] = ((panel["source"]=="arterial") & (panel["pco2"]>=45)).astype(int)
panel["flag_vbg_hypercapnia"] = ((panel["source"]=="venous") & (panel["pco2"]>=50)).astype(int)
panel["flag_any_gas_hypercapnia"] = ((panel["pco2"]>=45)).astype(int)

# per-stay unknown source rate
if len(panel) > 0:
    unk_rate = (
        panel.assign(_unk=(panel["source"].fillna("unknown") == "unknown"))
             .groupby("ed_stay_id", as_index=False)["_unk"].mean()
             .rename(columns={"_unk": "gas_source_unknown_rate"})
    )
else:
    unk_rate = panel[["ed_stay_id"]].drop_duplicates()
    unk_rate["gas_source_unknown_rate"] = 1.0

# collapse to ED stay flags
flags = panel.groupby("ed_stay_id", as_index=False).agg(
    flag_abg_hypercapnia=("flag_abg_hypercapnia","max"),
    flag_vbg_hypercapnia=("flag_vbg_hypercapnia","max"),
    flag_any_gas_hypercapnia=("flag_any_gas_hypercapnia","max"),
)

ed_df = ed_df.merge(flags, on="ed_stay_id", how="left")
ed_df = ed_df.merge(unk_rate, on="ed_stay_id", how="left")
ed_df["gas_source_unknown_rate"] = ed_df["gas_source_unknown_rate"].fillna(1.0)


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# ABG vs VBG classification (label-based fallback only; specimen source may be unavailable)\n\n# Use label heuristics from d_labitems if available (arterial/venous hints)\nlabitems_map = labitems.set_index("itemid")["label"].to_dict()\n\ndef infer_source(itemid):\n    label = str(labitems_map.get(itemid, "")).lower()\n    if re.search(r"arterial|abg|a[- ]?line", label):\n        return "arterial"\n    if re.search(r"venous|vbg|central|mixed", label):\n        return "venous"\n    return "unknown"\n\n# assign source for panels based on pco2 item label (fallback)\nif pco2_ids:\n    pco2_itemid = labs.loc[labs["itemid"].isin(pco2_ids)].groupby(["ed_stay_id","specimen_id"], as_index=False)["itemid"].first()\n    pco2_itemid["source"] = pco2_itemid["itemid"].map(infer_source)\n    panel = panel.merge(pco2_itemid[["ed_stay_id","specimen_id","source"]], on=["ed_stay_id","specimen_id"], how="left")\nelse:\n    panel["source"] = "unknown"\n\n# flags\npanel["flag_abg_hypercapnia"] = ((panel["source"]=="arterial") & (panel["pco2"]>=45)).astype(int)\npanel["flag_vbg_hypercapnia"] = ((panel["source"]=="venous") & (panel["pco2"]>=50)).astype(int)\npanel["flag_any_gas_hypercapnia"] = ((panel["pco2"]>=45)).astype(int)\n\n# per-stay unknown source rate\nif len(panel) > 0:\n    unk_rate = (\n        panel.assign(_unk=(panel["source"].fillna("unknown") == "unknown"))\n             .groupby("ed_stay_id", as_index=False)["_unk"].mean()\n             .rename(columns={"_unk": "gas_source_unknown_rate"})\n    )\nelse:\n    unk_rate = panel[["ed_stay_id"]].drop_duplicates()\n    unk_rate["gas_source_unknown_rate"] = 1.0\n\n# collapse to ED stay flags\nflags = panel.groupby("ed_stay_id", as_index=False).agg(\n    flag_abg_hypercapnia=("flag_abg_hypercapnia","max"),\n    flag_vbg_hypercapnia=("flag_vbg_hypercapnia","max"),\n    flag_any_gas_hypercapnia=("flag_any_gas_hypercapnia","max"),\n)\n\ned_df = ed_df.merge(flags, on="ed_stay_id", how="left")\ned_df = ed_df.merge(unk_rate, on="ed_stay_id", how="left")\ned_df["gas_source_unknown_rate"] = ed_df["gas_source_unknown_rate"].fillna(1.0)\n\n', 'execution_count': 34}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 77
[NbConvertApp] Executing cell:
# Discover ICU POC itemids from d_items

SQL["ditems_sql"] = f"""
SELECT itemid, label, category
FROM `{PHYS}.{ICU}.d_items`
"""

ditems = run_sql_bq(sql("ditems_sql"))

icu_patterns = {
    "pco2": re.compile(r"p\s*co2|pco2|pco₂", re.I),
    "ph": re.compile(r"ph", re.I),
    "hco3": re.compile(r"hco3|bicarbonate", re.I),
    "lactate": re.compile(r"lactate", re.I),
    "specimen": re.compile(r"specimen|source|type", re.I),
}

icu_cat = re.compile(r"blood\s*gas|blood gas|resp|arterial|venous", re.I)

icu_matches = {}
for name, pat in icu_patterns.items():
    dfm = ditems[ditems["label"].str.contains(pat, na=False)]
    dfm = dfm[dfm["category"].str.contains(icu_cat, na=False)]
    icu_matches[name] = dfm[["itemid","label","category"]]

icu_itemids = sorted({int(i) for dfm in icu_matches.values() for i in dfm["itemid"].tolist()})
print("ICU POC candidate itemids:", len(icu_itemids))


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Discover ICU POC itemids from d_items\n\nSQL["ditems_sql"] = f"""\nSELECT itemid, label, category\nFROM `{PHYS}.{ICU}.d_items`\n"""\n\nditems = run_sql_bq(sql("ditems_sql"))\n\nicu_patterns = {\n    "pco2": re.compile(r"\x08p\\s*co2\x08|pco2|pco₂", re.I),\n    "ph": re.compile(r"\x08ph\x08", re.I),\n    "hco3": re.compile(r"hco3|bicarbonate", re.I),\n    "lactate": re.compile(r"lactate", re.I),\n    "specimen": re.compile(r"specimen|source|type", re.I),\n}\n\nicu_cat = re.compile(r"blood\\s*gas|blood gas|resp|arterial|venous", re.I)\n\nicu_matches = {}\nfor name, pat in icu_patterns.items():\n    dfm = ditems[ditems["label"].str.contains(pat, na=False)]\n    dfm = dfm[dfm["category"].str.contains(icu_cat, na=False)]\n    icu_matches[name] = dfm[["itemid","label","category"]]\n\nicu_itemids = sorted({int(i) for dfm in icu_matches.values() for i in dfm["itemid"].tolist()})\nprint("ICU POC candidate itemids:", len(icu_itemids))\n\n', 'execution_count': 35}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ICU POC candidate itemids: 6\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
# Extract ICU chartevents within ED 0–24h window for cohort ICU stays

if len(icu_itemids) == 0:
    icu_poc_long = pd.DataFrame()
    print("No ICU POC itemids found.")
else:
    # ensure icu stay ids available
    if 'icu' not in globals():
        raise NameError("ICU stays table 'icu' not found; run ICU phase first.")

    icu_stays = icu[["icu_stay_id","hadm_id","intime"]].copy()
    icu_stays = icu_stays.dropna(subset=["icu_stay_id"])

    icu_poc_sql = f"""
    WITH icu_stays AS (
      SELECT stay_id AS icu_stay_id, hadm_id
      FROM `{PHYS}.{ICU}.icustays`
      WHERE stay_id IN UNNEST(@icu_stay_ids)
    ),
    eds AS (
      SELECT stay_id AS ed_stay_id, hadm_id, intime AS ed_intime
      FROM `{PHYS}.{ED}.edstays`
      WHERE hadm_id IN (SELECT hadm_id FROM icu_stays)
    )
    SELECT
      s.icu_stay_id,
      e.ed_stay_id,
      e.hadm_id,
      ce.charttime,
      ce.itemid,
      ce.valuenum,
      ce.valueuom
    FROM `{PHYS}.{ICU}.chartevents` ce
    JOIN icu_stays s ON s.icu_stay_id = ce.stay_id
    JOIN eds e ON e.hadm_id = s.hadm_id
    WHERE ce.itemid IN UNNEST(@itemids)
      AND ce.charttime BETWEEN e.ed_intime AND TIMESTAMP_ADD(e.ed_intime, INTERVAL 24 HOUR)
    """

    icu_poc_long = run_sql_bq(icu_poc_sql, {"icu_stay_ids": icu_stays["icu_stay_id"].astype(int).tolist(), "itemids": icu_itemids})
    print("ICU POC long rows:", len(icu_poc_long))

# Build panels by 5-minute bins per ICU stay
if len(icu_poc_long) > 0:
    icu_poc_long["time_bin"] = icu_poc_long["charttime"].dt.floor("5min")

    def pick_from_ids(df, ids, name):
        tmp = df.loc[df["itemid"].isin(ids), ["icu_stay_id","time_bin","valuenum"]]
        tmp = tmp.rename(columns={"valuenum": name})
        return tmp.groupby(["icu_stay_id","time_bin"], as_index=False).first()

    pco2_ids = set(icu_matches.get("pco2", pd.DataFrame()).get("itemid", []).tolist())
    ph_ids = set(icu_matches.get("ph", pd.DataFrame()).get("itemid", []).tolist())
    hco3_ids = set(icu_matches.get("hco3", pd.DataFrame()).get("itemid", []).tolist())
    lact_ids = set(icu_matches.get("lactate", pd.DataFrame()).get("itemid", []).tolist())

    panel_poc = (
        icu_poc_long.groupby(["icu_stay_id","time_bin"], as_index=False)
        .agg(panel_time=("charttime","min"))
    )
    if pco2_ids:
        panel_poc = panel_poc.merge(pick_from_ids(icu_poc_long, pco2_ids, "pco2"), on=["icu_stay_id","time_bin"], how="left")
    if ph_ids:
        panel_poc = panel_poc.merge(pick_from_ids(icu_poc_long, ph_ids, "ph"), on=["icu_stay_id","time_bin"], how="left")
    if hco3_ids:
        panel_poc = panel_poc.merge(pick_from_ids(icu_poc_long, hco3_ids, "hco3"), on=["icu_stay_id","time_bin"], how="left")
    if lact_ids:
        panel_poc = panel_poc.merge(pick_from_ids(icu_poc_long, lact_ids, "lactate"), on=["icu_stay_id","time_bin"], how="left")

    # map to ED stay via hadm_id
    panel_poc = panel_poc.merge(icu[["icu_stay_id","hadm_id"]], on="icu_stay_id", how="left")
    panel_poc = panel_poc.merge(ed_df[["ed_stay_id","hadm_id","ed_intime"]], on="hadm_id", how="left")
    panel_poc["dt_hours"] = (panel_poc["panel_time"] - panel_poc["ed_intime"]).dt.total_seconds() / 3600.0

# Ensure expected panel_poc columns exist even if analyte is absent
for col in ["pco2", "ph", "hco3", "lactate"]:
    if col not in panel_poc.columns:
        panel_poc[col] = pd.NA

    p24_poc = panel_poc.loc[panel_poc["dt_hours"].between(0,24, inclusive="both")]

    # derive flags (source unknown in POC)
    p24_poc["flag_any_gas_hypercapnia_poc"] = (p24_poc["pco2"] >= 45).astype(int)

    poc_flags = p24_poc.groupby("ed_stay_id", as_index=False).agg(
        flag_any_gas_hypercapnia_poc=("flag_any_gas_hypercapnia_poc","max")
    )

# Avoid duplicate column on re-run
if "flag_any_gas_hypercapnia_poc" in ed_df.columns:
    ed_df = ed_df.drop(columns=["flag_any_gas_hypercapnia_poc"])

    ed_df = ed_df.merge(poc_flags, on="ed_stay_id", how="left")

    # incremental yield
    base = ed_df.get("flag_any_gas_hypercapnia", pd.Series(0, index=ed_df.index)).fillna(0).astype(int)
    poc = ed_df.get("flag_any_gas_hypercapnia_poc", pd.Series(0, index=ed_df.index)).fillna(0).astype(int)
    inc = ((base == 0) & (poc == 1)).sum()
    print("ICU POC incremental hypercapnia cases (ED stays):", int(inc))

    # optional export
    panel_poc.to_parquet(DATA_DIR / "gas_panels_poc.parquet", index=False)
else:
    panel_poc = pd.DataFrame()





[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Extract ICU chartevents within ED 0–24h window for cohort ICU stays\n\nif len(icu_itemids) == 0:\n    icu_poc_long = pd.DataFrame()\n    print("No ICU POC itemids found.")\nelse:\n    # ensure icu stay ids available\n    if \'icu\' not in globals():\n        raise NameError("ICU stays table \'icu\' not found; run ICU phase first.")\n\n    icu_stays = icu[["icu_stay_id","hadm_id","intime"]].copy()\n    icu_stays = icu_stays.dropna(subset=["icu_stay_id"])\n\n    icu_poc_sql = f"""\n    WITH icu_stays AS (\n      SELECT stay_id AS icu_stay_id, hadm_id\n      FROM `{PHYS}.{ICU}.icustays`\n      WHERE stay_id IN UNNEST(@icu_stay_ids)\n    ),\n    eds AS (\n      SELECT stay_id AS ed_stay_id, hadm_id, intime AS ed_intime\n      FROM `{PHYS}.{ED}.edstays`\n      WHERE hadm_id IN (SELECT hadm_id FROM icu_stays)\n    )\n    SELECT\n      s.icu_stay_id,\n      e.ed_stay_id,\n      e.hadm_id,\n      ce.charttime,\n      ce.itemid,\n      ce.valuenum,\n      ce.valueuom\n    FROM `{PHYS}.{ICU}.chartevents` ce\n    JOIN icu_stays s ON s.icu_stay_id = ce.stay_id\n    JOIN eds e ON e.hadm_id = s.hadm_id\n    WHERE ce.itemid IN UNNEST(@itemids)\n      AND ce.charttime BETWEEN e.ed_intime AND TIMESTAMP_ADD(e.ed_intime, INTERVAL 24 HOUR)\n    """\n\n    icu_poc_long = run_sql_bq(icu_poc_sql, {"icu_stay_ids": icu_stays["icu_stay_id"].astype(int).tolist(), "itemids": icu_itemids})\n    print("ICU POC long rows:", len(icu_poc_long))\n\n# Build panels by 5-minute bins per ICU stay\nif len(icu_poc_long) > 0:\n    icu_poc_long["time_bin"] = icu_poc_long["charttime"].dt.floor("5min")\n\n    def pick_from_ids(df, ids, name):\n        tmp = df.loc[df["itemid"].isin(ids), ["icu_stay_id","time_bin","valuenum"]]\n        tmp = tmp.rename(columns={"valuenum": name})\n        return tmp.groupby(["icu_stay_id","time_bin"], as_index=False).first()\n\n    pco2_ids = set(icu_matches.get("pco2", pd.DataFrame()).get("itemid", []).tolist())\n    ph_ids = set(icu_matches.get("ph", pd.DataFrame()).get("itemid", []).tolist())\n    hco3_ids = set(icu_matches.get("hco3", pd.DataFrame()).get("itemid", []).tolist())\n    lact_ids = set(icu_matches.get("lactate", pd.DataFrame()).get("itemid", []).tolist())\n\n    panel_poc = (\n        icu_poc_long.groupby(["icu_stay_id","time_bin"], as_index=False)\n        .agg(panel_time=("charttime","min"))\n    )\n    if pco2_ids:\n        panel_poc = panel_poc.merge(pick_from_ids(icu_poc_long, pco2_ids, "pco2"), on=["icu_stay_id","time_bin"], how="left")\n    if ph_ids:\n        panel_poc = panel_poc.merge(pick_from_ids(icu_poc_long, ph_ids, "ph"), on=["icu_stay_id","time_bin"], how="left")\n    if hco3_ids:\n        panel_poc = panel_poc.merge(pick_from_ids(icu_poc_long, hco3_ids, "hco3"), on=["icu_stay_id","time_bin"], how="left")\n    if lact_ids:\n        panel_poc = panel_poc.merge(pick_from_ids(icu_poc_long, lact_ids, "lactate"), on=["icu_stay_id","time_bin"], how="left")\n\n    # map to ED stay via hadm_id\n    panel_poc = panel_poc.merge(icu[["icu_stay_id","hadm_id"]], on="icu_stay_id", how="left")\n    panel_poc = panel_poc.merge(ed_df[["ed_stay_id","hadm_id","ed_intime"]], on="hadm_id", how="left")\n    panel_poc["dt_hours"] = (panel_poc["panel_time"] - panel_poc["ed_intime"]).dt.total_seconds() / 3600.0\n\n# Ensure expected panel_poc columns exist even if analyte is absent\nfor col in ["pco2", "ph", "hco3", "lactate"]:\n    if col not in panel_poc.columns:\n        panel_poc[col] = pd.NA\n\n    p24_poc = panel_poc.loc[panel_poc["dt_hours"].between(0,24, inclusive="both")]\n\n    # derive flags (source unknown in POC)\n    p24_poc["flag_any_gas_hypercapnia_poc"] = (p24_poc["pco2"] >= 45).astype(int)\n\n    poc_flags = p24_poc.groupby("ed_stay_id", as_index=False).agg(\n        flag_any_gas_hypercapnia_poc=("flag_any_gas_hypercapnia_poc","max")\n    )\n\n# Avoid duplicate column on re-run\nif "flag_any_gas_hypercapnia_poc" in ed_df.columns:\n    ed_df = ed_df.drop(columns=["flag_any_gas_hypercapnia_poc"])\n\n    ed_df = ed_df.merge(poc_flags, on="ed_stay_id", how="left")\n\n    # incremental yield\n    base = ed_df.get("flag_any_gas_hypercapnia", pd.Series(0, index=ed_df.index)).fillna(0).astype(int)\n    poc = ed_df.get("flag_any_gas_hypercapnia_poc", pd.Series(0, index=ed_df.index)).fillna(0).astype(int)\n    inc = ((base == 0) & (poc == 1)).sum()\n    print("ICU POC incremental hypercapnia cases (ED stays):", int(inc))\n\n    # optional export\n    panel_poc.to_parquet(DATA_DIR / "gas_panels_poc.parquet", index=False)\nelse:\n    panel_poc = pd.DataFrame()\n\n\n\n\n', 'execution_count': 36}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'ICU POC long rows: 45400\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stderr', 'text': '/var/folders/vf/n84t7b8171lf64smq4ddvw1m0000gn/T/ipykernel_80016/4023957673.py:83: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  p24_poc["flag_any_gas_hypercapnia_poc"] = (p24_poc["pco2"] >= 45).astype(int)\n/var/folders/vf/n84t7b8171lf64smq4ddvw1m0000gn/T/ipykernel_80016/4023957673.py:83: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  p24_poc["flag_any_gas_hypercapnia_poc"] = (p24_poc["pco2"] >= 45).astype(int)\n/var/folders/vf/n84t7b8171lf64smq4ddvw1m0000gn/T/ipykernel_80016/4023957673.py:83: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  p24_poc["flag_any_gas_hypercapnia_poc"] = (p24_poc["pco2"] >= 45).astype(int)\n/var/folders/vf/n84t7b8171lf64smq4ddvw1m0000gn/T/ipykernel_80016/4023957673.py:83: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  p24_poc["flag_any_gas_hypercapnia_poc"] = (p24_poc["pco2"] >= 45).astype(int)\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 80
[NbConvertApp] Executing cell:
# OMR BMI/height/weight

try:
    omr_sql = f"""
    SELECT subject_id, chartdate, result_name, result_value
    FROM `{PHYS}.{HOSP}.omr`
    WHERE LOWER(result_name) IN ('bmi','height','weight')
    """
    omr = run_sql_bq(omr_sql)
    print("OMR rows:", len(omr))

    if len(omr) == 0:
        print("OMR query returned 0 rows; BMI/height/weight will remain missing.")
    else:
        omr["result_name"] = omr["result_name"].str.lower()
        omr["result_value_num"] = (
            omr["result_value"].astype(str)
            .str.extract(r"(-?\d+(?:\.\d+)?)", expand=False)
        )
        omr["result_value_num"] = pd.to_numeric(omr["result_value_num"], errors="coerce")

        omr_pivot = (
            omr.pivot_table(
                index=["subject_id","chartdate"],
                columns="result_name",
                values="result_value_num",
                aggfunc="first",
            )
            .reset_index()
        )

        # attach closest pre-ED (within 365 days)
        ed_dates = ed_df[["ed_stay_id","subject_id","ed_intime"]].copy()
        ed_dates["ed_date"] = ed_dates["ed_intime"].dt.date
        omr_pivot["chartdate"] = pd.to_datetime(omr_pivot["chartdate"]).dt.date

        merged = ed_dates.merge(omr_pivot, on="subject_id", how="left")
        merged["days_before"] = (pd.to_datetime(merged["ed_date"]) - pd.to_datetime(merged["chartdate"])).dt.days
        merged = merged.loc[(merged["days_before"] >= 0) & (merged["days_before"] <= 365)]

        closest = (
            merged.sort_values(["ed_stay_id","days_before"]).groupby("ed_stay_id", as_index=False).first()
            .rename(columns={
                "bmi":"bmi_closest_pre_ed",
                "height":"height_closest_pre_ed",
                "weight":"weight_closest_pre_ed",
            })
        )

        ed_df = ed_df.merge(
            closest[["ed_stay_id","bmi_closest_pre_ed","height_closest_pre_ed","weight_closest_pre_ed"]],
            on="ed_stay_id", how="left"
        )
except Exception as e:
    print("OMR not available or failed:", e)


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# OMR BMI/height/weight\n\ntry:\n    omr_sql = f"""\n    SELECT subject_id, chartdate, result_name, result_value\n    FROM `{PHYS}.{HOSP}.omr`\n    WHERE LOWER(result_name) IN (\'bmi\',\'height\',\'weight\')\n    """\n    omr = run_sql_bq(omr_sql)\n    print("OMR rows:", len(omr))\n\n    if len(omr) == 0:\n        print("OMR query returned 0 rows; BMI/height/weight will remain missing.")\n    else:\n        omr["result_name"] = omr["result_name"].str.lower()\n        omr["result_value_num"] = (\n            omr["result_value"].astype(str)\n            .str.extract(r"(-?\\d+(?:\\.\\d+)?)", expand=False)\n        )\n        omr["result_value_num"] = pd.to_numeric(omr["result_value_num"], errors="coerce")\n\n        omr_pivot = (\n            omr.pivot_table(\n                index=["subject_id","chartdate"],\n                columns="result_name",\n                values="result_value_num",\n                aggfunc="first",\n            )\n            .reset_index()\n        )\n\n        # attach closest pre-ED (within 365 days)\n        ed_dates = ed_df[["ed_stay_id","subject_id","ed_intime"]].copy()\n        ed_dates["ed_date"] = ed_dates["ed_intime"].dt.date\n        omr_pivot["chartdate"] = pd.to_datetime(omr_pivot["chartdate"]).dt.date\n\n        merged = ed_dates.merge(omr_pivot, on="subject_id", how="left")\n        merged["days_before"] = (pd.to_datetime(merged["ed_date"]) - pd.to_datetime(merged["chartdate"])).dt.days\n        merged = merged.loc[(merged["days_before"] >= 0) & (merged["days_before"] <= 365)]\n\n        closest = (\n            merged.sort_values(["ed_stay_id","days_before"]).groupby("ed_stay_id", as_index=False).first()\n            .rename(columns={\n                "bmi":"bmi_closest_pre_ed",\n                "height":"height_closest_pre_ed",\n                "weight":"weight_closest_pre_ed",\n            })\n        )\n\n        ed_df = ed_df.merge(\n            closest[["ed_stay_id","bmi_closest_pre_ed","height_closest_pre_ed","weight_closest_pre_ed"]],\n            on="ed_stay_id", how="left"\n        )\nexcept Exception as e:\n    print("OMR not available or failed:", e)\n\n', 'execution_count': 37}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'OMR rows: 46494\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 82
[NbConvertApp] Executing cell:
# ICD code pulls for comorbidity flags (hospital + ED; combined OR)
# NOTE: Use prefix filters to reduce CPU and avoid regex-heavy scans.

FLAGS = [
    "flag_copd","flag_osa_ohs","flag_chf","flag_neuromuscular",
    "flag_opioid_substance","flag_pneumonia",
]

SQL["icd_flags_sql"] = f"""
WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x),
icd AS (
  SELECT
    hadm_id,
    UPPER(REPLACE(icd_code, \".\", \"\")) AS code_norm
  FROM `{PHYS}.{HOSP}.diagnoses_icd`
  WHERE hadm_id IN (SELECT hadm_id FROM hadms)
    AND (
      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"J43\") OR
      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"J44\") OR
      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"G473\") OR
      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"E662\") OR
      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"I50\") OR
      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"G12\") OR
      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"G70\") OR
      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"G71\") OR
      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"F11\") OR
      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"T40\") OR
      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"F13\") OR
      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"J12\") OR
      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"J13\") OR
      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"J14\") OR
      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"J15\") OR
      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"J16\") OR
      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"J17\") OR
      STARTS_WITH(UPPER(REPLACE(icd_code, \".\", \"\")), \"J18\")
    )
)
SELECT
  hadm_id,
  MAX(IF(STARTS_WITH(code_norm, \"J43\") OR STARTS_WITH(code_norm, \"J44\"), 1, 0)) AS flag_copd,
  MAX(IF(STARTS_WITH(code_norm, \"G473\") OR STARTS_WITH(code_norm, \"E662\"), 1, 0)) AS flag_osa_ohs,
  MAX(IF(STARTS_WITH(code_norm, \"I50\"), 1, 0)) AS flag_chf,
  MAX(IF(STARTS_WITH(code_norm, \"G12\") OR STARTS_WITH(code_norm, \"G70\") OR STARTS_WITH(code_norm, \"G71\"), 1, 0)) AS flag_neuromuscular,
  MAX(IF(STARTS_WITH(code_norm, \"F11\") OR STARTS_WITH(code_norm, \"T40\") OR STARTS_WITH(code_norm, \"F13\"), 1, 0)) AS flag_opioid_substance,
  MAX(IF(STARTS_WITH(code_norm, \"J12\") OR STARTS_WITH(code_norm, \"J13\") OR STARTS_WITH(code_norm, \"J14\") OR STARTS_WITH(code_norm, \"J15\") OR STARTS_WITH(code_norm, \"J16\") OR STARTS_WITH(code_norm, \"J17\") OR STARTS_WITH(code_norm, \"J18\"), 1, 0)) AS flag_pneumonia
FROM icd
GROUP BY hadm_id
"""

SQL["ed_icd_flags_sql"] = f"""
WITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x),
ed_dx AS (
  SELECT
    s.hadm_id,
    UPPER(REPLACE(d.icd_code, \".\", \"\")) AS code_norm
  FROM `{PHYS}.{ED}.diagnosis` d
  JOIN `{PHYS}.{ED}.edstays` s
    ON s.stay_id = d.stay_id
  WHERE s.hadm_id IN (SELECT hadm_id FROM hadms)
    AND (
      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"J43\") OR
      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"J44\") OR
      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"G473\") OR
      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"E662\") OR
      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"I50\") OR
      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"G12\") OR
      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"G70\") OR
      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"G71\") OR
      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"F11\") OR
      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"T40\") OR
      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"F13\") OR
      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"J12\") OR
      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"J13\") OR
      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"J14\") OR
      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"J15\") OR
      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"J16\") OR
      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"J17\") OR
      STARTS_WITH(UPPER(REPLACE(d.icd_code, \".\", \"\")), \"J18\")
    )
)
SELECT
  hadm_id,
  MAX(IF(STARTS_WITH(code_norm, \"J43\") OR STARTS_WITH(code_norm, \"J44\"), 1, 0)) AS flag_copd,
  MAX(IF(STARTS_WITH(code_norm, \"G473\") OR STARTS_WITH(code_norm, \"E662\"), 1, 0)) AS flag_osa_ohs,
  MAX(IF(STARTS_WITH(code_norm, \"I50\"), 1, 0)) AS flag_chf,
  MAX(IF(STARTS_WITH(code_norm, \"G12\") OR STARTS_WITH(code_norm, \"G70\") OR STARTS_WITH(code_norm, \"G71\"), 1, 0)) AS flag_neuromuscular,
  MAX(IF(STARTS_WITH(code_norm, \"F11\") OR STARTS_WITH(code_norm, \"T40\") OR STARTS_WITH(code_norm, \"F13\"), 1, 0)) AS flag_opioid_substance,
  MAX(IF(STARTS_WITH(code_norm, \"J12\") OR STARTS_WITH(code_norm, \"J13\") OR STARTS_WITH(code_norm, \"J14\") OR STARTS_WITH(code_norm, \"J15\") OR STARTS_WITH(code_norm, \"J16\") OR STARTS_WITH(code_norm, \"J17\") OR STARTS_WITH(code_norm, \"J18\"), 1, 0)) AS flag_pneumonia
FROM ed_dx
GROUP BY hadm_id
"""

flag_hosp = run_sql_bq(sql("icd_flags_sql"), {"hadms": hadm_list})
flag_ed = run_sql_bq(sql("ed_icd_flags_sql"), {"hadms": hadm_list})

flag_hosp = flag_hosp.rename(columns={k: f"{k}_hosp" for k in FLAGS})
flag_ed = flag_ed.rename(columns={k: f"{k}_ed" for k in FLAGS})

flag_df = flag_hosp.merge(flag_ed, on="hadm_id", how="outer")
for k in FLAGS:
    hosp_col = f"{k}_hosp"
    ed_col = f"{k}_ed"
    if hosp_col not in flag_df.columns:
        flag_df[hosp_col] = 0
    if ed_col not in flag_df.columns:
        flag_df[ed_col] = 0
    flag_df[k] = ((flag_df[hosp_col].fillna(0).astype(int) == 1) | (flag_df[ed_col].fillna(0).astype(int) == 1)).astype(int)

# merge into admission-level and ED-stay-level frames (override if present)
for _df_name in ["df", "ed_df"]:
    if _df_name in globals():
        _df = globals()[_df_name]
        drop_cols = [c for c in flag_df.columns if c != "hadm_id" and c in _df.columns]
        if drop_cols:
            _df = _df.drop(columns=drop_cols)
        _df = _df.merge(flag_df, on="hadm_id", how="left")
        globals()[_df_name] = _df

# prevalence (combined flags)
for k in FLAGS:
    if k in ed_df.columns:
        print(k, int(ed_df[k].fillna(0).sum()))


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# ICD code pulls for comorbidity flags (hospital + ED; combined OR)\n# NOTE: Use prefix filters to reduce CPU and avoid regex-heavy scans.\n\nFLAGS = [\n    "flag_copd","flag_osa_ohs","flag_chf","flag_neuromuscular",\n    "flag_opioid_substance","flag_pneumonia",\n]\n\nSQL["icd_flags_sql"] = f"""\nWITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x),\nicd AS (\n  SELECT\n    hadm_id,\n    UPPER(REPLACE(icd_code, \\".\\", \\"\\")) AS code_norm\n  FROM `{PHYS}.{HOSP}.diagnoses_icd`\n  WHERE hadm_id IN (SELECT hadm_id FROM hadms)\n    AND (\n      STARTS_WITH(UPPER(REPLACE(icd_code, \\".\\", \\"\\")), \\"J43\\") OR\n      STARTS_WITH(UPPER(REPLACE(icd_code, \\".\\", \\"\\")), \\"J44\\") OR\n      STARTS_WITH(UPPER(REPLACE(icd_code, \\".\\", \\"\\")), \\"G473\\") OR\n      STARTS_WITH(UPPER(REPLACE(icd_code, \\".\\", \\"\\")), \\"E662\\") OR\n      STARTS_WITH(UPPER(REPLACE(icd_code, \\".\\", \\"\\")), \\"I50\\") OR\n      STARTS_WITH(UPPER(REPLACE(icd_code, \\".\\", \\"\\")), \\"G12\\") OR\n      STARTS_WITH(UPPER(REPLACE(icd_code, \\".\\", \\"\\")), \\"G70\\") OR\n      STARTS_WITH(UPPER(REPLACE(icd_code, \\".\\", \\"\\")), \\"G71\\") OR\n      STARTS_WITH(UPPER(REPLACE(icd_code, \\".\\", \\"\\")), \\"F11\\") OR\n      STARTS_WITH(UPPER(REPLACE(icd_code, \\".\\", \\"\\")), \\"T40\\") OR\n      STARTS_WITH(UPPER(REPLACE(icd_code, \\".\\", \\"\\")), \\"F13\\") OR\n      STARTS_WITH(UPPER(REPLACE(icd_code, \\".\\", \\"\\")), \\"J12\\") OR\n      STARTS_WITH(UPPER(REPLACE(icd_code, \\".\\", \\"\\")), \\"J13\\") OR\n      STARTS_WITH(UPPER(REPLACE(icd_code, \\".\\", \\"\\")), \\"J14\\") OR\n      STARTS_WITH(UPPER(REPLACE(icd_code, \\".\\", \\"\\")), \\"J15\\") OR\n      STARTS_WITH(UPPER(REPLACE(icd_code, \\".\\", \\"\\")), \\"J16\\") OR\n      STARTS_WITH(UPPER(REPLACE(icd_code, \\".\\", \\"\\")), \\"J17\\") OR\n      STARTS_WITH(UPPER(REPLACE(icd_code, \\".\\", \\"\\")), \\"J18\\")\n    )\n)\nSELECT\n  hadm_id,\n  MAX(IF(STARTS_WITH(code_norm, \\"J43\\") OR STARTS_WITH(code_norm, \\"J44\\"), 1, 0)) AS flag_copd,\n  MAX(IF(STARTS_WITH(code_norm, \\"G473\\") OR STARTS_WITH(code_norm, \\"E662\\"), 1, 0)) AS flag_osa_ohs,\n  MAX(IF(STARTS_WITH(code_norm, \\"I50\\"), 1, 0)) AS flag_chf,\n  MAX(IF(STARTS_WITH(code_norm, \\"G12\\") OR STARTS_WITH(code_norm, \\"G70\\") OR STARTS_WITH(code_norm, \\"G71\\"), 1, 0)) AS flag_neuromuscular,\n  MAX(IF(STARTS_WITH(code_norm, \\"F11\\") OR STARTS_WITH(code_norm, \\"T40\\") OR STARTS_WITH(code_norm, \\"F13\\"), 1, 0)) AS flag_opioid_substance,\n  MAX(IF(STARTS_WITH(code_norm, \\"J12\\") OR STARTS_WITH(code_norm, \\"J13\\") OR STARTS_WITH(code_norm, \\"J14\\") OR STARTS_WITH(code_norm, \\"J15\\") OR STARTS_WITH(code_norm, \\"J16\\") OR STARTS_WITH(code_norm, \\"J17\\") OR STARTS_WITH(code_norm, \\"J18\\"), 1, 0)) AS flag_pneumonia\nFROM icd\nGROUP BY hadm_id\n"""\n\nSQL["ed_icd_flags_sql"] = f"""\nWITH hadms AS (SELECT x AS hadm_id FROM UNNEST(@hadms) AS x),\ned_dx AS (\n  SELECT\n    s.hadm_id,\n    UPPER(REPLACE(d.icd_code, \\".\\", \\"\\")) AS code_norm\n  FROM `{PHYS}.{ED}.diagnosis` d\n  JOIN `{PHYS}.{ED}.edstays` s\n    ON s.stay_id = d.stay_id\n  WHERE s.hadm_id IN (SELECT hadm_id FROM hadms)\n    AND (\n      STARTS_WITH(UPPER(REPLACE(d.icd_code, \\".\\", \\"\\")), \\"J43\\") OR\n      STARTS_WITH(UPPER(REPLACE(d.icd_code, \\".\\", \\"\\")), \\"J44\\") OR\n      STARTS_WITH(UPPER(REPLACE(d.icd_code, \\".\\", \\"\\")), \\"G473\\") OR\n      STARTS_WITH(UPPER(REPLACE(d.icd_code, \\".\\", \\"\\")), \\"E662\\") OR\n      STARTS_WITH(UPPER(REPLACE(d.icd_code, \\".\\", \\"\\")), \\"I50\\") OR\n      STARTS_WITH(UPPER(REPLACE(d.icd_code, \\".\\", \\"\\")), \\"G12\\") OR\n      STARTS_WITH(UPPER(REPLACE(d.icd_code, \\".\\", \\"\\")), \\"G70\\") OR\n      STARTS_WITH(UPPER(REPLACE(d.icd_code, \\".\\", \\"\\")), \\"G71\\") OR\n      STARTS_WITH(UPPER(REPLACE(d.icd_code, \\".\\", \\"\\")), \\"F11\\") OR\n      STARTS_WITH(UPPER(REPLACE(d.icd_code, \\".\\", \\"\\")), \\"T40\\") OR\n      STARTS_WITH(UPPER(REPLACE(d.icd_code, \\".\\", \\"\\")), \\"F13\\") OR\n      STARTS_WITH(UPPER(REPLACE(d.icd_code, \\".\\", \\"\\")), \\"J12\\") OR\n      STARTS_WITH(UPPER(REPLACE(d.icd_code, \\".\\", \\"\\")), \\"J13\\") OR\n      STARTS_WITH(UPPER(REPLACE(d.icd_code, \\".\\", \\"\\")), \\"J14\\") OR\n      STARTS_WITH(UPPER(REPLACE(d.icd_code, \\".\\", \\"\\")), \\"J15\\") OR\n      STARTS_WITH(UPPER(REPLACE(d.icd_code, \\".\\", \\"\\")), \\"J16\\") OR\n      STARTS_WITH(UPPER(REPLACE(d.icd_code, \\".\\", \\"\\")), \\"J17\\") OR\n      STARTS_WITH(UPPER(REPLACE(d.icd_code, \\".\\", \\"\\")), \\"J18\\")\n    )\n)\nSELECT\n  hadm_id,\n  MAX(IF(STARTS_WITH(code_norm, \\"J43\\") OR STARTS_WITH(code_norm, \\"J44\\"), 1, 0)) AS flag_copd,\n  MAX(IF(STARTS_WITH(code_norm, \\"G473\\") OR STARTS_WITH(code_norm, \\"E662\\"), 1, 0)) AS flag_osa_ohs,\n  MAX(IF(STARTS_WITH(code_norm, \\"I50\\"), 1, 0)) AS flag_chf,\n  MAX(IF(STARTS_WITH(code_norm, \\"G12\\") OR STARTS_WITH(code_norm, \\"G70\\") OR STARTS_WITH(code_norm, \\"G71\\"), 1, 0)) AS flag_neuromuscular,\n  MAX(IF(STARTS_WITH(code_norm, \\"F11\\") OR STARTS_WITH(code_norm, \\"T40\\") OR STARTS_WITH(code_norm, \\"F13\\"), 1, 0)) AS flag_opioid_substance,\n  MAX(IF(STARTS_WITH(code_norm, \\"J12\\") OR STARTS_WITH(code_norm, \\"J13\\") OR STARTS_WITH(code_norm, \\"J14\\") OR STARTS_WITH(code_norm, \\"J15\\") OR STARTS_WITH(code_norm, \\"J16\\") OR STARTS_WITH(code_norm, \\"J17\\") OR STARTS_WITH(code_norm, \\"J18\\"), 1, 0)) AS flag_pneumonia\nFROM ed_dx\nGROUP BY hadm_id\n"""\n\nflag_hosp = run_sql_bq(sql("icd_flags_sql"), {"hadms": hadm_list})\nflag_ed = run_sql_bq(sql("ed_icd_flags_sql"), {"hadms": hadm_list})\n\nflag_hosp = flag_hosp.rename(columns={k: f"{k}_hosp" for k in FLAGS})\nflag_ed = flag_ed.rename(columns={k: f"{k}_ed" for k in FLAGS})\n\nflag_df = flag_hosp.merge(flag_ed, on="hadm_id", how="outer")\nfor k in FLAGS:\n    hosp_col = f"{k}_hosp"\n    ed_col = f"{k}_ed"\n    if hosp_col not in flag_df.columns:\n        flag_df[hosp_col] = 0\n    if ed_col not in flag_df.columns:\n        flag_df[ed_col] = 0\n    flag_df[k] = ((flag_df[hosp_col].fillna(0).astype(int) == 1) | (flag_df[ed_col].fillna(0).astype(int) == 1)).astype(int)\n\n# merge into admission-level and ED-stay-level frames (override if present)\nfor _df_name in ["df", "ed_df"]:\n    if _df_name in globals():\n        _df = globals()[_df_name]\n        drop_cols = [c for c in flag_df.columns if c != "hadm_id" and c in _df.columns]\n        if drop_cols:\n            _df = _df.drop(columns=drop_cols)\n        _df = _df.merge(flag_df, on="hadm_id", how="left")\n        globals()[_df_name] = _df\n\n# prevalence (combined flags)\nfor k in FLAGS:\n    if k in ed_df.columns:\n        print(k, int(ed_df[k].fillna(0).sum()))\n\n', 'execution_count': 38}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'flag_copd 2760\nflag_osa_ohs 1891\nflag_chf 4685\nflag_neuromuscular 106\nflag_opioid_substance 830\nflag_pneumonia 2878\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 84
[NbConvertApp] Executing cell:
# Timing phenotypes

anchor = "ed_intime_first" if "ed_intime_first" in ed_df.columns else "ed_intime"

ed_df["dt_first_qualifying_gas_hours"] = (ed_df["first_gas_time"] - ed_df[anchor]).dt.total_seconds() / 3600.0
ed_df["presenting_hypercapnia"] = (ed_df["dt_first_qualifying_gas_hours"] <= 6).astype("Int64")
ed_df["late_hypercapnia"] = (ed_df["dt_first_qualifying_gas_hours"] > 6).astype("Int64")

# NIV/IMV timing relative to first ED presentation
if "first_imv_time" in ed_df.columns:
    ed_df["dt_first_imv_hours"] = (ed_df["first_imv_time"] - ed_df[anchor]).dt.total_seconds() / 3600.0
if "first_niv_time" in ed_df.columns:
    ed_df["dt_first_niv_hours"] = (ed_df["first_niv_time"] - ed_df[anchor]).dt.total_seconds() / 3600.0

# Bands
bins_ph = [-1, 7.20, 7.30, 7.35, 99]
labels_ph = ["<7.20","7.20–7.29","7.30–7.34","≥7.35"]
ed_df["ph_band"] = pd.cut(ed_df["first_ph"], bins=bins_ph, labels=labels_ph)

bins_hco3 = [-1, 24, 30, 999]
labels_hco3 = ["<24","24–29","≥30"]
ed_df["hco3_band"] = pd.cut(ed_df["first_hco3"], bins=bins_hco3, labels=labels_hco3)

bins_lac = [-1, 2, 4, 999]
labels_lac = ["<2","2–4",">4"]
ed_df["lactate_band"] = pd.cut(ed_df["first_lactate"], bins=bins_lac, labels=labels_lac)


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Timing phenotypes\n\nanchor = "ed_intime_first" if "ed_intime_first" in ed_df.columns else "ed_intime"\n\ned_df["dt_first_qualifying_gas_hours"] = (ed_df["first_gas_time"] - ed_df[anchor]).dt.total_seconds() / 3600.0\ned_df["presenting_hypercapnia"] = (ed_df["dt_first_qualifying_gas_hours"] <= 6).astype("Int64")\ned_df["late_hypercapnia"] = (ed_df["dt_first_qualifying_gas_hours"] > 6).astype("Int64")\n\n# NIV/IMV timing relative to first ED presentation\nif "first_imv_time" in ed_df.columns:\n    ed_df["dt_first_imv_hours"] = (ed_df["first_imv_time"] - ed_df[anchor]).dt.total_seconds() / 3600.0\nif "first_niv_time" in ed_df.columns:\n    ed_df["dt_first_niv_hours"] = (ed_df["first_niv_time"] - ed_df[anchor]).dt.total_seconds() / 3600.0\n\n# Bands\nbins_ph = [-1, 7.20, 7.30, 7.35, 99]\nlabels_ph = ["<7.20","7.20–7.29","7.30–7.34","≥7.35"]\ned_df["ph_band"] = pd.cut(ed_df["first_ph"], bins=bins_ph, labels=labels_ph)\n\nbins_hco3 = [-1, 24, 30, 999]\nlabels_hco3 = ["<24","24–29","≥30"]\ned_df["hco3_band"] = pd.cut(ed_df["first_hco3"], bins=bins_hco3, labels=labels_hco3)\n\nbins_lac = [-1, 2, 4, 999]\nlabels_lac = ["<2","2–4",">4"]\ned_df["lactate_band"] = pd.cut(ed_df["first_lactate"], bins=bins_lac, labels=labels_lac)\n\n', 'execution_count': 39}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 86
[NbConvertApp] Executing cell:
# --- QA checks (lightweight, deterministic)
STRICT_QA = False

# 1) Key uniqueness
if 'ed_df' in globals() and 'ed_stay_id' in ed_df.columns:
    try:
        assert_unique(ed_df, 'ed_stay_id', 'ed_df')
        print('QA: ed_stay_id unique OK')
    except Exception as e:
        print('QA FAIL:', e)
        if STRICT_QA:
            raise

# 2) Inclusion sanity: any_hypercap_icd==0 implies gas criteria met
if 'ed_df' in globals() and 'any_hypercap_icd' in ed_df.columns:
    gas_flag = ed_df.get('flag_any_gas_hypercapnia', pd.Series(0, index=ed_df.index)).fillna(0).astype(int)
    icd_flag = ed_df['any_hypercap_icd'].fillna(0).astype(int)
    viol = (icd_flag == 0) & (gas_flag == 0)
    n_viol = int(viol.sum())
    print('QA: ICD==0 & Gas==0 count:', n_viol)
    if STRICT_QA and n_viol > 0:
        raise AssertionError('Found rows without ICD and without gas criteria.')

# 3) Temporal sanity: first_gas_time >= ed_intime (allow small negative drift)
if 'ed_df' in globals() and 'first_gas_time' in ed_df.columns and 'ed_intime' in ed_df.columns:
    dt = (pd.to_datetime(ed_df['first_gas_time']) - pd.to_datetime(ed_df['ed_intime'])).dt.total_seconds() / 3600
    n_neg = int((dt < -1).sum())  # allow 1h clock drift
    print('QA: first_gas_time < ed_intime by >1h:', n_neg)
    if STRICT_QA and n_neg > 0:
        raise AssertionError('first_gas_time before ed_intime by >1h')

# 4) Range checks (report only)
if 'ed_df' in globals():
    ranges = {
        'first_ph': (6.8, 7.8),
        'first_pco2': (10, 200),
        'first_lactate': (0, 30),
        'creatinine': (0, 20),
    }
    rc = check_ranges(ed_df, ranges)
    if not rc.empty:
        print('QA range check (n out-of-range):')
        print(rc.to_string(index=False))


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': "# --- QA checks (lightweight, deterministic)\nSTRICT_QA = False\n\n# 1) Key uniqueness\nif 'ed_df' in globals() and 'ed_stay_id' in ed_df.columns:\n    try:\n        assert_unique(ed_df, 'ed_stay_id', 'ed_df')\n        print('QA: ed_stay_id unique OK')\n    except Exception as e:\n        print('QA FAIL:', e)\n        if STRICT_QA:\n            raise\n\n# 2) Inclusion sanity: any_hypercap_icd==0 implies gas criteria met\nif 'ed_df' in globals() and 'any_hypercap_icd' in ed_df.columns:\n    gas_flag = ed_df.get('flag_any_gas_hypercapnia', pd.Series(0, index=ed_df.index)).fillna(0).astype(int)\n    icd_flag = ed_df['any_hypercap_icd'].fillna(0).astype(int)\n    viol = (icd_flag == 0) & (gas_flag == 0)\n    n_viol = int(viol.sum())\n    print('QA: ICD==0 & Gas==0 count:', n_viol)\n    if STRICT_QA and n_viol > 0:\n        raise AssertionError('Found rows without ICD and without gas criteria.')\n\n# 3) Temporal sanity: first_gas_time >= ed_intime (allow small negative drift)\nif 'ed_df' in globals() and 'first_gas_time' in ed_df.columns and 'ed_intime' in ed_df.columns:\n    dt = (pd.to_datetime(ed_df['first_gas_time']) - pd.to_datetime(ed_df['ed_intime'])).dt.total_seconds() / 3600\n    n_neg = int((dt < -1).sum())  # allow 1h clock drift\n    print('QA: first_gas_time < ed_intime by >1h:', n_neg)\n    if STRICT_QA and n_neg > 0:\n        raise AssertionError('first_gas_time before ed_intime by >1h')\n\n# 4) Range checks (report only)\nif 'ed_df' in globals():\n    ranges = {\n        'first_ph': (6.8, 7.8),\n        'first_pco2': (10, 200),\n        'first_lactate': (0, 30),\n        'creatinine': (0, 20),\n    }\n    rc = check_ranges(ed_df, ranges)\n    if not rc.empty:\n        print('QA range check (n out-of-range):')\n        print(rc.to_string(index=False))\n\n", 'execution_count': 40}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'QA: ed_stay_id unique OK\nQA: first_gas_time < ed_intime by >1h: 0\nQA range check (n out-of-range):\n          col  n_bad\n     first_ph      7\n   first_pco2      8\nfirst_lactate      0\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
import json

# T1 — uniqueness and join explosion guard
if ed_df["ed_stay_id"].nunique() != len(ed_df):
    raise ValueError("ed_stay_id not unique after merges")

# T2 — missingness summary for new fields
new_fields = [c for c in TARGET_FIELDS if c in ed_df.columns]
miss = pd.DataFrame({
    "field": new_fields,
    "missing_n": [int(ed_df[c].isna().sum()) for c in new_fields],
    "missing_pct": [float(ed_df[c].isna().mean()) for c in new_fields],
})
print(miss.sort_values("missing_pct", ascending=False).head(30))

# T3 — lab capture completeness
pct_any_6h = float(p06["ed_stay_id"].nunique() / max(ed_df["ed_stay_id"].nunique(),1))
pct_any_24h = float(p24["ed_stay_id"].nunique() / max(ed_df["ed_stay_id"].nunique(),1))
print("% any gas panel 0–6h:", round(pct_any_6h*100,1))
print("% any gas panel 0–24h:", round(pct_any_24h*100,1))

# Use per-stay unknown rate if available
if "gas_source_unknown_rate" in ed_df.columns:
    source_unknown_rate = float(ed_df["gas_source_unknown_rate"].mean())
else:
    source_unknown_rate = 1.0
print("% source unknown (panel-level):", round(source_unknown_rate*100,1))

# T4 — QA summary artifact
qa_summary = {
    "ed_rows": int(len(ed_df)),
    "ed_unique": int(ed_df["ed_stay_id"].nunique()),
    "icu_link_rate": float(ed_df["icu_intime_first"].notna().mean()) if "icu_intime_first" in ed_df.columns else None,
    "pct_any_gas_0_6h": pct_any_6h,
    "pct_any_gas_0_24h": pct_any_24h,
    "source_unknown_rate": source_unknown_rate,
}
qa_path = WORK_DIR / "qa_summary.json"
qa_path.write_text(json.dumps(qa_summary, indent=2))
print("Wrote:", qa_path)


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': 'import json\n\n# T1 — uniqueness and join explosion guard\nif ed_df["ed_stay_id"].nunique() != len(ed_df):\n    raise ValueError("ed_stay_id not unique after merges")\n\n# T2 — missingness summary for new fields\nnew_fields = [c for c in TARGET_FIELDS if c in ed_df.columns]\nmiss = pd.DataFrame({\n    "field": new_fields,\n    "missing_n": [int(ed_df[c].isna().sum()) for c in new_fields],\n    "missing_pct": [float(ed_df[c].isna().mean()) for c in new_fields],\n})\nprint(miss.sort_values("missing_pct", ascending=False).head(30))\n\n# T3 — lab capture completeness\npct_any_6h = float(p06["ed_stay_id"].nunique() / max(ed_df["ed_stay_id"].nunique(),1))\npct_any_24h = float(p24["ed_stay_id"].nunique() / max(ed_df["ed_stay_id"].nunique(),1))\nprint("% any gas panel 0–6h:", round(pct_any_6h*100,1))\nprint("% any gas panel 0–24h:", round(pct_any_24h*100,1))\n\n# Use per-stay unknown rate if available\nif "gas_source_unknown_rate" in ed_df.columns:\n    source_unknown_rate = float(ed_df["gas_source_unknown_rate"].mean())\nelse:\n    source_unknown_rate = 1.0\nprint("% source unknown (panel-level):", round(source_unknown_rate*100,1))\n\n# T4 — QA summary artifact\nqa_summary = {\n    "ed_rows": int(len(ed_df)),\n    "ed_unique": int(ed_df["ed_stay_id"].nunique()),\n    "icu_link_rate": float(ed_df["icu_intime_first"].notna().mean()) if "icu_intime_first" in ed_df.columns else None,\n    "pct_any_gas_0_6h": pct_any_6h,\n    "pct_any_gas_0_24h": pct_any_24h,\n    "source_unknown_rate": source_unknown_rate,\n}\nqa_path = WORK_DIR / "qa_summary.json"\nqa_path.write_text(json.dumps(qa_summary, indent=2))\nprint("Wrote:", qa_path)\n\n', 'execution_count': 41}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '                    field  missing_n  missing_pct\n76  weight_closest_pre_ed      17449     1.000000\n4      bmi_closest_pre_ed      17449     1.000000\n54  height_closest_pre_ed      17449     1.000000\n53              hco3_band      16866     0.966588\n38             first_hco3      16866     0.966588\n16        ed_first_rhythm      16748     0.959826\n5               deathtime      15131     0.867156\n69          max_pco2_0_6h      13399     0.767895\n71            min_ph_0_6h      13278     0.760961\n10     dt_first_niv_hours      12743     0.730300\n49         flag_pneumonia       9526     0.545934\n44               flag_chf       9526     0.545934\n45              flag_copd       9526     0.545934\n46     flag_neuromuscular       9526     0.545934\n47  flag_opioid_substance       9526     0.545934\n48           flag_osa_ohs       9526     0.545934\n9      dt_first_imv_hours       7150     0.409766\n63           lactate_band       6141     0.351940\n39          first_lactate       6141     0.351940\n68         max_pco2_0_24h       6027     0.345407\n40             first_pco2       6027     0.345407\n60       icu_outtime_last       5888     0.337441\n59          icu_los_total       5888     0.337441\n58       icu_intime_first       5888     0.337441\n36         first_careunit       5888     0.337441\n65          last_careunit       5888     0.337441\n72            n_icu_stays       5888     0.337441\n70           min_ph_0_24h       5737     0.328787\n73                ph_band       5737     0.328787\n41               first_ph       5737     0.328787\n% any gas panel 0–6h: 41.5\n% any gas panel 0–24h: 96.4\n% source unknown (panel-level): 100.0\nWrote: /Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/qa_summary.json\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 89
[NbConvertApp] Executing cell:
# Save outputs
cohort_path = DATA_DIR / "cohort_ed_stay.parquet"
ed_df.to_parquet(cohort_path, index=False)
print("Wrote:", cohort_path)

ed_vitals_long.to_parquet(DATA_DIR / "ed_vitals_long.parquet", index=False)
labs_long.to_parquet(DATA_DIR / "labs_long.parquet", index=False)
panel.to_parquet(DATA_DIR / "gas_panels.parquet", index=False)




[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Save outputs\ncohort_path = DATA_DIR / "cohort_ed_stay.parquet"\ned_df.to_parquet(cohort_path, index=False)\nprint("Wrote:", cohort_path)\n\ned_vitals_long.to_parquet(DATA_DIR / "ed_vitals_long.parquet", index=False)\nlabs_long.to_parquet(DATA_DIR / "labs_long.parquet", index=False)\npanel.to_parquet(DATA_DIR / "gas_panels.parquet", index=False)\n\n\n\n', 'execution_count': 42}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Wrote: /Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/MIMIC tabular data/cohort_ed_stay.parquet\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
# Also export to Excel (tabular dataset)
from datetime import datetime

out_dir = DATA_DIR
out_dir.mkdir(parents=True, exist_ok=True)
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

xlsx_path = out_dir / f"mimic_hypercap_EXT_EDstay_bq_gas_{timestamp}.xlsx"

with pd.ExcelWriter(xlsx_path, engine="openpyxl") as xw:
    ed_df.to_excel(xw, sheet_name="cohort_ed_stay", index=False)
    # Optional: include QA tables if present
    try:
        miss.to_excel(xw, sheet_name="missingness", index=False)
    except Exception:
        pass

print("Saved:", xlsx_path)


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Also export to Excel (tabular dataset)\nfrom datetime import datetime\n\nout_dir = DATA_DIR\nout_dir.mkdir(parents=True, exist_ok=True)\ntimestamp = datetime.now().strftime(\'%Y%m%d_%H%M%S\')\n\nxlsx_path = out_dir / f"mimic_hypercap_EXT_EDstay_bq_gas_{timestamp}.xlsx"\n\nwith pd.ExcelWriter(xlsx_path, engine="openpyxl") as xw:\n    ed_df.to_excel(xw, sheet_name="cohort_ed_stay", index=False)\n    # Optional: include QA tables if present\n    try:\n        miss.to_excel(xw, sheet_name="missingness", index=False)\n    except Exception:\n        pass\n\nprint("Saved:", xlsx_path)\n\n', 'execution_count': 43}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Saved: /Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/MIMIC tabular data/mimic_hypercap_EXT_EDstay_bq_gas_20260204_164401.xlsx\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
# Final Excel outputs requested: all encounters + ED chief-complaint only
from datetime import datetime

out_dir = DATA_DIR
out_dir.mkdir(parents=True, exist_ok=True)
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

# 1) All encounters meeting inclusion criteria (admission-level), with ED linkage flags
ed_link = (
    ed_df.groupby("hadm_id", as_index=False)
         .agg(ed_stay_id_first=("ed_stay_id", "first"), n_ed_stays=("ed_stay_id", "nunique"))
)
all_encounters = df.merge(ed_link, on="hadm_id", how="left")
all_encounters["has_ed_encounter"] = all_encounters["n_ed_stays"].fillna(0).astype(int).gt(0).astype(int)
all_encounters["encounter_source"] = all_encounters["has_ed_encounter"].map({1: "ED-linked", 0: "Inpatient-only"})

all_path = out_dir / f"mimic_hypercap_EXT_all_encounters_bq_{timestamp}.xlsx"
with pd.ExcelWriter(all_path, engine="openpyxl") as xw:
    all_encounters.to_excel(xw, sheet_name="all_encounters", index=False)
print("Saved:", all_path)

# 2) ED chief-complaint-only (ED-stay level)
if "ed_triage_cc" not in ed_df.columns:
    raise KeyError("Column 'ed_triage_cc' not found in ed_df. Ensure ED triage merge ran.")

mask_cc = ed_df["ed_triage_cc"].notna() & (ed_df["ed_triage_cc"].astype(str).str.strip() != "")
ed_cc_only = ed_df.loc[mask_cc].copy()

cc_path = out_dir / f"mimic_hypercap_EXT_EDcc_only_edstay_bq_{timestamp}.xlsx"
with pd.ExcelWriter(cc_path, engine="openpyxl") as xw:
    ed_cc_only.to_excel(xw, sheet_name="ed_cc_only", index=False)
print("Saved:", cc_path)


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Final Excel outputs requested: all encounters + ED chief-complaint only\nfrom datetime import datetime\n\nout_dir = DATA_DIR\nout_dir.mkdir(parents=True, exist_ok=True)\ntimestamp = datetime.now().strftime(\'%Y%m%d_%H%M%S\')\n\n# 1) All encounters meeting inclusion criteria (admission-level), with ED linkage flags\ned_link = (\n    ed_df.groupby("hadm_id", as_index=False)\n         .agg(ed_stay_id_first=("ed_stay_id", "first"), n_ed_stays=("ed_stay_id", "nunique"))\n)\nall_encounters = df.merge(ed_link, on="hadm_id", how="left")\nall_encounters["has_ed_encounter"] = all_encounters["n_ed_stays"].fillna(0).astype(int).gt(0).astype(int)\nall_encounters["encounter_source"] = all_encounters["has_ed_encounter"].map({1: "ED-linked", 0: "Inpatient-only"})\n\nall_path = out_dir / f"mimic_hypercap_EXT_all_encounters_bq_{timestamp}.xlsx"\nwith pd.ExcelWriter(all_path, engine="openpyxl") as xw:\n    all_encounters.to_excel(xw, sheet_name="all_encounters", index=False)\nprint("Saved:", all_path)\n\n# 2) ED chief-complaint-only (ED-stay level)\nif "ed_triage_cc" not in ed_df.columns:\n    raise KeyError("Column \'ed_triage_cc\' not found in ed_df. Ensure ED triage merge ran.")\n\nmask_cc = ed_df["ed_triage_cc"].notna() & (ed_df["ed_triage_cc"].astype(str).str.strip() != "")\ned_cc_only = ed_df.loc[mask_cc].copy()\n\ncc_path = out_dir / f"mimic_hypercap_EXT_EDcc_only_edstay_bq_{timestamp}.xlsx"\nwith pd.ExcelWriter(cc_path, engine="openpyxl") as xw:\n    ed_cc_only.to_excel(xw, sheet_name="ed_cc_only", index=False)\nprint("Saved:", cc_path)\n\n', 'execution_count': 44}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Saved: /Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/MIMIC tabular data/mimic_hypercap_EXT_all_encounters_bq_20260204_164414.xlsx\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Saved: /Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/MIMIC tabular data/mimic_hypercap_EXT_EDcc_only_edstay_bq_20260204_164414.xlsx\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
# Final CC output (cohort-only; excludes NLP-derived columns)
from datetime import datetime

# Ensure cc-only frames exist
if "df_cc" not in globals():
    raise NameError("df_cc not found. Run the ED-CC-only export cell first.")

if "ed_cc_only" not in globals():
    if "ed_df" not in globals():
        raise NameError("ed_df not found. Run the ED-stay build cells first.")
    if "ed_triage_cc" not in ed_df.columns:
        raise KeyError("ed_triage_cc missing in ed_df; cannot build ed_cc_only.")
    _mask_cc = ed_df["ed_triage_cc"].notna() & (ed_df["ed_triage_cc"].astype(str).str.strip() != "")
    ed_cc_only = ed_df.loc[_mask_cc].copy()

# Add ED-stay columns (dedup to earliest ED stay per hadm_id)
ed_tmp = ed_cc_only.copy()
if "ed_intime" in ed_tmp.columns:
    ed_tmp = ed_tmp.sort_values(["hadm_id", "ed_intime", "ed_stay_id"], na_position="last")
else:
    ed_tmp = ed_tmp.sort_values(["hadm_id", "ed_stay_id"], na_position="last")
ed_first = ed_tmp.drop_duplicates("hadm_id", keep="first")

add_ed_cols = [c for c in ed_first.columns if c not in df_cc.columns]

final_cc = df_cc.merge(ed_first[["hadm_id"] + add_ed_cols], on="hadm_id", how="left")

out_date = datetime.now().strftime("%Y-%m-%d")
out_path = DATA_DIR / f"{out_date} MIMICIV all with CC.xlsx"
final_cc.to_excel(out_path, index=False)

print("Saved:", out_path)
print("Base rows:", len(df_cc), "| Added from ED-stay:", len(add_ed_cols))
print("Total columns:", len(final_cc.columns))


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Final CC output (cohort-only; excludes NLP-derived columns)\nfrom datetime import datetime\n\n# Ensure cc-only frames exist\nif "df_cc" not in globals():\n    raise NameError("df_cc not found. Run the ED-CC-only export cell first.")\n\nif "ed_cc_only" not in globals():\n    if "ed_df" not in globals():\n        raise NameError("ed_df not found. Run the ED-stay build cells first.")\n    if "ed_triage_cc" not in ed_df.columns:\n        raise KeyError("ed_triage_cc missing in ed_df; cannot build ed_cc_only.")\n    _mask_cc = ed_df["ed_triage_cc"].notna() & (ed_df["ed_triage_cc"].astype(str).str.strip() != "")\n    ed_cc_only = ed_df.loc[_mask_cc].copy()\n\n# Add ED-stay columns (dedup to earliest ED stay per hadm_id)\ned_tmp = ed_cc_only.copy()\nif "ed_intime" in ed_tmp.columns:\n    ed_tmp = ed_tmp.sort_values(["hadm_id", "ed_intime", "ed_stay_id"], na_position="last")\nelse:\n    ed_tmp = ed_tmp.sort_values(["hadm_id", "ed_stay_id"], na_position="last")\ned_first = ed_tmp.drop_duplicates("hadm_id", keep="first")\n\nadd_ed_cols = [c for c in ed_first.columns if c not in df_cc.columns]\n\nfinal_cc = df_cc.merge(ed_first[["hadm_id"] + add_ed_cols], on="hadm_id", how="left")\n\nout_date = datetime.now().strftime("%Y-%m-%d")\nout_path = DATA_DIR / f"{out_date} MIMICIV all with CC.xlsx"\nfinal_cc.to_excel(out_path, index=False)\n\nprint("Saved:", out_path)\nprint("Base rows:", len(df_cc), "| Added from ED-stay:", len(add_ed_cols))\nprint("Total columns:", len(final_cc.columns))\n\n', 'execution_count': 45}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Saved: /Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/MIMIC tabular data/2026-02-04 MIMICIV all with CC.xlsx\nBase rows: 17424 | Added from ED-stay: 67\nTotal columns: 156\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
# Data dictionary (OSF-style) for final cohort output
from datetime import datetime
import numpy as np

# Choose target dataset for dictionary
if "final_cc" in globals():
    dd_target = final_cc
elif "df_cc" in globals():
    dd_target = df_cc
elif "df" in globals():
    dd_target = df
else:
    raise NameError("No cohort dataframe found for data dictionary (expected final_cc/df_cc/df).")

# Helper functions

def _readable(name: str) -> str:
    return name.replace("_", " ").strip().title()


def _infer_units(col: str) -> str:
    if col.endswith("_hours"):
        return "hours"
    if col.endswith("_time") or col.endswith("_intime") or col.endswith("_outtime") or col.endswith("_date"):
        return "datetime"
    if col.startswith("flag_") or col.endswith("_flag") or col.endswith("_before_imv"):
        return "binary (0/1)"
    if col.endswith("_paco2") or col.endswith("_pco2"):
        return "mmHg"
    if col.endswith("_ph"):
        return "pH"
    if col.endswith("_lactate"):
        return "mmol/L"
    return ""


def _allowed_values(series):
    s = series.dropna()
    if s.empty:
        return ""
    if s.dtype.kind in "biu":
        vals = sorted(map(int, s.unique()))
        if len(vals) <= 15:
            return ", ".join(map(str, vals))
        return f"{min(vals)}–{max(vals)}"
    if s.dtype.kind in "f":
        return f"{np.nanmin(s):.3g}–{np.nanmax(s):.3g}"
    # object/categorical
    vals = sorted(map(str, s.unique()))
    return ", ".join(vals[:15]) + (" …" if len(vals) > 15 else "")


META = {
    # IDs
    "subject_id": {
        "label": "Subject identifier",
        "definition": "Identifier for a unique patient in MIMIC-IV.",
        "source": "mimiciv_hosp.patients",
        "derivation": "as recorded",
    },
    "hadm_id": {
        "label": "Hospital admission identifier",
        "definition": "Identifier for a hospital admission.",
        "source": "mimiciv_hosp.admissions",
        "derivation": "as recorded",
    },
    "ed_stay_id": {
        "label": "ED stay identifier",
        "definition": "Identifier for an ED stay.",
        "source": "mimiciv_ed.edstays",
        "derivation": "as recorded",
    },
    "ed_intime": {
        "label": "ED arrival time",
        "definition": "ED arrival time for this ED stay.",
        "source": "mimiciv_ed.edstays",
        "derivation": "as recorded",
        "units": "datetime",
    },
    "ed_outtime": {
        "label": "ED departure time",
        "definition": "ED departure time for this ED stay.",
        "source": "mimiciv_ed.edstays",
        "derivation": "as recorded",
        "units": "datetime",
    },
    "ed_intime_first": {
        "label": "First ED arrival time (admission)",
        "definition": "Earliest ED arrival time among ED stays linked to the admission.",
        "source": "mimiciv_ed.edstays",
        "derivation": "min(edstays.intime) per hadm_id",
        "units": "datetime",
    },
    # Demographics / triage
    "ed_gender": {
        "label": "ED gender",
        "definition": "Gender as recorded in ED stay table.",
        "source": "mimiciv_ed.edstays",
        "derivation": "as recorded",
    },
    "ed_race": {
        "label": "ED race",
        "definition": "Race as recorded in ED stay table.",
        "source": "mimiciv_ed.edstays",
        "derivation": "as recorded",
    },
    "hosp_race": {
        "label": "Hospital race",
        "definition": "Race as recorded in admissions table.",
        "source": "mimiciv_hosp.admissions",
        "derivation": "as recorded",
    },
    "ed_triage_cc": {
        "label": "ED chief complaint",
        "definition": "Chief complaint recorded at ED triage.",
        "source": "mimiciv_ed.triage",
        "derivation": "as recorded",
    },
    "ed_triage_acuity": {
        "label": "ED triage acuity",
        "definition": "Triage acuity level recorded at ED presentation.",
        "source": "mimiciv_ed.triage",
        "derivation": "as recorded",
    },
    # Outcomes
    "hospital_expire_flag": {
        "label": "Hospital expire flag",
        "definition": "Hospital mortality indicator from admissions table.",
        "source": "mimiciv_hosp.admissions",
        "derivation": "as recorded",
        "units": "binary (0/1)",
    },
    "in_hospital_death": {
        "label": "In-hospital death",
        "definition": "Indicator for in-hospital death during admission.",
        "source": "mimiciv_hosp.admissions",
        "derivation": "(hospital_expire_flag == 1) OR (deathtime not null)",
        "units": "binary (0/1)",
    },
    # Ventilation
    "imv_flag": {
        "label": "IMV flag",
        "definition": "Indicator of invasive mechanical ventilation during admission.",
        "source": "ICD procedures + ICU chartevents",
        "derivation": "max(IMV ICD flag, IMV chart flag)",
        "units": "binary (0/1)",
    },
    "niv_flag": {
        "label": "NIV flag",
        "definition": "Indicator of noninvasive ventilation during admission.",
        "source": "ICD procedures + ICU chartevents",
        "derivation": "max(NIV ICD flag, NIV chart flag)",
        "units": "binary (0/1)",
    },
    "first_imv_time": {
        "label": "First IMV time",
        "definition": "Earliest charted time of IMV in ICU chartevents (if present).",
        "source": "mimiciv_icu.chartevents",
        "derivation": "min charttime among IMV charted events",
        "units": "datetime",
    },
    "first_niv_time": {
        "label": "First NIV time",
        "definition": "Earliest charted time of NIV in ICU chartevents (if present).",
        "source": "mimiciv_icu.chartevents",
        "derivation": "min charttime among NIV charted events",
        "units": "datetime",
    },
    "dt_first_imv_hours": {
        "label": "Hours to first IMV",
        "definition": "Hours from first ED arrival to first IMV time.",
        "source": "Derived",
        "derivation": "(first_imv_time - ed_intime_first) in hours",
        "units": "hours",
    },
    "dt_first_niv_hours": {
        "label": "Hours to first NIV",
        "definition": "Hours from first ED arrival to first NIV time.",
        "source": "Derived",
        "derivation": "(first_niv_time - ed_intime_first) in hours",
        "units": "hours",
    },
    "abg_before_imv": {
        "label": "ABG before IMV",
        "definition": "Indicator that first ABG time precedes first IMV time.",
        "source": "Derived",
        "derivation": "first_abg_time < first_imv_time",
        "units": "binary (0/1)",
    },
    "vbg_before_imv": {
        "label": "VBG before IMV",
        "definition": "Indicator that first VBG time precedes first IMV time.",
        "source": "Derived",
        "derivation": "first_vbg_time < first_imv_time",
        "units": "binary (0/1)",
    },
    # Gas/ABG/VBG
    "abg_hypercap_threshold": {
        "label": "ABG hypercapnia threshold met",
        "definition": "Indicator that any arterial pCO2 ≥ 45 mmHg in hosp/ICU pCO2 extraction.",
        "source": "mimiciv_hosp.labevents + mimiciv_icu.chartevents",
        "derivation": "max(arterial pCO2 ≥ 45) per hadm_id",
        "units": "binary (0/1)",
    },
    "vbg_hypercap_threshold": {
        "label": "VBG hypercapnia threshold met",
        "definition": "Indicator that any venous pCO2 ≥ 50 mmHg in hosp/ICU pCO2 extraction.",
        "source": "mimiciv_hosp.labevents + mimiciv_icu.chartevents",
        "derivation": "max(venous pCO2 ≥ 50) per hadm_id",
        "units": "binary (0/1)",
    },
    "first_gas_time": {
        "label": "First gas panel time",
        "definition": "Earliest gas panel time in the ED 0–24h window.",
        "source": "mimiciv_hosp.labevents (gas panels)",
        "derivation": "min panel_time within ED 0–24h window",
        "units": "datetime",
    },
    "dt_first_qualifying_gas_hours": {
        "label": "Hours to first gas panel",
        "definition": "Hours from first ED arrival to first gas panel time.",
        "source": "Derived",
        "derivation": "(first_gas_time - ed_intime_first) in hours",
        "units": "hours",
    },
    "presenting_hypercapnia": {
        "label": "Presenting (≤6h) gas timing",
        "definition": "Indicator that first gas panel occurred within 6h of first ED arrival.",
        "source": "Derived",
        "derivation": "dt_first_qualifying_gas_hours ≤ 6",
        "units": "binary (0/1)",
    },
    "late_hypercapnia": {
        "label": "Late (>6h) gas timing",
        "definition": "Indicator that first gas panel occurred >6h after first ED arrival.",
        "source": "Derived",
        "derivation": "dt_first_qualifying_gas_hours > 6",
        "units": "binary (0/1)",
    },
    "gas_source_unknown_rate": {
        "label": "Gas source unknown rate",
        "definition": "Proportion of gas panels with unknown arterial/venous source per ED stay.",
        "source": "Derived from gas panel source inference",
        "derivation": "mean(source == 'unknown') per ed_stay_id",
        "units": "proportion",
    },
    # Comorbidities
    "flag_copd": {
        "label": "COPD/emphysema flag",
        "definition": "Indicator for COPD/emphysema ICD codes in ED or hospital diagnoses.",
        "source": "mimiciv_hosp.diagnoses_icd + mimiciv_ed.diagnosis",
        "derivation": "ICD prefixes J43/J44 (ED OR hospital)",
        "units": "binary (0/1)",
    },
    "flag_osa_ohs": {
        "label": "OSA/OHS flag",
        "definition": "Indicator for OSA/OHS ICD codes in ED or hospital diagnoses.",
        "source": "mimiciv_hosp.diagnoses_icd + mimiciv_ed.diagnosis",
        "derivation": "ICD prefixes G473/E662 (ED OR hospital)",
        "units": "binary (0/1)",
    },
    "flag_chf": {
        "label": "CHF flag",
        "definition": "Indicator for CHF ICD codes in ED or hospital diagnoses.",
        "source": "mimiciv_hosp.diagnoses_icd + mimiciv_ed.diagnosis",
        "derivation": "ICD prefix I50 (ED OR hospital)",
        "units": "binary (0/1)",
    },
    "flag_neuromuscular": {
        "label": "Neuromuscular flag",
        "definition": "Indicator for neuromuscular ICD codes in ED or hospital diagnoses.",
        "source": "mimiciv_hosp.diagnoses_icd + mimiciv_ed.diagnosis",
        "derivation": "ICD prefixes G12/G70/G71 (ED OR hospital)",
        "units": "binary (0/1)",
    },
    "flag_opioid_substance": {
        "label": "Opioid/substance flag",
        "definition": "Indicator for opioid/substance ICD codes in ED or hospital diagnoses.",
        "source": "mimiciv_hosp.diagnoses_icd + mimiciv_ed.diagnosis",
        "derivation": "ICD prefixes F11/T40/F13 (ED OR hospital)",
        "units": "binary (0/1)",
    },
    "flag_pneumonia": {
        "label": "Pneumonia flag",
        "definition": "Indicator for pneumonia ICD codes in ED or hospital diagnoses.",
        "source": "mimiciv_hosp.diagnoses_icd + mimiciv_ed.diagnosis",
        "derivation": "ICD prefixes J12–J18 (ED OR hospital)",
        "units": "binary (0/1)",
    },
}

rows = []
for col in dd_target.columns:
    info = META.get(col, {})
    series = dd_target[col]
    rows.append({
        "variable": col,
        "label": info.get("label", _readable(col)),
        "units": info.get("units", _infer_units(col)),
        "allowed_values": info.get("allowed_values", _allowed_values(series)),
        "definition": info.get("definition", "UNCONFIRMED"),
        "synonyms": info.get("synonyms", ""),
        "description": info.get("description", ""),
        "source": info.get("source", "UNCONFIRMED"),
        "derivation": info.get("derivation", "UNCONFIRMED"),
        "dtype": str(series.dtype),
        "example_value": series.dropna().iloc[0] if series.dropna().shape[0] else None,
    })

data_dict = pd.DataFrame(rows)

out_date = datetime.now().strftime("%Y-%m-%d")
out_xlsx = DATA_DIR / f"{out_date} MIMICIV all with CC_data_dictionary.xlsx"
out_csv = DATA_DIR / f"{out_date} MIMICIV all with CC_data_dictionary.csv"

data_dict.to_excel(out_xlsx, index=False)
data_dict.to_csv(out_csv, index=False)

print("Saved data dictionary:", out_xlsx)
print("Saved data dictionary:", out_csv)
print("Rows:", len(data_dict))


[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '# Data dictionary (OSF-style) for final cohort output\nfrom datetime import datetime\nimport numpy as np\n\n# Choose target dataset for dictionary\nif "final_cc" in globals():\n    dd_target = final_cc\nelif "df_cc" in globals():\n    dd_target = df_cc\nelif "df" in globals():\n    dd_target = df\nelse:\n    raise NameError("No cohort dataframe found for data dictionary (expected final_cc/df_cc/df).")\n\n# Helper functions\n\ndef _readable(name: str) -> str:\n    return name.replace("_", " ").strip().title()\n\n\ndef _infer_units(col: str) -> str:\n    if col.endswith("_hours"):\n        return "hours"\n    if col.endswith("_time") or col.endswith("_intime") or col.endswith("_outtime") or col.endswith("_date"):\n        return "datetime"\n    if col.startswith("flag_") or col.endswith("_flag") or col.endswith("_before_imv"):\n        return "binary (0/1)"\n    if col.endswith("_paco2") or col.endswith("_pco2"):\n        return "mmHg"\n    if col.endswith("_ph"):\n        return "pH"\n    if col.endswith("_lactate"):\n        return "mmol/L"\n    return ""\n\n\ndef _allowed_values(series):\n    s = series.dropna()\n    if s.empty:\n        return ""\n    if s.dtype.kind in "biu":\n        vals = sorted(map(int, s.unique()))\n        if len(vals) <= 15:\n            return ", ".join(map(str, vals))\n        return f"{min(vals)}–{max(vals)}"\n    if s.dtype.kind in "f":\n        return f"{np.nanmin(s):.3g}–{np.nanmax(s):.3g}"\n    # object/categorical\n    vals = sorted(map(str, s.unique()))\n    return ", ".join(vals[:15]) + (" …" if len(vals) > 15 else "")\n\n\nMETA = {\n    # IDs\n    "subject_id": {\n        "label": "Subject identifier",\n        "definition": "Identifier for a unique patient in MIMIC-IV.",\n        "source": "mimiciv_hosp.patients",\n        "derivation": "as recorded",\n    },\n    "hadm_id": {\n        "label": "Hospital admission identifier",\n        "definition": "Identifier for a hospital admission.",\n        "source": "mimiciv_hosp.admissions",\n        "derivation": "as recorded",\n    },\n    "ed_stay_id": {\n        "label": "ED stay identifier",\n        "definition": "Identifier for an ED stay.",\n        "source": "mimiciv_ed.edstays",\n        "derivation": "as recorded",\n    },\n    "ed_intime": {\n        "label": "ED arrival time",\n        "definition": "ED arrival time for this ED stay.",\n        "source": "mimiciv_ed.edstays",\n        "derivation": "as recorded",\n        "units": "datetime",\n    },\n    "ed_outtime": {\n        "label": "ED departure time",\n        "definition": "ED departure time for this ED stay.",\n        "source": "mimiciv_ed.edstays",\n        "derivation": "as recorded",\n        "units": "datetime",\n    },\n    "ed_intime_first": {\n        "label": "First ED arrival time (admission)",\n        "definition": "Earliest ED arrival time among ED stays linked to the admission.",\n        "source": "mimiciv_ed.edstays",\n        "derivation": "min(edstays.intime) per hadm_id",\n        "units": "datetime",\n    },\n    # Demographics / triage\n    "ed_gender": {\n        "label": "ED gender",\n        "definition": "Gender as recorded in ED stay table.",\n        "source": "mimiciv_ed.edstays",\n        "derivation": "as recorded",\n    },\n    "ed_race": {\n        "label": "ED race",\n        "definition": "Race as recorded in ED stay table.",\n        "source": "mimiciv_ed.edstays",\n        "derivation": "as recorded",\n    },\n    "hosp_race": {\n        "label": "Hospital race",\n        "definition": "Race as recorded in admissions table.",\n        "source": "mimiciv_hosp.admissions",\n        "derivation": "as recorded",\n    },\n    "ed_triage_cc": {\n        "label": "ED chief complaint",\n        "definition": "Chief complaint recorded at ED triage.",\n        "source": "mimiciv_ed.triage",\n        "derivation": "as recorded",\n    },\n    "ed_triage_acuity": {\n        "label": "ED triage acuity",\n        "definition": "Triage acuity level recorded at ED presentation.",\n        "source": "mimiciv_ed.triage",\n        "derivation": "as recorded",\n    },\n    # Outcomes\n    "hospital_expire_flag": {\n        "label": "Hospital expire flag",\n        "definition": "Hospital mortality indicator from admissions table.",\n        "source": "mimiciv_hosp.admissions",\n        "derivation": "as recorded",\n        "units": "binary (0/1)",\n    },\n    "in_hospital_death": {\n        "label": "In-hospital death",\n        "definition": "Indicator for in-hospital death during admission.",\n        "source": "mimiciv_hosp.admissions",\n        "derivation": "(hospital_expire_flag == 1) OR (deathtime not null)",\n        "units": "binary (0/1)",\n    },\n    # Ventilation\n    "imv_flag": {\n        "label": "IMV flag",\n        "definition": "Indicator of invasive mechanical ventilation during admission.",\n        "source": "ICD procedures + ICU chartevents",\n        "derivation": "max(IMV ICD flag, IMV chart flag)",\n        "units": "binary (0/1)",\n    },\n    "niv_flag": {\n        "label": "NIV flag",\n        "definition": "Indicator of noninvasive ventilation during admission.",\n        "source": "ICD procedures + ICU chartevents",\n        "derivation": "max(NIV ICD flag, NIV chart flag)",\n        "units": "binary (0/1)",\n    },\n    "first_imv_time": {\n        "label": "First IMV time",\n        "definition": "Earliest charted time of IMV in ICU chartevents (if present).",\n        "source": "mimiciv_icu.chartevents",\n        "derivation": "min charttime among IMV charted events",\n        "units": "datetime",\n    },\n    "first_niv_time": {\n        "label": "First NIV time",\n        "definition": "Earliest charted time of NIV in ICU chartevents (if present).",\n        "source": "mimiciv_icu.chartevents",\n        "derivation": "min charttime among NIV charted events",\n        "units": "datetime",\n    },\n    "dt_first_imv_hours": {\n        "label": "Hours to first IMV",\n        "definition": "Hours from first ED arrival to first IMV time.",\n        "source": "Derived",\n        "derivation": "(first_imv_time - ed_intime_first) in hours",\n        "units": "hours",\n    },\n    "dt_first_niv_hours": {\n        "label": "Hours to first NIV",\n        "definition": "Hours from first ED arrival to first NIV time.",\n        "source": "Derived",\n        "derivation": "(first_niv_time - ed_intime_first) in hours",\n        "units": "hours",\n    },\n    "abg_before_imv": {\n        "label": "ABG before IMV",\n        "definition": "Indicator that first ABG time precedes first IMV time.",\n        "source": "Derived",\n        "derivation": "first_abg_time < first_imv_time",\n        "units": "binary (0/1)",\n    },\n    "vbg_before_imv": {\n        "label": "VBG before IMV",\n        "definition": "Indicator that first VBG time precedes first IMV time.",\n        "source": "Derived",\n        "derivation": "first_vbg_time < first_imv_time",\n        "units": "binary (0/1)",\n    },\n    # Gas/ABG/VBG\n    "abg_hypercap_threshold": {\n        "label": "ABG hypercapnia threshold met",\n        "definition": "Indicator that any arterial pCO2 ≥ 45 mmHg in hosp/ICU pCO2 extraction.",\n        "source": "mimiciv_hosp.labevents + mimiciv_icu.chartevents",\n        "derivation": "max(arterial pCO2 ≥ 45) per hadm_id",\n        "units": "binary (0/1)",\n    },\n    "vbg_hypercap_threshold": {\n        "label": "VBG hypercapnia threshold met",\n        "definition": "Indicator that any venous pCO2 ≥ 50 mmHg in hosp/ICU pCO2 extraction.",\n        "source": "mimiciv_hosp.labevents + mimiciv_icu.chartevents",\n        "derivation": "max(venous pCO2 ≥ 50) per hadm_id",\n        "units": "binary (0/1)",\n    },\n    "first_gas_time": {\n        "label": "First gas panel time",\n        "definition": "Earliest gas panel time in the ED 0–24h window.",\n        "source": "mimiciv_hosp.labevents (gas panels)",\n        "derivation": "min panel_time within ED 0–24h window",\n        "units": "datetime",\n    },\n    "dt_first_qualifying_gas_hours": {\n        "label": "Hours to first gas panel",\n        "definition": "Hours from first ED arrival to first gas panel time.",\n        "source": "Derived",\n        "derivation": "(first_gas_time - ed_intime_first) in hours",\n        "units": "hours",\n    },\n    "presenting_hypercapnia": {\n        "label": "Presenting (≤6h) gas timing",\n        "definition": "Indicator that first gas panel occurred within 6h of first ED arrival.",\n        "source": "Derived",\n        "derivation": "dt_first_qualifying_gas_hours ≤ 6",\n        "units": "binary (0/1)",\n    },\n    "late_hypercapnia": {\n        "label": "Late (>6h) gas timing",\n        "definition": "Indicator that first gas panel occurred >6h after first ED arrival.",\n        "source": "Derived",\n        "derivation": "dt_first_qualifying_gas_hours > 6",\n        "units": "binary (0/1)",\n    },\n    "gas_source_unknown_rate": {\n        "label": "Gas source unknown rate",\n        "definition": "Proportion of gas panels with unknown arterial/venous source per ED stay.",\n        "source": "Derived from gas panel source inference",\n        "derivation": "mean(source == \'unknown\') per ed_stay_id",\n        "units": "proportion",\n    },\n    # Comorbidities\n    "flag_copd": {\n        "label": "COPD/emphysema flag",\n        "definition": "Indicator for COPD/emphysema ICD codes in ED or hospital diagnoses.",\n        "source": "mimiciv_hosp.diagnoses_icd + mimiciv_ed.diagnosis",\n        "derivation": "ICD prefixes J43/J44 (ED OR hospital)",\n        "units": "binary (0/1)",\n    },\n    "flag_osa_ohs": {\n        "label": "OSA/OHS flag",\n        "definition": "Indicator for OSA/OHS ICD codes in ED or hospital diagnoses.",\n        "source": "mimiciv_hosp.diagnoses_icd + mimiciv_ed.diagnosis",\n        "derivation": "ICD prefixes G473/E662 (ED OR hospital)",\n        "units": "binary (0/1)",\n    },\n    "flag_chf": {\n        "label": "CHF flag",\n        "definition": "Indicator for CHF ICD codes in ED or hospital diagnoses.",\n        "source": "mimiciv_hosp.diagnoses_icd + mimiciv_ed.diagnosis",\n        "derivation": "ICD prefix I50 (ED OR hospital)",\n        "units": "binary (0/1)",\n    },\n    "flag_neuromuscular": {\n        "label": "Neuromuscular flag",\n        "definition": "Indicator for neuromuscular ICD codes in ED or hospital diagnoses.",\n        "source": "mimiciv_hosp.diagnoses_icd + mimiciv_ed.diagnosis",\n        "derivation": "ICD prefixes G12/G70/G71 (ED OR hospital)",\n        "units": "binary (0/1)",\n    },\n    "flag_opioid_substance": {\n        "label": "Opioid/substance flag",\n        "definition": "Indicator for opioid/substance ICD codes in ED or hospital diagnoses.",\n        "source": "mimiciv_hosp.diagnoses_icd + mimiciv_ed.diagnosis",\n        "derivation": "ICD prefixes F11/T40/F13 (ED OR hospital)",\n        "units": "binary (0/1)",\n    },\n    "flag_pneumonia": {\n        "label": "Pneumonia flag",\n        "definition": "Indicator for pneumonia ICD codes in ED or hospital diagnoses.",\n        "source": "mimiciv_hosp.diagnoses_icd + mimiciv_ed.diagnosis",\n        "derivation": "ICD prefixes J12–J18 (ED OR hospital)",\n        "units": "binary (0/1)",\n    },\n}\n\nrows = []\nfor col in dd_target.columns:\n    info = META.get(col, {})\n    series = dd_target[col]\n    rows.append({\n        "variable": col,\n        "label": info.get("label", _readable(col)),\n        "units": info.get("units", _infer_units(col)),\n        "allowed_values": info.get("allowed_values", _allowed_values(series)),\n        "definition": info.get("definition", "UNCONFIRMED"),\n        "synonyms": info.get("synonyms", ""),\n        "description": info.get("description", ""),\n        "source": info.get("source", "UNCONFIRMED"),\n        "derivation": info.get("derivation", "UNCONFIRMED"),\n        "dtype": str(series.dtype),\n        "example_value": series.dropna().iloc[0] if series.dropna().shape[0] else None,\n    })\n\ndata_dict = pd.DataFrame(rows)\n\nout_date = datetime.now().strftime("%Y-%m-%d")\nout_xlsx = DATA_DIR / f"{out_date} MIMICIV all with CC_data_dictionary.xlsx"\nout_csv = DATA_DIR / f"{out_date} MIMICIV all with CC_data_dictionary.csv"\n\ndata_dict.to_excel(out_xlsx, index=False)\ndata_dict.to_csv(out_csv, index=False)\n\nprint("Saved data dictionary:", out_xlsx)\nprint("Saved data dictionary:", out_csv)\nprint("Rows:", len(data_dict))\n\n', 'execution_count': 46}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': 'Saved data dictionary: /Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/MIMIC tabular data/2026-02-04 MIMICIV all with CC_data_dictionary.xlsx\nSaved data dictionary: /Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/MIMIC tabular data/2026-02-04 MIMICIV all with CC_data_dictionary.csv\nRows: 156\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Writing 222701 bytes to MIMICIV_hypercap_EXT_cohort.ipynb
