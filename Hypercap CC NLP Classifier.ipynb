{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f413ae45",
   "metadata": {},
   "source": [
    "# Workbook for MIMIC Hypercapnia Presenting Chief Concern Analysis\n",
    "\n",
    "This notebook is intentionally self-contained and deterministic.\n",
    "It performs NLP-based mapping from free-text ED chief complaints to NHAMCS top-level RVC categories,\n",
    "with strict compatibility outputs for downstream analysis notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f56d2a5",
   "metadata": {},
   "source": [
    "## 1) Runtime contract and imports\n",
    "\n",
    "- No runtime package installation.\n",
    "- Fail fast if required dependencies are missing.\n",
    "- Keep environment deterministic and explicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "950758ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T21:14:04.404630Z",
     "iopub.status.busy": "2026-02-08T21:14:04.404315Z",
     "iopub.status.idle": "2026-02-08T21:14:10.224691Z",
     "shell.execute_reply": "2026-02-08T21:14:10.224408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy pipeline: pending config-driven load\n"
     ]
    }
   ],
   "source": [
    "# Purpose: verify required packages, import shared dependencies, and initialize NLP runtime objects.\n",
    "# Why this matters: later cells assume these imports/models exist, so we fail early if the environment is incomplete.\n",
    "\n",
    "import importlib\n",
    "\n",
    "REQUIRED_IMPORTS = {\n",
    "    \"numpy\": \"numpy\",\n",
    "    \"pandas\": \"pandas\",\n",
    "    \"torch\": \"torch\",\n",
    "    \"ftfy\": \"ftfy\",\n",
    "    \"spacy\": \"spacy\",\n",
    "    \"symspellpy\": \"symspellpy\",\n",
    "    \"sentence_transformers\": \"sentence_transformers\",\n",
    "    \"openpyxl\": \"openpyxl\",\n",
    "    \"dotenv\": \"python-dotenv\",\n",
    "}\n",
    "\n",
    "missing = []\n",
    "for import_name, package_name in REQUIRED_IMPORTS.items():\n",
    "    if importlib.util.find_spec(import_name) is None:\n",
    "        missing.append((import_name, package_name))\n",
    "\n",
    "if missing:\n",
    "    message = \"Missing required dependencies (install via uv sync): \" + \", \".join(\n",
    "        f\"{import_name} ({package_name})\" for import_name, package_name in missing\n",
    "    )\n",
    "    raise ImportError(message)\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "from collections import Counter, defaultdict\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import ftfy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "\n",
    "# Silence widget-only tqdm warning in environments without Jupyter progress widgets.\n",
    "try:\n",
    "    from tqdm import TqdmWarning\n",
    "except Exception:  # pragma: no cover - defensive for uncommon tqdm import edge cases\n",
    "    TqdmWarning = None\n",
    "\n",
    "if TqdmWarning is not None:\n",
    "    warnings.filterwarnings(\n",
    "        \"ignore\",\n",
    "        message=\"IProgress not found.*\",\n",
    "        category=TqdmWarning,\n",
    "    )\n",
    "\n",
    "# Reduce HF Hub warning noise in notebook output. If HF_TOKEN is set, authenticated requests still work.\n",
    "try:\n",
    "    from huggingface_hub import logging as hf_logging\n",
    "\n",
    "    hf_logging.set_verbosity_error()\n",
    "except Exception:  # pragma: no cover - keeps notebook robust if hub internals change\n",
    "    pass\n",
    "\n",
    "logging.getLogger(\"huggingface_hub\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"hf_xet\").setLevel(logging.ERROR)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# spaCy model loading is config-driven in the setup cell so behavior is explicit.\n",
    "NLP: Any | None = None\n",
    "SPACY_MODEL_LABEL = \"UNINITIALIZED (loaded in setup cell)\"\n",
    "print(\"spaCy pipeline: pending config-driven load\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4d5f85",
   "metadata": {},
   "source": [
    "## 2) Config and constants\n",
    "\n",
    "### Core API (single-cell contract)\n",
    "This cell defines the notebook's public in-notebook interfaces:\n",
    "- `ClassifierConfig`\n",
    "- `get_cc_column`\n",
    "- `normalize_cc`\n",
    "- `build_prototype_resources`\n",
    "- `build_segment_cache`\n",
    "- `classify_segments_cached`\n",
    "- `assign_rvc_per_segment_and_visit_cached`\n",
    "- `validate_output_schema`\n",
    "- `save_with_suffix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "083e889a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T21:14:10.226917Z",
     "iopub.status.busy": "2026-02-08T21:14:10.226678Z",
     "iopub.status.idle": "2026-02-08T21:14:10.284166Z",
     "shell.execute_reply": "2026-02-08T21:14:10.283886Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose: centralize the notebook's core API (config, constants, data structures, and reusable functions).\n",
    "# Why this matters: keeping definitions in one place makes behavior predictable and easier for new contributors to trace.\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ClassifierConfig:\n",
    "    \"\"\"Configuration for notebook-contained NLP classification.\"\"\"\n",
    "\n",
    "    seed: int = 1234\n",
    "    work_dir_env_var: str = \"WORK_DIR\"\n",
    "    data_dirname: str = \"MIMIC tabular data\"\n",
    "    input_filename: str = \"MIMICIV all with CC.xlsx\"\n",
    "    input_filename_env_var: str = \"CLASSIFIER_INPUT_FILENAME\"\n",
    "    output_suffix: str = \"_with_NLP\"\n",
    "    output_sheet_name: str = \"with_NLP\"\n",
    "    model_name: str = \"NeuML/bioclinical-modernbert-base-embeddings\"\n",
    "    model_trust_remote_code: bool = True\n",
    "    model_batch_size: int = 256\n",
    "    segment_batch_size: int = 512\n",
    "    scoring_method: str = \"max\"  # {\"max\", \"lse\"}\n",
    "    alpha: float = 12.0\n",
    "    abstain_threshold: float = 0.40\n",
    "    max_rfv: int = 5\n",
    "    max_segments_per_visit: int = 5\n",
    "    prototype_dedup_sim_thresh: float = 0.92\n",
    "    summary_weight_strategy: str = \"uniform\"  # {\"uniform\", \"by_rows\"}\n",
    "    run_optional_plots: bool = False\n",
    "    require_spacy_model: bool = True\n",
    "    allow_uncodable_primary: bool = True\n",
    "    empty_primary_policy: str = \"blank\"  # {\"blank\", \"uncodable\"}\n",
    "    appendix_relpath: str = \"Annotation/nhamcs_rvc_2022_appendixII_codes.csv\"\n",
    "    summary_relpath: str = \"Annotation/nhamcs_rvc_2022_summary_by_top_level_17.csv\"\n",
    "    hf_token_env_var: str = \"HF_TOKEN\"\n",
    "\n",
    "\n",
    "# --- Ontology labels and precedence used by final visit-level outputs. ---\n",
    "RVC_NAME: dict[str, str] = {\n",
    "    \"RVC-INJ\": \"Injuries & adverse effects\",\n",
    "    \"RVC-SYM-RESP\": \"Symptom – Respiratory\",\n",
    "    \"RVC-SYM-CIRC\": \"Symptom – Circulatory\",\n",
    "    \"RVC-SYM-NERV\": \"Symptom – Nervous\",\n",
    "    \"RVC-SYM-DIG\": \"Symptom – Digestive\",\n",
    "    \"RVC-SYM-GU\": \"Symptom – Genitourinary\",\n",
    "    \"RVC-SYM-MSK\": \"Symptom – Musculoskeletal\",\n",
    "    \"RVC-SYM-SKIN\": \"Symptom – Skin/Hair/Nails\",\n",
    "    \"RVC-SYM-EYE\": \"Symptom – Eye/Ear\",\n",
    "    \"RVC-SYM-GEN\": \"Symptom – General\",\n",
    "    \"RVC-SYM-PSY\": \"Symptom – Psychological\",\n",
    "    \"RVC-DIS\": \"Diseases (patient-stated)\",\n",
    "    \"RVC-TEST\": \"Abnormal test result\",\n",
    "    \"RVC-DIAG\": \"Diagnostic/Screening/Preventive\",\n",
    "    \"RVC-TREAT\": \"Treatment/Medication\",\n",
    "    \"RVC-ADMIN\": \"Administrative\",\n",
    "    \"RVC-UNCL\": \"Uncodable/Unknown\",\n",
    "}\n",
    "\n",
    "PRECEDENCE_RVC: list[str] = [\n",
    "    \"RVC-INJ\",\n",
    "    \"RVC-SYM-RESP\",\n",
    "    \"RVC-SYM-CIRC\",\n",
    "    \"RVC-SYM-NERV\",\n",
    "    \"RVC-SYM-DIG\",\n",
    "    \"RVC-SYM-GU\",\n",
    "    \"RVC-SYM-MSK\",\n",
    "    \"RVC-SYM-SKIN\",\n",
    "    \"RVC-SYM-EYE\",\n",
    "    \"RVC-SYM-GEN\",\n",
    "    \"RVC-SYM-PSY\",\n",
    "    \"RVC-DIS\",\n",
    "    \"RVC-TEST\",\n",
    "    \"RVC-DIAG\",\n",
    "    \"RVC-TREAT\",\n",
    "    \"RVC-ADMIN\",\n",
    "    \"RVC-UNCL\",\n",
    "]\n",
    "\n",
    "RVC_SHORT: dict[str, str] = {\n",
    "    \"RVC-INJ\": \"injury\",\n",
    "    \"RVC-SYM-RESP\": \"resp\",\n",
    "    \"RVC-SYM-CIRC\": \"circ\",\n",
    "    \"RVC-SYM-NERV\": \"neuro\",\n",
    "    \"RVC-SYM-DIG\": \"gi\",\n",
    "    \"RVC-SYM-GU\": \"gu\",\n",
    "    \"RVC-SYM-MSK\": \"msk\",\n",
    "    \"RVC-SYM-SKIN\": \"skin\",\n",
    "    \"RVC-SYM-EYE\": \"eye_ear\",\n",
    "    \"RVC-SYM-GEN\": \"general\",\n",
    "    \"RVC-SYM-PSY\": \"psych\",\n",
    "    \"RVC-DIS\": \"disease\",\n",
    "    \"RVC-TEST\": \"abn_test\",\n",
    "    \"RVC-DIAG\": \"screen\",\n",
    "    \"RVC-TREAT\": \"treatment\",\n",
    "    \"RVC-ADMIN\": \"admin\",\n",
    "    \"RVC-UNCL\": \"uncodable\",\n",
    "}\n",
    "\n",
    "# --- Text normalization vocabulary and complaint parsing constants. ---\n",
    "CC_CANDIDATES: tuple[str, ...] = (\n",
    "    \"ed_triage_cc\",\n",
    "    \"chief_complaint\",\n",
    "    \"ed_chief_complaint\",\n",
    "    \"chiefcomplaint\",\n",
    "    \"chief_complaint_text\",\n",
    "    \"ed_cc\",\n",
    "    \"cc\",\n",
    ")\n",
    "\n",
    "PROTECT = (\"n/v\", \"n/v/d\", \"s/p\", \"w/\", \"w/o\", \"h/o\", \"c/o\", \"h/a\", \"f/c\", \"c/p\")\n",
    "MISSING_RE = re.compile(\n",
    "    r\"^(?:n/?a|na|none|unknown|unk|no chief complaint|no complaint|not applicable|not available)$\",\n",
    "    re.I,\n",
    ")\n",
    "NEG_RE = re.compile(r\"^(?:denies?|no|not|without|w/o)\\b\", re.I)\n",
    "\n",
    "# Preserve a small set of clinically meaningful \"absence\" complaints instead of dropping as generic negation.\n",
    "ABSENCE_SYMPTOM_RULES: tuple[tuple[re.Pattern[str], str], ...] = (\n",
    "    (\n",
    "        re.compile(r\"^(?:no|without|w/o)\\s+(?:urine|urinary)\\s+(?:output|void(?:ing)?)\\b\", re.I),\n",
    "        \"urinary retention\",\n",
    "    ),\n",
    "    (\n",
    "        re.compile(r\"^(?:no|without|w/o)\\s+(?:bowel movement|bm|stool(?: output)?)\\b\", re.I),\n",
    "        \"constipation\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "SEG_OVR: dict[str, str] = {\n",
    "    \"n/v\": \"nausea vomiting\",\n",
    "    \"n/v/d\": \"nausea vomiting diarrhea\",\n",
    "    \"nvd\": \"nausea vomiting diarrhea\",\n",
    "    \"w/\": \"with\",\n",
    "    \"w/o\": \"without\",\n",
    "    \"s/p\": \"status post\",\n",
    "    \"h/o\": \"history of\",\n",
    "    \"c/o\": \"complains of\",\n",
    "    \"h/a\": \"headache\",\n",
    "    \"f/c\": \"fever chills\",\n",
    "    \"c/p\": \"chest pain\",\n",
    "    \"mva\": \"motor vehicle accident\",\n",
    "    \"mvc\": \"motor vehicle collision\",\n",
    "    \"ped vs auto\": \"pedestrian struck by vehicle\",\n",
    "    \"gsw\": \"gunshot wound\",\n",
    "    \"sa\": \"sexual assault\",\n",
    "}\n",
    "\n",
    "TOK_OVR: dict[str, str] = {\n",
    "    \"sob\": \"shortness of breath\",\n",
    "    \"doe\": \"dyspnea on exertion\",\n",
    "    \"cp\": \"chest pain\",\n",
    "    \"palps\": \"palpitations\",\n",
    "    \"ha\": \"headache\",\n",
    "    \"h/a\": \"headache\",\n",
    "    \"f/c\": \"fever chills\",\n",
    "    \"c/p\": \"chest pain\",\n",
    "    \"loc\": \"loss of consciousness\",\n",
    "    \"ams\": \"altered mental status\",\n",
    "    \"sz\": \"seizure\",\n",
    "    \"szs\": \"seizure\",\n",
    "    \"cva\": \"stroke\",\n",
    "    \"tia\": \"transient ischemic attack\",\n",
    "    \"abd\": \"abdominal\",\n",
    "    \"abdo\": \"abdominal\",\n",
    "    \"rlq\": \"right lower quadrant\",\n",
    "    \"ruq\": \"right upper quadrant\",\n",
    "    \"llq\": \"left lower quadrant\",\n",
    "    \"luq\": \"left upper quadrant\",\n",
    "    \"lbp\": \"low back pain\",\n",
    "    \"brbpr\": \"rectal bleeding\",\n",
    "    \"uti\": \"urinary tract infection\",\n",
    "    \"dysuria\": \"painful urination\",\n",
    "    \"hematuria\": \"blood in urine\",\n",
    "    \"nv\": \"nausea vomiting\",\n",
    "    \"nvd\": \"nausea vomiting diarrhea\",\n",
    "    \"sorethroat\": \"sore throat\",\n",
    "    \"uri\": \"upper respiratory infection\",\n",
    "    \"pna\": \"pneumonia\",\n",
    "    # Conservative high-frequency shorthand expansions from uncodable output audit.\n",
    "    \"ich\": \"intracranial hemorrhage\",\n",
    "    \"sah\": \"subarachnoid hemorrhage\",\n",
    "    \"stemi\": \"st elevation myocardial infarction\",\n",
    "    \"nstemi\": \"non st elevation myocardial infarction\",\n",
    "    \"chf\": \"congestive heart failure\",\n",
    "    \"pe\": \"pulmonary embolism\",\n",
    "    \"sbo\": \"small bowel obstruction\",\n",
    "    \"gib\": \"gastrointestinal bleeding\",\n",
    "    \"hypoglycemia\": \"hypoglycemia\",\n",
    "    \"hypokalemia\": \"hypokalemia\",\n",
    "    \"arf\": \"acute renal failure\",\n",
    "    \"ili\": \"influenza like illness\",\n",
    "    \"covid\": \"covid\",\n",
    "    \"flu\": \"influenza\",\n",
    "    \"si\": \"suicidal ideation\",\n",
    "    \"hi\": \"homicidal ideation\",\n",
    "    \"avh\": \"auditory visual hallucinations\",\n",
    "    \"etoh\": \"alcohol\",\n",
    "    \"od\": \"overdose\",\n",
    "    \"fb\": \"foreign body\",\n",
    "    \"vag\": \"vaginal\",\n",
    "    \"vb\": \"vaginal bleeding\",\n",
    "    \"hg\": \"hyperemesis gravidarum\",\n",
    "    \"s/p\": \"status post\",\n",
    "    \"w/\": \"with\",\n",
    "    \"w/o\": \"without\",\n",
    "    \"h/o\": \"history of\",\n",
    "    \"c/o\": \"complains of\",\n",
    "    \"n/v\": \"nausea vomiting\",\n",
    "    \"n/v/d\": \"nausea vomiting diarrhea\",\n",
    "}\n",
    "\n",
    "CTX_OVR: dict[str, tuple[Any, str, str | None]] = {\n",
    "    \"od\": (\n",
    "        lambda seg: not re.search(r\"\\b(eye|vision|ophth|ophthalm|cornea|ocular)\\b\", seg),\n",
    "        \"overdose\",\n",
    "        \"right eye\",\n",
    "    ),\n",
    "    \"os\": (\n",
    "        lambda seg: re.search(r\"\\b(eye|vision|ophth|ophthalm|cornea|ocular)\\b\", seg),\n",
    "        \"left eye\",\n",
    "        None,\n",
    "    ),\n",
    "    \"ou\": (\n",
    "        lambda seg: re.search(r\"\\b(eye|vision|ophth|ophthalm|cornea|ocular)\\b\", seg),\n",
    "        \"both eyes\",\n",
    "        None,\n",
    "    ),\n",
    "}\n",
    "\n",
    "ABBR_MAP: dict[str, str] = {}\n",
    "\n",
    "SAFE_SHORT_TOKENS = {\"cp\", \"ha\", \"nv\", \"gi\", \"gu\", \"ms\", \"sx\", \"bp\", \"hr\", \"doe\", \"sob\", \"loc\"}\n",
    "\n",
    "_PUNCT_EDGE = re.compile(r\"(^[^\\w/]+)|([^\\w/]+$)\")\n",
    "SEP_SPLIT_RE = re.compile(r\"\\s*(?:[;:,]|[+&])\\s*\")\n",
    "\n",
    "CLINICAL_KEYS = {\n",
    "    \"pain\",\n",
    "    \"fever\",\n",
    "    \"chill\",\n",
    "    \"cough\",\n",
    "    \"wheez\",\n",
    "    \"dyspnea\",\n",
    "    \"shortness\",\n",
    "    \"hemopty\",\n",
    "    \"nausea\",\n",
    "    \"vomit\",\n",
    "    \"diarrhea\",\n",
    "    \"bleeding\",\n",
    "    \"rash\",\n",
    "    \"injury\",\n",
    "    \"wound\",\n",
    "    \"laceration\",\n",
    "    \"fracture\",\n",
    "    \"sprain\",\n",
    "    \"assault\",\n",
    "    \"fall\",\n",
    "    \"seizure\",\n",
    "    \"syncope\",\n",
    "    \"weakness\",\n",
    "    \"numbness\",\n",
    "    \"tingling\",\n",
    "    \"headache\",\n",
    "    \"dizziness\",\n",
    "    \"palpitation\",\n",
    "    \"chest\",\n",
    "    \"abdominal\",\n",
    "    \"flank\",\n",
    "    \"pelvic\",\n",
    "    \"back\",\n",
    "    \"neck\",\n",
    "    \"urinary\",\n",
    "    \"dysuria\",\n",
    "    \"hematuria\",\n",
    "    \"pregnan\",\n",
    "    \"vaginal\",\n",
    "    \"overdose\",\n",
    "    \"intoxication\",\n",
    "    \"anxiety\",\n",
    "    \"depression\",\n",
    "    \"psychosis\",\n",
    "    \"sore\",\n",
    "    \"throat\",\n",
    "    \"ear\",\n",
    "    \"eye\",\n",
    "    \"dental\",\n",
    "    \"asthma\",\n",
    "    \"copd\",\n",
    "    \"flu\",\n",
    "    \"covid\",\n",
    "    \"pneumonia\",\n",
    "}\n",
    "\n",
    "_DUR_RE = re.compile(\n",
    "    r\"\\b([a-z]{2,})\\s*x\\s*(\\d{1,3})\\s*(d|day|h|hr|hour|w|wk|week|m|mo|mon|month|y|yr|year)s?\\b\",\n",
    "    re.I,\n",
    ")\n",
    "_UNIT_MAP = {\n",
    "    \"d\": \"days\",\n",
    "    \"day\": \"days\",\n",
    "    \"h\": \"hours\",\n",
    "    \"hr\": \"hours\",\n",
    "    \"hour\": \"hours\",\n",
    "    \"w\": \"weeks\",\n",
    "    \"wk\": \"weeks\",\n",
    "    \"week\": \"weeks\",\n",
    "    \"m\": \"months\",\n",
    "    \"mo\": \"months\",\n",
    "    \"mon\": \"months\",\n",
    "    \"month\": \"months\",\n",
    "    \"y\": \"years\",\n",
    "    \"yr\": \"years\",\n",
    "    \"year\": \"years\",\n",
    "}\n",
    "\n",
    "# --- Rule-gate regex definitions for deterministic high-confidence overrides. ---\n",
    "RX_FOCAL_WEAK = re.compile(\n",
    "    r\"\\b(focal|left|right|arm|leg|hand|face|hemiparesis|hemiplegia).*\\bweakness\\b|\\bweakness\\b.*\\b(focal|left|right|arm|leg|hand|face)\\b\",\n",
    "    re.I,\n",
    ")\n",
    "\n",
    "RULE_RX: dict[str, re.Pattern[str]] = {\n",
    "    \"RVC-INJ\": re.compile(\n",
    "        r\"\\b(mvc|mva|collision|ped(\\s|)vs(\\s|)auto|assault|fell|fall|gsw|gunshot|stab|laceration|fracture|burn|foreign body|poison(ing)?|overdose|od\\b|adverse (drug|medication) reaction)\\b\",\n",
    "        re.I,\n",
    "    ),\n",
    "    \"RVC-SYM-RESP\": re.compile(\n",
    "        r\"\\b(shortness of breath|dyspnea|doe\\b|wheeze|respiratory distress|cough(ing)?|hemoptysis|sputum)\\b\",\n",
    "        re.I,\n",
    "    ),\n",
    "    \"RVC-SYM-CIRC\": re.compile(\n",
    "        r\"\\b(chest pain|palpitation(s)?|hypotension|low bp|bradycardia|tachycardia|arrhythmia|irregular heartbeat|edema|leg swelling|ankle swelling|peripheral edema|claudication)\\b\",\n",
    "        re.I,\n",
    "    ),\n",
    "    \"RVC-SYM-NERV\": re.compile(\n",
    "        r\"\\b(alter(ed)? mental status|ams\\b|confusion|letharg(y|ic)|unresponsive|syncope|faint(ing)?|seizure(s)?|slurred speech|dysarthria|aphasia|facial droop|headache|migraine|dizziness|vertigo|numbness|tingling)\\b\",\n",
    "        re.I,\n",
    "    ),\n",
    "    \"RVC-SYM-DIG\": re.compile(\n",
    "        r\"\\b(abd(ominal)? pain|abdo pain|nausea|vomit(ting)?|diarrhea|rectal bleeding|hematemesis|dysphagia|jaundice|gastrointestinal bleeding|gi bleeding)\\b\",\n",
    "        re.I,\n",
    "    ),\n",
    "    \"RVC-SYM-GU\": re.compile(\n",
    "        r\"\\b(dysuria|hematuria|urinary (frequency|urgency)|flank pain|pelvic pain|vaginal (bleeding|discharge)|penile discharge)\\b\",\n",
    "        re.I,\n",
    "    ),\n",
    "    \"RVC-SYM-MSK\": re.compile(\n",
    "        r\"\\b(back pain|neck pain|shoulder pain|knee pain|hip pain|joint pain|gait problem)\\b\",\n",
    "        re.I,\n",
    "    ),\n",
    "    \"RVC-SYM-SKIN\": re.compile(r\"\\b(rash|pruritus|itch(ing)?|cellulitis|wound(?! check))\\b\", re.I),\n",
    "    \"RVC-SYM-EYE\": re.compile(\n",
    "        r\"\\b(eye (pain|redness|irritation)|red eye|pink ?eye|conjunctivitis|blurry vision|vision (loss|change)|ear pain|hearing (loss|change)|tinnitus|ear discharge|vertigo)\\b\",\n",
    "        re.I,\n",
    "    ),\n",
    "    \"RVC-SYM-GEN\": re.compile(r\"\\b(fever|chills|fatigue|malaise|weakness(?!.*focal))\\b\", re.I),\n",
    "    \"RVC-SYM-PSY\": re.compile(\n",
    "        r\"\\b(anxiety|depression|insomnia|agitation|suicidal ideation|homicidal ideation|si\\b|hi\\b|intoxication)\\b\",\n",
    "        re.I,\n",
    "    ),\n",
    "    \"RVC-DIS\": re.compile(\n",
    "        r\"\\b(asthma attack|copd flare|pneumonia|diabetes(?!.*test)|hypertensive crisis|stroke diagnosed|stroke|intracranial hemorrhage|subarachnoid hemorrhage|st elevation myocardial infarction|non st elevation myocardial infarction|congestive heart failure|pulmonary embolism|small bowel obstruction|acute renal failure|influenza like illness|hypoglycemia|hypokalemia)\\b\",\n",
    "        re.I,\n",
    "    ),\n",
    "    \"RVC-TEST\": re.compile(\n",
    "        r\"\\b(abnormal (lab|labs|imaging|ct|mri|x[- ]?ray|ekg|ecg)|told (to come|results?)|positive (test|culture|blood culture)|blood culture (positive|grew|growth))\\b\",\n",
    "        re.I,\n",
    "    ),\n",
    "    \"RVC-DIAG\": re.compile(\n",
    "        r\"\\b((needs|for) (covid|flu|strep) test|screen(ing)?|bp (check|recheck)|workup|routine testing|prenatal)\\b\",\n",
    "        re.I,\n",
    "    ),\n",
    "    \"RVC-TREAT\": re.compile(\n",
    "        r\"\\b(refill|prescription refill|rx refill|dressing change|suture removal|injection|detox clearance|wound check)\\b\",\n",
    "        re.I,\n",
    "    ),\n",
    "    \"RVC-ADMIN\": re.compile(r\"\\b(form|paperwork|note|letter|clearance|insurance|work (note|letter)|transfer(red)?|intubated transfer)\\b\", re.I),\n",
    "}\n",
    "\n",
    "GROUP_MAP: list[tuple[re.Pattern[str], str]] = [\n",
    "    (re.compile(r\"\\binjur|poison|overdose|adverse effect\", re.I), \"RVC-INJ\"),\n",
    "    (re.compile(r\"\\brespir\", re.I), \"RVC-SYM-RESP\"),\n",
    "    (re.compile(r\"\\bcircul|cardio|lymph\", re.I), \"RVC-SYM-CIRC\"),\n",
    "    (re.compile(r\"\\bnerv|neuro\", re.I), \"RVC-SYM-NERV\"),\n",
    "    (re.compile(r\"\\bdigest|gastro|abd\", re.I), \"RVC-SYM-DIG\"),\n",
    "    (re.compile(r\"\\bgenitour|urinar|renal|pelvic|vagin|penil|breast\", re.I), \"RVC-SYM-GU\"),\n",
    "    (re.compile(r\"\\bmusculoskelet|msk|joint|back|neck\", re.I), \"RVC-SYM-MSK\"),\n",
    "    (re.compile(r\"\\bskin|hair|nail|derma\", re.I), \"RVC-SYM-SKIN\"),\n",
    "    (re.compile(r\"\\beye|ear|vision|hearing|vertigo\", re.I), \"RVC-SYM-EYE\"),\n",
    "    (re.compile(r\"\\bgeneral|fever|malaise|weakness\", re.I), \"RVC-SYM-GEN\"),\n",
    "    (re.compile(r\"\\bpsych|mental\", re.I), \"RVC-SYM-PSY\"),\n",
    "    (re.compile(r\"\\bdisease|diagnos|known|asthma|copd|diabetes|hypertens|pneumonia|stroke\", re.I), \"RVC-DIS\"),\n",
    "    (re.compile(r\"\\babnormal .*test|abnormal (lab|imaging|ekg|ecg)|positive (test|culture|result)\", re.I), \"RVC-TEST\"),\n",
    "    (re.compile(r\"\\bscreen|diagnostic|prenatal|immuniz|vaccine|bp check|test\\b\", re.I), \"RVC-DIAG\"),\n",
    "    (re.compile(r\"\\btreat|medicat|refill|suture|dressing|injection|therapy|wound check\", re.I), \"RVC-TREAT\"),\n",
    "    (re.compile(r\"\\badmin|paperwork|form|clearance|insurance|work (note|letter)\", re.I), \"RVC-ADMIN\"),\n",
    "    (re.compile(r\"\\buncodable|unknown|illegible|none\", re.I), \"RVC-UNCL\"),\n",
    "]\n",
    "\n",
    "RE_DROP = re.compile(r\"\\b(nec|nos|unspecified|other|other and unspecified)\\b\", re.I)\n",
    "RE_SPACE = re.compile(r\"\\s+\")\n",
    "BODY_PART_ONLY_RE = re.compile(r\"(arm|leg|back|knee|hip|wrist|elbow|ankle|hand|foot|toe|finger|eye|ear)\")\n",
    "\n",
    "CANON_REPLACERS: list[tuple[re.Pattern[str], str]] = [\n",
    "    (re.compile(r\"\\bshortness of breath\\b\", flags=re.I), \"dyspnea\"),\n",
    "    (re.compile(r\"\\bdyspneic?\\b\", flags=re.I), \"dyspnea\"),\n",
    "    (re.compile(r\"\\bdyspnea on exertion\\b\", flags=re.I), \"dyspnea_exertion\"),\n",
    "    (re.compile(r\"\\bstatus post\\b\", flags=re.I), \"status_post\"),\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "RVC_PROTOS_CURATED: dict[str, list[str]] = {\n",
    "    \"RVC-INJ\": [\n",
    "        \"injury: laceration cut\",\n",
    "        \"injury: fracture\",\n",
    "        \"injury: burn\",\n",
    "        \"injury: contusion bruise\",\n",
    "        \"injury: foreign body\",\n",
    "        \"injury: motor vehicle collision\",\n",
    "        \"injury: fall\",\n",
    "        \"adverse effect: medication reaction\",\n",
    "        \"poisoning overdose\",\n",
    "        \"injury: assault\",\n",
    "        \"gunshot wound\",\n",
    "    ],\n",
    "    \"RVC-SYM-RESP\": [\n",
    "        \"respiratory symptom: shortness of breath dyspnea\",\n",
    "        \"respiratory symptom: dyspnea on exertion\",\n",
    "        \"respiratory symptom: wheeze\",\n",
    "        \"respiratory symptom: cough\",\n",
    "        \"respiratory symptom: hemoptysis\",\n",
    "        \"respiratory symptom: sputum production\",\n",
    "        \"respiratory symptom: respiratory distress\",\n",
    "    ],\n",
    "    \"RVC-SYM-CIRC\": [\n",
    "        \"circulatory symptom: chest pain suspected cardiac\",\n",
    "        \"circulatory symptom: palpitations\",\n",
    "        \"circulatory symptom: hypotension low blood pressure\",\n",
    "        \"circulatory symptom: bradycardia\",\n",
    "        \"circulatory symptom: tachycardia\",\n",
    "        \"circulatory symptom: peripheral edema leg swelling ankle swelling\",\n",
    "        \"circulatory symptom: claudication\",\n",
    "    ],\n",
    "    \"RVC-SYM-NERV\": [\n",
    "        \"neurologic symptom: altered mental status confusion\",\n",
    "        \"neurologic symptom: lethargy decreased responsiveness\",\n",
    "        \"neurologic symptom: slurred speech dysarthria\",\n",
    "        \"neurologic symptom: aphasia word finding difficulty\",\n",
    "        \"neurologic symptom: facial droop\",\n",
    "        \"neurologic symptom: syncope fainting\",\n",
    "        \"neurologic symptom: seizure\",\n",
    "        \"neurologic symptom: headache\",\n",
    "        \"neurologic symptom: dizziness vertigo\",\n",
    "        \"neurologic symptom: numbness tingling focal weakness\",\n",
    "    ],\n",
    "    \"RVC-SYM-DIG\": [\n",
    "        \"digestive symptom: abdominal pain\",\n",
    "        \"digestive symptom: nausea vomiting\",\n",
    "        \"digestive symptom: diarrhea\",\n",
    "        \"digestive symptom: rectal bleeding\",\n",
    "        \"digestive symptom: dysphagia\",\n",
    "        \"digestive symptom: jaundice\",\n",
    "    ],\n",
    "    \"RVC-SYM-GU\": [\n",
    "        \"genitourinary symptom: dysuria painful urination\",\n",
    "        \"genitourinary symptom: urinary frequency urgency\",\n",
    "        \"genitourinary symptom: hematuria blood in urine\",\n",
    "        \"genitourinary symptom: flank pain\",\n",
    "        \"genitourinary symptom: pelvic pain\",\n",
    "        \"genitourinary symptom: vaginal discharge\",\n",
    "    ],\n",
    "    \"RVC-SYM-MSK\": [\n",
    "        \"musculoskeletal symptom: back pain\",\n",
    "        \"musculoskeletal symptom: neck pain\",\n",
    "        \"musculoskeletal symptom: joint pain\",\n",
    "        \"musculoskeletal symptom: hip knee shoulder pain\",\n",
    "        \"musculoskeletal symptom: gait problem\",\n",
    "    ],\n",
    "    \"RVC-SYM-SKIN\": [\n",
    "        \"skin symptom: rash\",\n",
    "        \"skin symptom: pruritus itching\",\n",
    "        \"skin symptom: cellulitis redness warmth\",\n",
    "        \"skin symptom: nontraumatic wound\",\n",
    "    ],\n",
    "    \"RVC-SYM-EYE\": [\n",
    "        \"eye symptom: eye pain\",\n",
    "        \"eye symptom: red eye eye redness\",\n",
    "        \"eye symptom: blurry vision vision loss\",\n",
    "        \"ear symptom: ear pain discharge\",\n",
    "        \"vestibular symptom: vertigo spinning\",\n",
    "    ],\n",
    "    \"RVC-SYM-GEN\": [\n",
    "        \"general symptom: fever chills\",\n",
    "        \"general symptom: fatigue malaise\",\n",
    "        \"general symptom: weakness generalized\",\n",
    "        \"general symptom: weight change\",\n",
    "        \"general symptom: edema swelling\",\n",
    "    ],\n",
    "    \"RVC-SYM-PSY\": [\n",
    "        \"psychological symptom: anxiety\",\n",
    "        \"psychological symptom: depression\",\n",
    "        \"psychological symptom: agitation\",\n",
    "        \"psychological symptom: insomnia\",\n",
    "        \"psychological symptom: suicidal ideation\",\n",
    "        \"psychological symptom: homicidal ideation\",\n",
    "        \"psychological symptom: substance intoxication without overdose\",\n",
    "    ],\n",
    "    \"RVC-DIS\": [\n",
    "        \"patient stated diagnosis: asthma attack\",\n",
    "        \"patient stated diagnosis: copd flare\",\n",
    "        \"patient stated diagnosis: pneumonia\",\n",
    "        \"patient stated diagnosis: diabetes problem\",\n",
    "        \"patient stated diagnosis: hypertensive crisis\",\n",
    "        \"patient stated diagnosis: stroke diagnosed\",\n",
    "    ],\n",
    "    \"RVC-TEST\": [\n",
    "        \"abnormal test: abnormal laboratory result\",\n",
    "        \"abnormal test: abnormal imaging result\",\n",
    "        \"abnormal test: positive culture\",\n",
    "        \"abnormal test: abnormal ecg ekg\",\n",
    "        \"abnormal test: positive blood culture\",\n",
    "    ],\n",
    "    \"RVC-DIAG\": [\n",
    "        \"diagnostic screening: needs covid test\",\n",
    "        \"diagnostic screening: blood pressure check\",\n",
    "        \"diagnostic screening: routine testing screening\",\n",
    "        \"diagnostic screening: prenatal check\",\n",
    "    ],\n",
    "    \"RVC-TREAT\": [\n",
    "        \"treatment medication: prescription refill\",\n",
    "        \"treatment medication: dressing change\",\n",
    "        \"treatment medication: injection therapy\",\n",
    "        \"treatment medication: suture removal\",\n",
    "        \"treatment: wound check\",\n",
    "    ],\n",
    "    \"RVC-ADMIN\": [\n",
    "        \"administrative reason: work form school form insurance paperwork\",\n",
    "        \"administrative reason: clearance note\",\n",
    "    ],\n",
    "    \"RVC-UNCL\": [\"uncodable or unknown reason\"],\n",
    "}\n",
    "\n",
    "for _group in RVC_NAME:\n",
    "    RVC_PROTOS_CURATED.setdefault(_group, [])\n",
    "\n",
    "LABEL2CODE = {\n",
    "    re.sub(r\"\\s+\", \" \", value.strip().lower().replace(\"–\", \"-\").replace(\"—\", \"-\")): code\n",
    "    for code, value in RVC_NAME.items()\n",
    "}\n",
    "\n",
    "sym = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "\n",
    "\n",
    "# --- Lightweight data containers passed between pipeline stages. ---\n",
    "@dataclass\n",
    "class SegmentEmbedCache:\n",
    "    \"\"\"Embeddings and similarity cache for unique complaint segments.\"\"\"\n",
    "\n",
    "    uniq_segments: list[str]\n",
    "    seg2idx: dict[str, int]\n",
    "    emb: torch.Tensor\n",
    "    sim_proto: torch.Tensor\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SegPred:\n",
    "    \"\"\"Per-segment prediction record.\"\"\"\n",
    "\n",
    "    seg_idx: int\n",
    "    segment: str\n",
    "    code: str\n",
    "    name: str\n",
    "    sim: float\n",
    "    rule_code: str | None\n",
    "    rule_used: bool\n",
    "\n",
    "\n",
    "def _add_lexicon_from_phrases(phrases: list[str]) -> None:\n",
    "    for phrase in phrases:\n",
    "        for token in str(phrase).split():\n",
    "            if token.isalpha():\n",
    "                sym.create_dictionary_entry(token, 1)\n",
    "\n",
    "\n",
    "_add_lexicon_from_phrases(list(SEG_OVR.values()) + list(TOK_OVR.values()) + list(ABBR_MAP.values()))\n",
    "\n",
    "\n",
    "# --- Core text preprocessing helpers. ---\n",
    "def get_cc_column(df: pd.DataFrame) -> str:\n",
    "    \"\"\"Return the first recognized chief-complaint column.\"\"\"\n",
    "    for column in CC_CANDIDATES:\n",
    "        if column in df.columns:\n",
    "            return column\n",
    "    raise KeyError(f\"No chief-complaint column found in candidates: {CC_CANDIDATES}\")\n",
    "\n",
    "\n",
    "def _clean_tok(token: str) -> str:\n",
    "    return _PUNCT_EDGE.sub(\"\", token)\n",
    "\n",
    "\n",
    "def _spell(token: str) -> str:\n",
    "    if len(token) <= 3 and token not in SAFE_SHORT_TOKENS:\n",
    "        return token\n",
    "    if not token.isalpha():\n",
    "        return token\n",
    "    candidates = sym.lookup(token, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "    return candidates[0].term if candidates else token\n",
    "\n",
    "\n",
    "def _expand_tokens(segment: str) -> str:\n",
    "    \"\"\"Apply contextual and curated abbreviation expansions in one segment.\"\"\"\n",
    "    output: list[str] = []\n",
    "    segment_context = segment\n",
    "    for token in segment.split():\n",
    "        cleaned = _clean_tok(token)\n",
    "        if not cleaned:\n",
    "            continue\n",
    "        if cleaned in CTX_OVR:\n",
    "            predicate, yes_value, no_value = CTX_OVR[cleaned]\n",
    "            if predicate(segment_context):\n",
    "                output.extend(yes_value.split())\n",
    "                continue\n",
    "            if no_value:\n",
    "                output.extend(no_value.split())\n",
    "                continue\n",
    "        if cleaned in TOK_OVR:\n",
    "            output.extend(TOK_OVR[cleaned].split())\n",
    "            continue\n",
    "        if cleaned in ABBR_MAP:\n",
    "            output.extend(ABBR_MAP[cleaned].split())\n",
    "            continue\n",
    "        output.append(cleaned)\n",
    "    return \" \".join(output)\n",
    "\n",
    "\n",
    "def _looks_clinical(text: str) -> bool:\n",
    "    lowered = text.lower()\n",
    "    return any(key in lowered for key in CLINICAL_KEYS) and bool(re.search(r\"[a-z]\", lowered))\n",
    "\n",
    "\n",
    "def _split_on_and_if_clinical(segment: str) -> list[str]:\n",
    "    parts = [part.strip() for part in re.split(r\"\\band\\b\", segment, flags=re.I)]\n",
    "    if len(parts) == 1:\n",
    "        return parts\n",
    "    output: list[str] = []\n",
    "    buffer = parts[0]\n",
    "    for next_part in parts[1:]:\n",
    "        if _looks_clinical(buffer) and _looks_clinical(next_part):\n",
    "            output.append(buffer.strip())\n",
    "            buffer = next_part\n",
    "        else:\n",
    "            buffer = f\"{buffer} and {next_part}\"\n",
    "    output.append(buffer.strip())\n",
    "    return output\n",
    "\n",
    "\n",
    "def _normalize_absence_symptom(segment: str) -> str | None:\n",
    "    \"\"\"Convert selected absence-style phrases into symptom phrases we can classify.\"\"\"\n",
    "    cleaned = segment.strip()\n",
    "    if not cleaned:\n",
    "        return None\n",
    "\n",
    "    for regex, replacement in ABSENCE_SYMPTOM_RULES:\n",
    "        if regex.search(cleaned):\n",
    "            return regex.sub(replacement, cleaned).strip()\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def _expand_duration(segment: str) -> str:\n",
    "    def repl(match: re.Match[str]) -> str:\n",
    "        base = match.group(1)\n",
    "        count = match.group(2)\n",
    "        unit = match.group(3).lower()\n",
    "        resolved_unit = _UNIT_MAP.get(unit, unit)\n",
    "        return f\"{base} for {count} {resolved_unit}\".strip()\n",
    "\n",
    "    return _DUR_RE.sub(repl, segment)\n",
    "\n",
    "\n",
    "def _lemmatize_segment(segment: str) -> str:\n",
    "    doc = NLP(segment)\n",
    "    lemmas: list[str] = []\n",
    "    for token in doc:\n",
    "        lemma = token.lemma_.strip() if token.lemma_ else \"\"\n",
    "        if not lemma or lemma == \"-PRON-\":\n",
    "            lemma = token.text\n",
    "        lemmas.append(lemma)\n",
    "    return \" \".join(lemmas).strip()\n",
    "\n",
    "\n",
    "def segment_cc(raw: str, keep_negated: bool = False) -> list[str]:\n",
    "    \"\"\"Split a raw chief complaint into normalized text segments.\"\"\"\n",
    "    if not isinstance(raw, str) or not raw.strip():\n",
    "        return []\n",
    "\n",
    "    text = ftfy.fix_text(raw).lower().strip()\n",
    "\n",
    "    for protected in PROTECT:\n",
    "        text = text.replace(protected, protected.replace(\"/\", \"§\"))\n",
    "\n",
    "    text = re.sub(r\"\\bn\\s*[,/&+]\\s*v\\s*[,/&+]\\s*d\\b\", \"n§v§d\", text)\n",
    "    text = re.sub(r\"\\bn\\s*[,/&+]\\s*v\\b\", \"n§v\", text)\n",
    "    text = re.sub(r\"\\bn\\s*/\\s*v\\s*/\\s*d\\b\", \"n§v§d\", text)\n",
    "    text = re.sub(r\"\\bn\\s*/\\s*v\\b\", \"n§v\", text)\n",
    "    text = re.sub(r\"\\bc\\s*/\\s*p\\b\", \"c§p\", text)\n",
    "    text = re.sub(r\"\\bs\\s*/\\s*p\\b\", \"s§p\", text)\n",
    "    text = re.sub(r\"\\bw\\s*/\\s*o\\b\", \"w§o\", text)\n",
    "    text = re.sub(r\"\\bh\\s*/\\s*o\\b\", \"h§o\", text)\n",
    "    text = re.sub(r\"\\bc\\s*/\\s*o\\b\", \"c§o\", text)\n",
    "\n",
    "    text = text.replace(\"/\", \"; \")\n",
    "\n",
    "    segments: list[str] = []\n",
    "    for block in SEP_SPLIT_RE.split(text):\n",
    "        if not block:\n",
    "            continue\n",
    "        block = block.replace(\"§\", \"/\").strip()\n",
    "        block = \" \".join(block.split())\n",
    "        if MISSING_RE.fullmatch(block):\n",
    "            continue\n",
    "        for segment in _split_on_and_if_clinical(block):\n",
    "            if not segment:\n",
    "                continue\n",
    "\n",
    "            normalized_absence = _normalize_absence_symptom(segment)\n",
    "            if normalized_absence is not None:\n",
    "                segment = normalized_absence\n",
    "            elif not keep_negated and NEG_RE.match(segment):\n",
    "                continue\n",
    "\n",
    "            if re.fullmatch(r\"[a-z]$\", segment):\n",
    "                continue\n",
    "            segments.append(segment)\n",
    "    return segments\n",
    "\n",
    "\n",
    "def normalize_cc(raw: str, keep_negated: bool = False) -> list[str]:\n",
    "    \"\"\"Normalize a raw chief complaint to deduplicated, lemmatized segments.\"\"\"\n",
    "    output: list[str] = []\n",
    "    seen: set[str] = set()\n",
    "\n",
    "    for segment in segment_cc(raw, keep_negated=keep_negated):\n",
    "        if segment in SEG_OVR:\n",
    "            segment = SEG_OVR[segment]\n",
    "        else:\n",
    "            tokens = segment.split()\n",
    "            if len(tokens) == 1:\n",
    "                token = _clean_tok(tokens[0])\n",
    "                if token in TOK_OVR:\n",
    "                    segment = TOK_OVR[token]\n",
    "                else:\n",
    "                    segment = ABBR_MAP.get(segment, segment)\n",
    "\n",
    "        segment = _expand_duration(segment)\n",
    "        segment = _expand_tokens(segment)\n",
    "        corrected_tokens = [_spell(token) for token in segment.split()]\n",
    "        segment = _lemmatize_segment(\" \".join(corrected_tokens))\n",
    "\n",
    "        if segment and segment not in seen:\n",
    "            output.append(segment)\n",
    "            seen.add(segment)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def canonize_cc_segments(segments: list[str]) -> list[str]:\n",
    "    \"\"\"Apply light concept-level canonicalization for diagnostics/visualization.\"\"\"\n",
    "    output: list[str] = []\n",
    "    for segment in segments:\n",
    "        transformed = segment\n",
    "        for regex, replacement in CANON_REPLACERS:\n",
    "            transformed = regex.sub(replacement, transformed)\n",
    "        output.append(transformed)\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def rule_gate(segment: str) -> tuple[str | None, list[str]]:\n",
    "    \"\"\"Apply deterministic rule overrides for high-confidence pattern matches.\"\"\"\n",
    "    text = f\" {segment.lower()} \"\n",
    "    if \"weakness\" in text and RX_FOCAL_WEAK.search(text):\n",
    "        return \"RVC-SYM-NERV\", [\"focal weakness\"]\n",
    "\n",
    "    hits: list[str] = []\n",
    "    for code, regex in RULE_RX.items():\n",
    "        if regex.search(text):\n",
    "            hits.append(code)\n",
    "\n",
    "    if not hits:\n",
    "        return None, []\n",
    "\n",
    "    for preferred_code in PRECEDENCE_RVC:\n",
    "        if preferred_code in hits:\n",
    "            return preferred_code, [preferred_code]\n",
    "\n",
    "    return hits[0], [hits[0]]\n",
    "\n",
    "\n",
    "# --- Ontology file parsing helpers (for Appendix and summary resources). ---\n",
    "def _guess_col(columns: list[str], candidates: list[str]) -> str | None:\n",
    "    lowered = [column.lower().strip() for column in columns]\n",
    "    for candidate in candidates:\n",
    "        if candidate in lowered:\n",
    "            return columns[lowered.index(candidate)]\n",
    "    return None\n",
    "\n",
    "\n",
    "def _norm_label(label: str) -> str:\n",
    "    if label is None:\n",
    "        return \"\"\n",
    "    cleaned = str(label).strip().lower().replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
    "    return re.sub(r\"\\s+\", \" \", cleaned)\n",
    "\n",
    "\n",
    "def map_group(label: str) -> str | None:\n",
    "    for regex, code in GROUP_MAP:\n",
    "        if regex.search(str(label)):\n",
    "            return code\n",
    "    return None\n",
    "\n",
    "\n",
    "def _label_to_code(label: str) -> str | None:\n",
    "    normalized = _norm_label(label)\n",
    "    if normalized in LABEL2CODE:\n",
    "        return LABEL2CODE[normalized]\n",
    "    return map_group(label)\n",
    "\n",
    "\n",
    "def canon_phrase(text: str) -> str:\n",
    "    cleaned = str(text).strip().replace(\"’\", \"'\").replace(\"–\", \"-\")\n",
    "    cleaned = RE_SPACE.sub(\" \", cleaned)\n",
    "    return cleaned.strip(\" .;:,\")\n",
    "\n",
    "\n",
    "def keep_phrase(text: str, group_code: str) -> bool:\n",
    "    lowered = text.lower()\n",
    "    if len(lowered) < 3:\n",
    "        return False\n",
    "    if RE_DROP.search(lowered):\n",
    "        return False\n",
    "    if group_code not in {\"RVC-INJ\", \"RVC-SYM-MSK\", \"RVC-SYM-EYE\", \"RVC-SYM-SKIN\"}:\n",
    "        if BODY_PART_ONLY_RE.fullmatch(lowered):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def load_appendix_phrases(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load and normalize Appendix II prototype phrases by RVC code.\"\"\"\n",
    "    if not path.exists():\n",
    "        return pd.DataFrame(columns=[\"group_code\", \"phrase\"])\n",
    "\n",
    "    appendix_df = pd.read_csv(path, dtype=str)\n",
    "\n",
    "    if {\"top_level_group17\", \"text\"}.issubset(appendix_df.columns):\n",
    "        group_col = \"top_level_group17\"\n",
    "        phrase_col = \"text\"\n",
    "    else:\n",
    "        group_col = _guess_col(\n",
    "            list(appendix_df.columns),\n",
    "            [\"top_level_group17\", \"group\", \"top_level_group\", \"rvc_group\", \"module\"],\n",
    "        )\n",
    "        phrase_col = _guess_col(\n",
    "            list(appendix_df.columns),\n",
    "            [\"text\", \"phrase\", \"subentry\", \"entry\", \"label\"],\n",
    "        )\n",
    "        if group_col is None or phrase_col is None:\n",
    "            raise ValueError(f\"Unable to identify expected columns in appendix file: {path}\")\n",
    "\n",
    "    normalized = appendix_df[[group_col, phrase_col]].rename(\n",
    "        columns={group_col: \"group_raw\", phrase_col: \"phrase_raw\"}\n",
    "    )\n",
    "    normalized[\"group_code\"] = normalized[\"group_raw\"].map(_label_to_code)\n",
    "    normalized = normalized[normalized[\"group_code\"].notna()].copy()\n",
    "    normalized[\"phrase\"] = normalized[\"phrase_raw\"].map(canon_phrase)\n",
    "    normalized = normalized[\n",
    "        normalized.apply(lambda row: keep_phrase(row[\"phrase\"], row[\"group_code\"]), axis=1)\n",
    "    ]\n",
    "    return normalized[[\"group_code\", \"phrase\"]].drop_duplicates()\n",
    "\n",
    "\n",
    "def load_summary_weights(path: Path, strategy: str = \"uniform\") -> dict[str, float]:\n",
    "    \"\"\"Load optional group weights and map them to canonical RVC codes.\"\"\"\n",
    "    if not path.exists():\n",
    "        return {}\n",
    "\n",
    "    summary_df = pd.read_csv(path, dtype=str)\n",
    "    group_col = _guess_col(\n",
    "        list(summary_df.columns),\n",
    "        [\"top_level_group17\", \"group\", \"top_level_group\", \"rvc_group\", \"module\"],\n",
    "    )\n",
    "    if group_col is None:\n",
    "        group_col = summary_df.columns[0]\n",
    "\n",
    "    mapped_codes = summary_df[group_col].map(_label_to_code).dropna()\n",
    "    if mapped_codes.empty:\n",
    "        return {}\n",
    "\n",
    "    counts = mapped_codes.value_counts().astype(float)\n",
    "    if strategy == \"uniform\":\n",
    "        counts[:] = 1.0\n",
    "    elif strategy != \"by_rows\":\n",
    "        raise ValueError(\"strategy must be one of {'uniform', 'by_rows'}\")\n",
    "\n",
    "    scaled = np.log1p(counts) / np.log1p(counts).max()\n",
    "    return {code: float(max(1e-3, scaled.get(code, 1.0))) for code in PRECEDENCE_RVC}\n",
    "\n",
    "\n",
    "def dedup_prototypes(\n",
    "    rvc_prototypes: dict[str, list[str]],\n",
    "    encoder: SentenceTransformer,\n",
    "    sim_thresh: float = 0.92,\n",
    "    batch_size: int = 256,\n",
    ") -> dict[str, list[str]]:\n",
    "    \"\"\"Deduplicate near-identical prototype phrases within each group.\"\"\"\n",
    "    output: dict[str, list[str]] = {}\n",
    "\n",
    "    for group_code, phrases in rvc_prototypes.items():\n",
    "        normalized = sorted(dict.fromkeys(canon_phrase(phrase) for phrase in phrases))\n",
    "        if not normalized:\n",
    "            output[group_code] = []\n",
    "            continue\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            embedding = encoder.encode(\n",
    "                normalized,\n",
    "                convert_to_tensor=True,\n",
    "                normalize_embeddings=True,\n",
    "                batch_size=batch_size,\n",
    "            )\n",
    "\n",
    "        keep: list[str] = []\n",
    "        taken = torch.zeros(len(normalized), dtype=torch.bool)\n",
    "        for idx in range(len(normalized)):\n",
    "            if taken[idx]:\n",
    "                continue\n",
    "            keep.append(normalized[idx])\n",
    "            if len(normalized) == 1:\n",
    "                break\n",
    "            similarities = util.cos_sim(embedding[idx : idx + 1], embedding).squeeze(0).cpu().numpy()\n",
    "            duplicate_indices = np.where(similarities >= sim_thresh)[0]\n",
    "            taken[duplicate_indices] = True\n",
    "\n",
    "        output[group_code] = keep\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# --- Embedding/prototype resource construction. ---\n",
    "def build_prototype_resources(cfg: ClassifierConfig) -> dict[str, Any]:\n",
    "    \"\"\"Build model, prototype embeddings, scoring indices, and metadata.\"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    hf_token = os.getenv(cfg.hf_token_env_var, \"\").strip() or None\n",
    "    encoder = SentenceTransformer(\n",
    "        cfg.model_name,\n",
    "        device=device,\n",
    "        trust_remote_code=cfg.model_trust_remote_code,\n",
    "        token=hf_token,\n",
    "    )\n",
    "\n",
    "    work_dir = Path(os.getenv(cfg.work_dir_env_var, Path.cwd())).expanduser().resolve()\n",
    "    appendix_path = work_dir / cfg.appendix_relpath\n",
    "    summary_path = work_dir / cfg.summary_relpath\n",
    "\n",
    "    appendix_df = load_appendix_phrases(appendix_path)\n",
    "    group_weights = load_summary_weights(summary_path, strategy=cfg.summary_weight_strategy)\n",
    "\n",
    "    prototypes_augmented = {code: list(values) for code, values in RVC_PROTOS_CURATED.items()}\n",
    "    for group_code, group_df in appendix_df.groupby(\"group_code\"):\n",
    "        prototypes_augmented[group_code].extend(group_df[\"phrase\"].tolist())\n",
    "\n",
    "    prototypes_final = dedup_prototypes(\n",
    "        prototypes_augmented,\n",
    "        encoder=encoder,\n",
    "        sim_thresh=cfg.prototype_dedup_sim_thresh,\n",
    "        batch_size=cfg.model_batch_size,\n",
    "    )\n",
    "\n",
    "    all_prototype_texts: list[str] = []\n",
    "    proto_to_code: list[str] = []\n",
    "    for code, prototype_list in prototypes_final.items():\n",
    "        prefix = RVC_NAME[code].split(\"–\")[0].strip().lower()\n",
    "        for phrase in prototype_list:\n",
    "            text = phrase if \":\" in phrase else f\"{prefix}: {phrase}\"\n",
    "            all_prototype_texts.append(text)\n",
    "            proto_to_code.append(code)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        proto_emb = encoder.encode(\n",
    "            all_prototype_texts,\n",
    "            convert_to_tensor=True,\n",
    "            normalize_embeddings=True,\n",
    "            batch_size=cfg.model_batch_size,\n",
    "        )\n",
    "\n",
    "    code_to_proto_indices: dict[str, list[int]] = defaultdict(list)\n",
    "    for idx, code in enumerate(proto_to_code):\n",
    "        code_to_proto_indices[code].append(idx)\n",
    "\n",
    "    proto_weight = np.ones(len(proto_to_code), dtype=\"float32\")\n",
    "    if group_weights:\n",
    "        for idx, code in enumerate(proto_to_code):\n",
    "            proto_weight[idx] = max(1e-3, float(group_weights.get(code, 1.0)))\n",
    "\n",
    "    metadata = {\n",
    "        \"model_name\": cfg.model_name,\n",
    "        \"device\": device,\n",
    "        \"prototype_total\": len(all_prototype_texts),\n",
    "        \"prototype_count_by_group\": {code: len(code_to_proto_indices.get(code, [])) for code in PRECEDENCE_RVC},\n",
    "        \"appendix_rows_used\": int(len(appendix_df)),\n",
    "        \"spacy_model\": SPACY_MODEL_LABEL,\n",
    "        \"hf_token_present\": bool(hf_token),\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"encoder\": encoder,\n",
    "        \"device\": device,\n",
    "        \"proto_emb\": proto_emb,\n",
    "        \"proto_to_code\": proto_to_code,\n",
    "        \"code_to_proto_indices\": code_to_proto_indices,\n",
    "        \"proto_weight\": proto_weight,\n",
    "        \"metadata\": metadata,\n",
    "    }\n",
    "\n",
    "\n",
    "# --- Segment caching and scoring helpers. ---\n",
    "def build_segment_cache(\n",
    "    df: pd.DataFrame,\n",
    "    seg_col: str,\n",
    "    resources: dict[str, Any],\n",
    "    batch_size: int = 512,\n",
    ") -> SegmentEmbedCache:\n",
    "    \"\"\"Encode unique complaint segments once and cache similarity to all prototypes.\"\"\"\n",
    "    unique_segments: list[str] = []\n",
    "    seen: set[str] = set()\n",
    "\n",
    "    for value in df[seg_col]:\n",
    "        if isinstance(value, list):\n",
    "            iterable = value\n",
    "        elif isinstance(value, str):\n",
    "            iterable = [value]\n",
    "        else:\n",
    "            iterable = []\n",
    "\n",
    "        for segment in iterable:\n",
    "            cleaned = str(segment).strip()\n",
    "            if cleaned and cleaned not in seen:\n",
    "                unique_segments.append(cleaned)\n",
    "                seen.add(cleaned)\n",
    "\n",
    "    unique_segments.sort()\n",
    "\n",
    "    if not unique_segments:\n",
    "        return SegmentEmbedCache(\n",
    "            uniq_segments=[],\n",
    "            seg2idx={},\n",
    "            emb=torch.empty(0, 0),\n",
    "            sim_proto=torch.empty(0, 0),\n",
    "        )\n",
    "\n",
    "    encoder: SentenceTransformer = resources[\"encoder\"]\n",
    "    proto_emb: torch.Tensor = resources[\"proto_emb\"]\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        segment_emb = encoder.encode(\n",
    "            unique_segments,\n",
    "            convert_to_tensor=True,\n",
    "            normalize_embeddings=True,\n",
    "            batch_size=batch_size,\n",
    "            device=resources[\"device\"],\n",
    "        )\n",
    "        segment_emb = torch.nn.functional.normalize(segment_emb, p=2, dim=1)\n",
    "        segment_to_proto = segment_emb @ proto_emb.T\n",
    "\n",
    "    seg2idx = {segment: idx for idx, segment in enumerate(unique_segments)}\n",
    "    return SegmentEmbedCache(\n",
    "        uniq_segments=unique_segments,\n",
    "        seg2idx=seg2idx,\n",
    "        emb=segment_emb,\n",
    "        sim_proto=segment_to_proto,\n",
    "    )\n",
    "\n",
    "\n",
    "def group_scores_from_proto_row(\n",
    "    proto_row: torch.Tensor | np.ndarray,\n",
    "    resources: dict[str, Any],\n",
    "    method: str = \"max\",\n",
    "    alpha: float = 12.0,\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"Aggregate prototype-level similarities to group-level scores.\"\"\"\n",
    "    vector = proto_row.detach().cpu().numpy() if isinstance(proto_row, torch.Tensor) else np.asarray(proto_row)\n",
    "\n",
    "    scores: dict[str, float] = {}\n",
    "    code_to_proto = resources[\"code_to_proto_indices\"]\n",
    "    proto_weight = resources[\"proto_weight\"]\n",
    "\n",
    "    for group_code, indices in code_to_proto.items():\n",
    "        values = vector[indices]\n",
    "        if method == \"max\":\n",
    "            score = float(values.max(initial=-1.0))\n",
    "        elif method == \"lse\":\n",
    "            weights = proto_weight[indices]\n",
    "            logits = alpha * values + np.log(weights)\n",
    "            max_logit = logits.max()\n",
    "            score = float(max_logit + np.log(np.exp(logits - max_logit).sum()))\n",
    "        else:\n",
    "            raise ValueError(\"method must be one of {'max', 'lse'}\")\n",
    "        scores[group_code] = score\n",
    "\n",
    "    if method == \"lse\" and scores:\n",
    "        max_score = max(scores.values())\n",
    "        min_score = min(scores.values())\n",
    "        denom = (max_score - min_score) if max_score > min_score else 1.0\n",
    "        scores = {group: (score - min_score) / denom for group, score in scores.items()}\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def score_one_segment_soft(\n",
    "    segment: str,\n",
    "    resources: dict[str, Any],\n",
    "    method: str = \"max\",\n",
    "    alpha: float = 12.0,\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"Score a single segment directly against prototype embeddings.\"\"\"\n",
    "    if not isinstance(segment, str) or not segment.strip():\n",
    "        return {group: -1.0 for group in RVC_NAME}\n",
    "\n",
    "    encoder: SentenceTransformer = resources[\"encoder\"]\n",
    "    proto_emb: torch.Tensor = resources[\"proto_emb\"]\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        segment_emb = encoder.encode(\n",
    "            [segment],\n",
    "            convert_to_tensor=True,\n",
    "            normalize_embeddings=True,\n",
    "            device=resources[\"device\"],\n",
    "        )\n",
    "        similarities = util.cos_sim(segment_emb, proto_emb).squeeze(0)\n",
    "\n",
    "    return group_scores_from_proto_row(similarities, resources=resources, method=method, alpha=alpha)\n",
    "\n",
    "\n",
    "def classify_segments_cached(\n",
    "    segments: list[str],\n",
    "    cache: SegmentEmbedCache,\n",
    "    resources: dict[str, Any],\n",
    "    abstain: float = 0.40,\n",
    "    method: str = \"max\",\n",
    "    alpha: float = 12.0,\n",
    ") -> list[SegPred]:\n",
    "    \"\"\"Classify all segments for one visit using cache + rule-gate overrides.\"\"\"\n",
    "    predictions: list[SegPred] = []\n",
    "\n",
    "    for seg_idx, segment in enumerate(segments):\n",
    "        cleaned = str(segment).strip()\n",
    "        if not cleaned:\n",
    "            continue\n",
    "\n",
    "        override_code, _ = rule_gate(cleaned)\n",
    "\n",
    "        if cleaned in cache.seg2idx:\n",
    "            row_idx = cache.seg2idx[cleaned]\n",
    "            score_map = group_scores_from_proto_row(\n",
    "                cache.sim_proto[row_idx], resources=resources, method=method, alpha=alpha\n",
    "            )\n",
    "        else:\n",
    "            score_map = score_one_segment_soft(\n",
    "                cleaned, resources=resources, method=method, alpha=alpha\n",
    "            )\n",
    "\n",
    "        best_code, best_score = max(score_map.items(), key=lambda kv: kv[1])\n",
    "\n",
    "        if override_code is not None:\n",
    "            best_code = override_code\n",
    "\n",
    "        if best_score < abstain and override_code is None:\n",
    "            best_code = \"RVC-UNCL\"\n",
    "\n",
    "        predictions.append(\n",
    "            SegPred(\n",
    "                seg_idx=seg_idx,\n",
    "                segment=cleaned,\n",
    "                code=best_code,\n",
    "                name=RVC_NAME.get(best_code, best_code),\n",
    "                sim=float(best_score),\n",
    "                rule_code=override_code,\n",
    "                rule_used=override_code is not None,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def rfv_from_segment_preds(segpreds: list[SegPred], max_rfv: int = 5) -> list[dict[str, Any]]:\n",
    "    \"\"\"Aggregate segment-level predictions to visit-level RFV slots.\"\"\"\n",
    "    first_position: dict[str, int] = {}\n",
    "    best_similarity: dict[str, float] = {}\n",
    "    support_segment: dict[str, str] = {}\n",
    "\n",
    "    for prediction in segpreds:\n",
    "        group_code = prediction.code\n",
    "        if group_code == \"RVC-UNCL\":\n",
    "            continue\n",
    "\n",
    "        if group_code not in first_position or prediction.seg_idx < first_position[group_code]:\n",
    "            first_position[group_code] = prediction.seg_idx\n",
    "            support_segment[group_code] = prediction.segment\n",
    "\n",
    "        best_similarity[group_code] = max(best_similarity.get(group_code, -1.0), prediction.sim)\n",
    "\n",
    "    ordered = sorted(\n",
    "        first_position.keys(),\n",
    "        key=lambda code: (PRECEDENCE_RVC.index(code), first_position[code]),\n",
    "    )\n",
    "    ordered = ordered[:max_rfv]\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"code\": code,\n",
    "            \"name\": RVC_NAME[code],\n",
    "            \"support\": support_segment[code],\n",
    "            \"sim\": best_similarity[code],\n",
    "        }\n",
    "        for code in ordered\n",
    "    ]\n",
    "\n",
    "\n",
    "# --- Visit-level RFV assignment and output shaping. ---\n",
    "def assign_rvc_per_segment_and_visit_cached(\n",
    "    df: pd.DataFrame,\n",
    "    cache: SegmentEmbedCache,\n",
    "    resources: dict[str, Any],\n",
    "    seg_col: str = \"cc_cleaned\",\n",
    "    cfg: ClassifierConfig | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Attach standardized RFV outputs to each visit using cached segment embeddings.\"\"\"\n",
    "    active_cfg = cfg or ClassifierConfig()\n",
    "    max_rfv = active_cfg.max_rfv\n",
    "    if active_cfg.empty_primary_policy not in {\"blank\", \"uncodable\"}:\n",
    "        raise ValueError(\"empty_primary_policy must be one of {'blank', 'uncodable'}\")\n",
    "\n",
    "    rows: list[dict[str, Any]] = []\n",
    "    for value in df[seg_col].tolist():\n",
    "        if isinstance(value, list):\n",
    "            segments = [str(segment).strip() for segment in value if str(segment).strip()]\n",
    "        elif isinstance(value, str) and value.strip():\n",
    "            segments = [value.strip()]\n",
    "        else:\n",
    "            segments = []\n",
    "\n",
    "        segments = segments[: active_cfg.max_segments_per_visit]\n",
    "        segment_predictions = classify_segments_cached(\n",
    "            segments,\n",
    "            cache=cache,\n",
    "            resources=resources,\n",
    "            abstain=active_cfg.abstain_threshold,\n",
    "            method=active_cfg.scoring_method,\n",
    "            alpha=active_cfg.alpha,\n",
    "        )\n",
    "        visit_rfv = rfv_from_segment_preds(segment_predictions, max_rfv=max_rfv)\n",
    "\n",
    "        # If all non-empty segment predictions are uncodable, explicitly carry that into RFV1.\n",
    "        if (\n",
    "            not visit_rfv\n",
    "            and active_cfg.allow_uncodable_primary\n",
    "            and segments\n",
    "            and segment_predictions\n",
    "        ):\n",
    "            uncodable_predictions = [pred for pred in segment_predictions if pred.code == \"RVC-UNCL\"]\n",
    "            if uncodable_predictions:\n",
    "                chosen = max(uncodable_predictions, key=lambda pred: (pred.sim, -pred.seg_idx))\n",
    "                visit_rfv = [\n",
    "                    {\n",
    "                        \"code\": \"RVC-UNCL\",\n",
    "                        \"name\": RVC_NAME[\"RVC-UNCL\"],\n",
    "                        \"support\": chosen.segment,\n",
    "                        \"sim\": float(chosen.sim),\n",
    "                    }\n",
    "                ]\n",
    "\n",
    "        # Optional policy for truly empty complaints.\n",
    "        if (\n",
    "            not visit_rfv\n",
    "            and not segments\n",
    "            and active_cfg.allow_uncodable_primary\n",
    "            and active_cfg.empty_primary_policy == \"uncodable\"\n",
    "        ):\n",
    "            visit_rfv = [\n",
    "                {\n",
    "                    \"code\": \"RVC-UNCL\",\n",
    "                    \"name\": RVC_NAME[\"RVC-UNCL\"],\n",
    "                    \"support\": \"\",\n",
    "                    \"sim\": math.nan,\n",
    "                }\n",
    "            ]\n",
    "\n",
    "        row: dict[str, Any] = {\n",
    "            \"segment_preds\": json.dumps([prediction.__dict__ for prediction in segment_predictions], ensure_ascii=False)\n",
    "        }\n",
    "\n",
    "        for idx in range(max_rfv):\n",
    "            if idx < len(visit_rfv):\n",
    "                code = visit_rfv[idx][\"code\"]\n",
    "                name = visit_rfv[idx][\"name\"]\n",
    "                support = visit_rfv[idx][\"support\"]\n",
    "                sim = float(visit_rfv[idx][\"sim\"])\n",
    "            else:\n",
    "                code = \"\"\n",
    "                name = \"\"\n",
    "                support = \"\"\n",
    "                sim = math.nan\n",
    "\n",
    "            slot = idx + 1\n",
    "            row[f\"RFV{slot}\"] = RVC_SHORT.get(code, \"\")\n",
    "            row[f\"RFV{slot}_name\"] = name\n",
    "            row[f\"RFV{slot}_support\"] = support\n",
    "            row[f\"RFV{slot}_sim\"] = sim\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    drop_columns = [column for column in df.columns if str(column).startswith(\"RFV\") or column == \"segment_preds\"]\n",
    "    output_df = df.drop(columns=drop_columns, errors=\"ignore\").copy()\n",
    "    output_df = pd.concat([output_df, pd.DataFrame(rows, index=output_df.index)], axis=1)\n",
    "    return output_df\n",
    "\n",
    "\n",
    "# --- Output QA and diagnostic helpers used later in the notebook. ---\n",
    "def validate_output_schema(\n",
    "    df: pd.DataFrame,\n",
    "    max_rfv: int = 5,\n",
    "    require_primary_for_nonempty: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"Validate required output columns, uniqueness, and JSON shape for segment predictions.\"\"\"\n",
    "    required = [\"segment_preds\"]\n",
    "    for slot in range(1, max_rfv + 1):\n",
    "        required.extend(\n",
    "            [\n",
    "                f\"RFV{slot}\",\n",
    "                f\"RFV{slot}_name\",\n",
    "                f\"RFV{slot}_support\",\n",
    "                f\"RFV{slot}_sim\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    duplicates = df.columns[df.columns.duplicated()].tolist()\n",
    "    if duplicates:\n",
    "        raise ValueError(f\"Duplicate columns detected: {duplicates}\")\n",
    "\n",
    "    missing = [column for column in required if column not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required output columns: {missing}\")\n",
    "\n",
    "    bad_json_rows = 0\n",
    "    parsed_payloads: list[list[dict[str, Any]] | None] = []\n",
    "    for value in df[\"segment_preds\"].fillna(\"[]\"):\n",
    "        if not isinstance(value, str):\n",
    "            bad_json_rows += 1\n",
    "            parsed_payloads.append(None)\n",
    "            continue\n",
    "        try:\n",
    "            parsed = json.loads(value)\n",
    "        except json.JSONDecodeError:\n",
    "            bad_json_rows += 1\n",
    "            parsed_payloads.append(None)\n",
    "            continue\n",
    "        if not isinstance(parsed, list):\n",
    "            bad_json_rows += 1\n",
    "            parsed_payloads.append(None)\n",
    "            continue\n",
    "        parsed_payloads.append(parsed)\n",
    "\n",
    "    if bad_json_rows:\n",
    "        raise ValueError(f\"Found {bad_json_rows} malformed segment_preds rows\")\n",
    "\n",
    "    if require_primary_for_nonempty:\n",
    "        blank_primary_violations = 0\n",
    "        for primary_name, payload in zip(df[\"RFV1_name\"], parsed_payloads):\n",
    "            primary_blank = pd.isna(primary_name) or not str(primary_name).strip()\n",
    "            has_segments = isinstance(payload, list) and len(payload) > 0\n",
    "            if primary_blank and has_segments:\n",
    "                blank_primary_violations += 1\n",
    "\n",
    "        if blank_primary_violations:\n",
    "            raise ValueError(\n",
    "                \"Found blank RFV1_name for rows with non-empty segment predictions: \"\n",
    "                f\"{blank_primary_violations}\"\n",
    "            )\n",
    "\n",
    "    print(f\"Schema validation passed for {len(df):,} rows.\")\n",
    "\n",
    "\n",
    "def save_with_suffix(\n",
    "    df: pd.DataFrame,\n",
    "    working_path: str | Path,\n",
    "    suffix: str = \"_with_NLP\",\n",
    "    sheet_name: str = \"with_NLP\",\n",
    ") -> Path:\n",
    "    \"\"\"Write workbook with suffix and return the new path.\"\"\"\n",
    "    path = Path(working_path)\n",
    "    output_path = path.with_name(path.stem + suffix + path.suffix)\n",
    "    df.to_excel(output_path, index=False, sheet_name=sheet_name)\n",
    "    print(f\"Wrote: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def summarize_overrides(df: pd.DataFrame, column: str = \"segment_preds\") -> dict[str, Any]:\n",
    "    \"\"\"Summarize rule-override usage from serialized segment prediction payloads.\"\"\"\n",
    "    if column not in df.columns:\n",
    "        raise KeyError(f\"Missing column: {column}\")\n",
    "\n",
    "    segment_total = 0\n",
    "    segment_overridden = 0\n",
    "    by_rule: Counter = Counter()\n",
    "    by_final: Counter = Counter()\n",
    "    rows_total = 0\n",
    "    rows_with_override = 0\n",
    "\n",
    "    for value in df[column].dropna():\n",
    "        rows_total += 1\n",
    "        payload: Any\n",
    "        if isinstance(value, str):\n",
    "            try:\n",
    "                payload = json.loads(value)\n",
    "            except Exception:\n",
    "                continue\n",
    "        elif isinstance(value, list):\n",
    "            payload = value\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if not isinstance(payload, list):\n",
    "            continue\n",
    "\n",
    "        row_has_override = False\n",
    "        for segment_prediction in payload:\n",
    "            if not isinstance(segment_prediction, dict):\n",
    "                continue\n",
    "            segment_total += 1\n",
    "            if segment_prediction.get(\"rule_used\"):\n",
    "                segment_overridden += 1\n",
    "                row_has_override = True\n",
    "                by_rule[segment_prediction.get(\"rule_code\", \"UNKNOWN\")] += 1\n",
    "                by_final[segment_prediction.get(\"code\", \"UNKNOWN\")] += 1\n",
    "\n",
    "        if row_has_override:\n",
    "            rows_with_override += 1\n",
    "\n",
    "    return {\n",
    "        \"segments_total\": segment_total,\n",
    "        \"segments_overridden\": segment_overridden,\n",
    "        \"segment_override_rate\": (segment_overridden / segment_total) if segment_total else float(\"nan\"),\n",
    "        \"rows_total\": rows_total,\n",
    "        \"rows_with_override\": rows_with_override,\n",
    "        \"row_override_rate\": (rows_with_override / rows_total) if rows_total else float(\"nan\"),\n",
    "        \"by_rule\": by_rule,\n",
    "        \"by_final\": by_final,\n",
    "    }\n",
    "\n",
    "\n",
    "def validate_pipeline(\n",
    "    df: pd.DataFrame,\n",
    "    cache: SegmentEmbedCache,\n",
    "    resources: dict[str, Any],\n",
    "    seg_col: str = \"cc_cleaned\",\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"Run smoke checks for prototype health, segment coverage, and rule/classifier sanity.\"\"\"\n",
    "    metadata = resources[\"metadata\"]\n",
    "    prototype_counts = metadata[\"prototype_count_by_group\"]\n",
    "    missing_groups = [group for group, count in prototype_counts.items() if count == 0]\n",
    "    if missing_groups:\n",
    "        raise ValueError(f\"Groups missing prototypes: {missing_groups}\")\n",
    "\n",
    "    row_count = len(df)\n",
    "    nonempty_segment_rows = int(df[seg_col].apply(lambda value: isinstance(value, list) and len(value) > 0).sum())\n",
    "    unique_segments = len(cache.uniq_segments)\n",
    "\n",
    "    probes = [\n",
    "        \"shortness of breath\",\n",
    "        \"bradycardia\",\n",
    "        \"positive blood culture\",\n",
    "        \"alter mental status\",\n",
    "        \"abdominal pain / shortness of breath\",\n",
    "        \"right eye redness / pain\",\n",
    "    ]\n",
    "    probe_results: dict[str, list[tuple[str, str, float, bool]]] = {}\n",
    "    for probe in probes:\n",
    "        segments = segment_cc(probe)\n",
    "        predictions = classify_segments_cached(\n",
    "            segments,\n",
    "            cache=cache,\n",
    "            resources=resources,\n",
    "            method=\"max\",\n",
    "        )\n",
    "        probe_results[probe] = [\n",
    "            (prediction.segment, prediction.code, prediction.sim, prediction.rule_used)\n",
    "            for prediction in predictions\n",
    "        ]\n",
    "\n",
    "    print(\"Prototype integrity:\")\n",
    "    print(prototype_counts)\n",
    "    print(f\"Total prototypes: {metadata['prototype_total']}\")\n",
    "    print()\n",
    "    print(\"Segment column health:\")\n",
    "    print(f\"rows={row_count:,} | with>=1 segment={nonempty_segment_rows:,} | unique segments={unique_segments:,}\")\n",
    "    print()\n",
    "    print(\"Probe classifications:\")\n",
    "    for probe, result in probe_results.items():\n",
    "        print(f\"{probe} -> {result}\")\n",
    "\n",
    "    return {\n",
    "        \"row_count\": row_count,\n",
    "        \"rows_with_segments\": nonempty_segment_rows,\n",
    "        \"unique_segments\": unique_segments,\n",
    "        \"prototype_counts\": prototype_counts,\n",
    "        \"probe_results\": probe_results,\n",
    "    }\n",
    "\n",
    "\n",
    "def top_cc_plot(\n",
    "    df: pd.DataFrame,\n",
    "    candidates: tuple[str, ...] = CC_CANDIDATES,\n",
    "    top_n: int = 30,\n",
    "    normalize_case: str | None = \"upper\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Optional EDA: plot top raw chief complaints.\"\"\"\n",
    "    cc_column = get_cc_column(df)\n",
    "    series = (\n",
    "        df[cc_column]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    )\n",
    "\n",
    "    if normalize_case == \"upper\":\n",
    "        series = series.str.upper()\n",
    "    elif normalize_case == \"title\":\n",
    "        series = series.str.title()\n",
    "\n",
    "    counts = series.value_counts().head(top_n).sort_values(ascending=True)\n",
    "    axis = counts.plot(kind=\"barh\", figsize=(8, 10))\n",
    "    axis.set_xlabel(\"Count\")\n",
    "    axis.set_ylabel(\"Chief complaint\")\n",
    "    axis.set_title(f\"Top {top_n} Chief Complaints ({cc_column})\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return counts.sort_values(ascending=False).rename_axis(\"chief_complaint\").reset_index(name=\"count\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcac5d4",
   "metadata": {},
   "source": [
    "## 3) Input load and schema checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e12afe7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T21:14:10.285360Z",
     "iopub.status.busy": "2026-02-08T21:14:10.285263Z",
     "iopub.status.idle": "2026-02-08T21:14:51.310909Z",
     "shell.execute_reply": "2026-02-08T21:14:51.309838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy pipeline: en_core_web_sm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded input rows=41,322, cols=184\n",
      "Input path: /Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/MIMIC tabular data/MIMICIV all with CC.xlsx\n",
      "Added transitional aliases: ['age', 'dbp', 'hr', 'race', 'rr', 'sbp', 'spo2', 'temp']\n",
      "Detected chief complaint column: ed_triage_cc\n"
     ]
    }
   ],
   "source": [
    "# Purpose: instantiate config, set deterministic seeds, load the input workbook, and validate required identifiers.\n",
    "# Why this matters: stable setup reduces run-to-run variability and catches input issues before expensive model steps.\n",
    "\n",
    "cfg = ClassifierConfig()\n",
    "\n",
    "try:\n",
    "    NLP = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\", \"textcat\", \"senter\"])\n",
    "    SPACY_MODEL_LABEL = \"en_core_web_sm\"\n",
    "except OSError as exc:\n",
    "    if cfg.require_spacy_model:\n",
    "        raise ImportError(\n",
    "            \"spaCy model 'en_core_web_sm' is required but not installed in this environment. \"\n",
    "            \"Install it once with: python -m spacy download en_core_web_sm\"\n",
    "        ) from exc\n",
    "    NLP = spacy.blank(\"en\")\n",
    "    SPACY_MODEL_LABEL = \"spacy.blank('en')\"\n",
    "\n",
    "print(f\"spaCy pipeline: {SPACY_MODEL_LABEL}\")\n",
    "\n",
    "np.random.seed(cfg.seed)\n",
    "torch.manual_seed(cfg.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(cfg.seed)\n",
    "\n",
    "load_dotenv()\n",
    "WORK_DIR = Path(os.getenv(cfg.work_dir_env_var, Path.cwd())).expanduser().resolve()\n",
    "SRC_DIR = WORK_DIR / \"src\"\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "workflow_contracts = importlib.import_module(\"hypercap_cc_nlp.workflow_contracts\")\n",
    "resolve_classifier_input_path = workflow_contracts.resolve_classifier_input_path\n",
    "normalize_classifier_input_schema = workflow_contracts.normalize_classifier_input_schema\n",
    "ensure_required_workflow_columns = workflow_contracts.ensure_required_columns\n",
    "resolve_classifier_output_paths = workflow_contracts.resolve_classifier_output_paths\n",
    "CLASSIFIER_TRANSITIONAL_ALIASES = workflow_contracts.CLASSIFIER_TRANSITIONAL_ALIASES\n",
    "\n",
    "classifier_input_filename = os.getenv(cfg.input_filename_env_var, cfg.input_filename)\n",
    "file_path = resolve_classifier_input_path(WORK_DIR, classifier_input_filename)\n",
    "\n",
    "df = pd.read_excel(file_path, sheet_name=0, engine=\"openpyxl\")\n",
    "df.columns = df.columns.str.strip()\n",
    "pre_alias_columns = set(df.columns)\n",
    "df = normalize_classifier_input_schema(df)\n",
    "added_aliases = sorted(set(df.columns).difference(pre_alias_columns))\n",
    "\n",
    "ensure_required_workflow_columns(\n",
    "    df,\n",
    "    [\"hadm_id\", \"subject_id\"],\n",
    "    context=\"classifier input\",\n",
    ")\n",
    "\n",
    "print(f\"Loaded input rows={len(df):,}, cols={df.shape[1]:,}\")\n",
    "print(f\"Input path: {file_path}\")\n",
    "print(f\"Added transitional aliases: {added_aliases if added_aliases else 'None'}\")\n",
    "print(f\"Detected chief complaint column: {get_cc_column(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3b30be",
   "metadata": {},
   "source": [
    "## 4) Text cleaning and segmentation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2185b6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T21:14:51.319282Z",
     "iopub.status.busy": "2026-02-08T21:14:51.319118Z",
     "iopub.status.idle": "2026-02-08T21:15:53.577712Z",
     "shell.execute_reply": "2026-02-08T21:15:53.576035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text normalization complete.\n",
      "         ed_triage_cc               cc_cleaned         cc_cleaned_str\n",
      "0  Abd pain, Vomiting  [abdominal pain, vomit]  abdominal pain; vomit\n",
      "1             DYSPNEA                [dyspnea]                dyspnea\n",
      "2         BRADYCARDIA            [bradycardia]            bradycardia\n",
      "3      FEVER,HEADACHE        [fever, headache]        fever; headache\n",
      "4             DYSPNEA                [dyspnea]                dyspnea\n"
     ]
    }
   ],
   "source": [
    "# Purpose: define data-enrichment helpers and generate normalized chief-complaint text used by the classifier.\n",
    "# Why this matters: high-quality text normalization is foundational for both rule matching and embedding similarity.\n",
    "\n",
    "def add_hypercapnia_flags(\n",
    "    df: pd.DataFrame,\n",
    "    art_col: str = \"poc_paco2\",\n",
    "    art_uom_col: str = \"poc_paco2_uom\",\n",
    "    vbg_col: str = \"poc_vbg_paco2\",\n",
    "    vbg_uom_col: str = \"poc_vbg_paco2_uom\",\n",
    "    out_abg: str = \"hypercap_by_abg\",\n",
    "    out_vbg: str = \"hypercap_by_vbg\",\n",
    "    out_any: str = \"hypercap_by_bg\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Add ABG/VBG hypercapnia flags with kPa-to-mmHg conversion when needed.\"\"\"\n",
    "\n",
    "    def get_series(frame: pd.DataFrame, col: str, fill_value: float = float(\"nan\")) -> pd.Series:\n",
    "        return frame[col] if col in frame.columns else pd.Series(fill_value, index=frame.index)\n",
    "\n",
    "    def to_mmhg(values: pd.Series, uoms: pd.Series) -> pd.Series:\n",
    "        numeric = pd.to_numeric(values, errors=\"coerce\")\n",
    "        unit_text = uoms.astype(\"string\").str.strip().str.lower()\n",
    "        is_kpa = unit_text.str.contains(\"kpa\", na=False)\n",
    "        converted = numeric.copy()\n",
    "        converted.loc[is_kpa] = converted.loc[is_kpa] * 7.50062\n",
    "        return converted\n",
    "\n",
    "    art_vals = pd.to_numeric(get_series(df, art_col), errors=\"coerce\")\n",
    "    vbg_vals = pd.to_numeric(get_series(df, vbg_col), errors=\"coerce\")\n",
    "    art_uoms = get_series(df, art_uom_col)\n",
    "    vbg_uoms = get_series(df, vbg_uom_col)\n",
    "\n",
    "    art_mmhg = to_mmhg(art_vals, art_uoms)\n",
    "    vbg_mmhg = to_mmhg(vbg_vals, vbg_uoms)\n",
    "\n",
    "    df[out_abg] = (art_mmhg >= 45).astype(\"int8\")\n",
    "    df[out_vbg] = (vbg_mmhg >= 50).astype(\"int8\")\n",
    "    df[out_any] = ((df[out_abg] == 1) | (df[out_vbg] == 1)).astype(\"int8\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = add_hypercapnia_flags(df)\n",
    "cc_column = get_cc_column(df)\n",
    "df[\"cc_cleaned\"] = df[cc_column].apply(normalize_cc)\n",
    "df[\"cc_cleaned_str\"] = df[\"cc_cleaned\"].str.join(\"; \")\n",
    "df[\"cc_cleaned_canon\"] = df[\"cc_cleaned\"].apply(canonize_cc_segments)\n",
    "df[\"cc_cleaned_canon_str\"] = df[\"cc_cleaned_canon\"].str.join(\"; \")\n",
    "\n",
    "print(\"Text normalization complete.\")\n",
    "print(df[[cc_column, \"cc_cleaned\", \"cc_cleaned_str\"]].head(5).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3145369",
   "metadata": {},
   "source": [
    "## 5) Rule system and ontology mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f55371c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T21:15:53.592992Z",
     "iopub.status.busy": "2026-02-08T21:15:53.592381Z",
     "iopub.status.idle": "2026-02-08T21:15:53.601205Z",
     "shell.execute_reply": "2026-02-08T21:15:53.600818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule coverage preview:\n",
      "'shortness of breath' -> RVC-SYM-RESP\n",
      "'bradycardia' -> RVC-SYM-CIRC\n",
      "'positive blood culture' -> RVC-TEST\n",
      "'alter mental status' -> RVC-SYM-NERV\n",
      "'abdominal pain' -> RVC-SYM-DIG\n",
      "'right eye redness / pain' -> RVC-SYM-EYE\n",
      "'paperwork clearance' -> RVC-ADMIN\n"
     ]
    }
   ],
   "source": [
    "# Purpose: run a quick rule-coverage spot check on representative phrases.\n",
    "# Why this matters: confirms deterministic override logic is wired correctly before full inference.\n",
    "\n",
    "print(\"Rule coverage preview:\")\n",
    "probe_texts = [\n",
    "    \"shortness of breath\",\n",
    "    \"bradycardia\",\n",
    "    \"positive blood culture\",\n",
    "    \"alter mental status\",\n",
    "    \"abdominal pain\",\n",
    "    \"right eye redness / pain\",\n",
    "    \"paperwork clearance\",\n",
    "]\n",
    "for text in probe_texts:\n",
    "    override, _ = rule_gate(text)\n",
    "    print(f\"{text!r} -> {override}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc943e05",
   "metadata": {},
   "source": [
    "## 6) Prototype loading, deduplication, and embedding preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18638424",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T21:15:53.606930Z",
     "iopub.status.busy": "2026-02-08T21:15:53.606824Z",
     "iopub.status.idle": "2026-02-08T21:16:21.261980Z",
     "shell.execute_reply": "2026-02-08T21:16:21.259765Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0571ac64a1d44f45a8dca3f28f7309a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model metadata:\n",
      "- model_name: NeuML/bioclinical-modernbert-base-embeddings\n",
      "- device: cpu\n",
      "- spacy_model: en_core_web_sm\n",
      "- hf_token_present: True\n",
      "- appendix_rows_used: 606\n",
      "- total_prototypes: 693\n",
      "- prototype_count_by_group:\n",
      "{'RVC-INJ': 75, 'RVC-SYM-RESP': 43, 'RVC-SYM-CIRC': 18, 'RVC-SYM-NERV': 25, 'RVC-SYM-DIG': 52, 'RVC-SYM-GU': 76, 'RVC-SYM-MSK': 22, 'RVC-SYM-SKIN': 29, 'RVC-SYM-EYE': 50, 'RVC-SYM-GEN': 38, 'RVC-SYM-PSY': 40, 'RVC-DIS': 92, 'RVC-TEST': 12, 'RVC-DIAG': 47, 'RVC-TREAT': 54, 'RVC-ADMIN': 14, 'RVC-UNCL': 6}\n"
     ]
    }
   ],
   "source": [
    "# Purpose: build prototype embeddings/resources once and print metadata for reproducibility auditing.\n",
    "# Why this matters: model/device/prototype counts help explain classification behavior during review.\n",
    "\n",
    "resources = build_prototype_resources(cfg)\n",
    "metadata = resources[\"metadata\"]\n",
    "\n",
    "print(\"Model metadata:\")\n",
    "print(f\"- model_name: {metadata['model_name']}\")\n",
    "print(f\"- device: {metadata['device']}\")\n",
    "print(f\"- spacy_model: {metadata['spacy_model']}\")\n",
    "print(f\"- hf_token_present: {metadata['hf_token_present']}\")\n",
    "print(f\"- appendix_rows_used: {metadata['appendix_rows_used']}\")\n",
    "print(f\"- total_prototypes: {metadata['prototype_total']}\")\n",
    "print(\"- prototype_count_by_group:\")\n",
    "print(metadata[\"prototype_count_by_group\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d834805c",
   "metadata": {},
   "source": [
    "## 7) Segment cache build and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45bb3dad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T21:16:21.272668Z",
     "iopub.status.busy": "2026-02-08T21:16:21.272494Z",
     "iopub.status.idle": "2026-02-08T21:16:55.340818Z",
     "shell.execute_reply": "2026-02-08T21:16:55.337728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique segments cached: 3,154\n",
      "Cache emb shape: (3154, 768)\n",
      "Cache sim shape: (3154, 693)\n"
     ]
    }
   ],
   "source": [
    "# Purpose: build the segment cache and run visit-level RFV assignment across the full dataset.\n",
    "# Why this matters: caching unique segments improves runtime while preserving deterministic outputs.\n",
    "\n",
    "cache = build_segment_cache(\n",
    "    df=df,\n",
    "    seg_col=\"cc_cleaned\",\n",
    "    resources=resources,\n",
    "    batch_size=cfg.segment_batch_size,\n",
    ")\n",
    "\n",
    "df_out = assign_rvc_per_segment_and_visit_cached(\n",
    "    df=df,\n",
    "    cache=cache,\n",
    "    resources=resources,\n",
    "    seg_col=\"cc_cleaned\",\n",
    "    cfg=cfg,\n",
    ")\n",
    "\n",
    "print(f\"Unique segments cached: {len(cache.uniq_segments):,}\")\n",
    "print(f\"Cache emb shape: {tuple(cache.emb.shape)}\")\n",
    "print(f\"Cache sim shape: {tuple(cache.sim_proto.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11e64b4",
   "metadata": {},
   "source": [
    "## 8) Output shaping and compatibility mapping\n",
    "\n",
    "Standardized output columns:\n",
    "- `RFV1..RFV5` (legacy short-code compatibility)\n",
    "- `RFV1_name..RFV5_name`\n",
    "- `RFV1_support..RFV5_support`\n",
    "- `RFV1_sim..RFV5_sim`\n",
    "- `segment_preds`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80175d9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T21:16:55.353907Z",
     "iopub.status.busy": "2026-02-08T21:16:55.353760Z",
     "iopub.status.idle": "2026-02-08T21:16:55.497113Z",
     "shell.execute_reply": "2026-02-08T21:16:55.496811Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema validation passed for 41,322 rows.\n",
      "                                cc_cleaned     RFV1              RFV1_name              RFV1_support  RFV1_sim     RFV2          RFV2_name RFV2_support  RFV2_sim RFV3 RFV3_name RFV3_support  RFV3_sim RFV4 RFV4_name RFV4_support  RFV4_sim RFV5 RFV5_name RFV5_support  RFV5_sim\n",
      "0                  [abdominal pain, vomit]       gi    Symptom – Digestive            abdominal pain  0.830388                                                NaN                                   NaN                                   NaN                                   NaN\n",
      "1                                [dyspnea]     resp  Symptom – Respiratory                   dyspnea  0.720657                                                NaN                                   NaN                                   NaN                                   NaN\n",
      "2                            [bradycardia]     circ  Symptom – Circulatory               bradycardia  0.631536                                                NaN                                   NaN                                   NaN                                   NaN\n",
      "3                        [fever, headache]    neuro      Symptom – Nervous                  headache  0.751600  general  Symptom – General        fever  0.661924                                   NaN                                   NaN                                   NaN\n",
      "4                                [dyspnea]     resp  Symptom – Respiratory                   dyspnea  0.720657                                                NaN                                   NaN                                   NaN                                   NaN\n",
      "5                    [shortness of breath]     resp  Symptom – Respiratory       shortness of breath  0.840122                                                NaN                                   NaN                                   NaN                                   NaN\n",
      "6                           [nausea vomit]       gi    Symptom – Digestive              nausea vomit  0.777928                                                NaN                                   NaN                                   NaN                                   NaN\n",
      "7               [right eye redness / pain]  eye_ear      Symptom – Eye/Ear  right eye redness / pain  0.858664                                                NaN                                   NaN                                   NaN                                   NaN\n",
      "8  [abdominal pain, nausea vomit diarrhea]       gi    Symptom – Digestive            abdominal pain  0.830388                                                NaN                                   NaN                                   NaN                                   NaN\n",
      "9                                [dyspnea]     resp  Symptom – Respiratory                   dyspnea  0.720657                                                NaN                                   NaN                                   NaN                                   NaN\n"
     ]
    }
   ],
   "source": [
    "# Purpose: validate the output schema contract and preview standardized RFV columns.\n",
    "# Why this matters: downstream notebooks rely on these exact column names and structures.\n",
    "\n",
    "validate_output_schema(\n",
    "    df_out,\n",
    "    max_rfv=cfg.max_rfv,\n",
    "    require_primary_for_nonempty=cfg.allow_uncodable_primary,\n",
    ")\n",
    "\n",
    "display_cols = [\"cc_cleaned\"]\n",
    "for slot in range(1, cfg.max_rfv + 1):\n",
    "    display_cols.extend(\n",
    "        [\n",
    "            f\"RFV{slot}\",\n",
    "            f\"RFV{slot}_name\",\n",
    "            f\"RFV{slot}_support\",\n",
    "            f\"RFV{slot}_sim\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "print(df_out[display_cols].head(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecbff9c",
   "metadata": {},
   "source": [
    "## 9) Diagnostics and QA checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d008772",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T21:16:55.500661Z",
     "iopub.status.busy": "2026-02-08T21:16:55.500559Z",
     "iopub.status.idle": "2026-02-08T21:16:59.657335Z",
     "shell.execute_reply": "2026-02-08T21:16:59.651961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototype integrity:\n",
      "{'RVC-INJ': 75, 'RVC-SYM-RESP': 43, 'RVC-SYM-CIRC': 18, 'RVC-SYM-NERV': 25, 'RVC-SYM-DIG': 52, 'RVC-SYM-GU': 76, 'RVC-SYM-MSK': 22, 'RVC-SYM-SKIN': 29, 'RVC-SYM-EYE': 50, 'RVC-SYM-GEN': 38, 'RVC-SYM-PSY': 40, 'RVC-DIS': 92, 'RVC-TEST': 12, 'RVC-DIAG': 47, 'RVC-TREAT': 54, 'RVC-ADMIN': 14, 'RVC-UNCL': 6}\n",
      "Total prototypes: 693\n",
      "\n",
      "Segment column health:\n",
      "rows=41,322 | with>=1 segment=41,311 | unique segments=3,154\n",
      "\n",
      "Probe classifications:\n",
      "shortness of breath -> [('shortness of breath', 'RVC-SYM-RESP', 0.8401217460632324, True)]\n",
      "bradycardia -> [('bradycardia', 'RVC-SYM-CIRC', 0.6315359473228455, True)]\n",
      "positive blood culture -> [('positive blood culture', 'RVC-TEST', 0.8695230484008789, True)]\n",
      "alter mental status -> [('alter mental status', 'RVC-SYM-NERV', 0.7240737080574036, True)]\n",
      "abdominal pain / shortness of breath -> [('abdominal pain', 'RVC-SYM-DIG', 0.830388069152832, True), ('shortness of breath', 'RVC-SYM-RESP', 0.8401217460632324, True)]\n",
      "right eye redness / pain -> [('right eye redness', 'RVC-SYM-EYE', 0.9090079069137573, True), ('pain', 'RVC-SYM-DIG', 0.7539336085319519, False)]\n",
      "\n",
      "Regression summary\n",
      "------------------\n",
      "Input rows: 41,322\n",
      "Output rows: 41,322\n",
      "Row count preserved: True\n",
      "\n",
      "Top RFV1_name counts:\n",
      "RFV1_name\n",
      "Symptom – Respiratory         8094\n",
      "Symptom – Nervous             6579\n",
      "Symptom – Digestive           5782\n",
      "Symptom – Circulatory         4186\n",
      "Diseases (patient-stated)     3324\n",
      "Injuries & adverse effects    2927\n",
      "Symptom – Genitourinary       2596\n",
      "Symptom – General             2588\n",
      "Symptom – Eye/Ear             1027\n",
      "Symptom – Skin/Hair/Nails      988\n",
      "Uncodable/Unknown              917\n",
      "Symptom – Musculoskeletal      714\n",
      "Administrative                 650\n",
      "Abnormal test result           399\n",
      "Treatment/Medication           284\n",
      "\n",
      "Override usage:\n",
      "Segments total: 63,733\n",
      "Segments overridden: 46,480\n",
      "Segment override rate: 0.7293\n",
      "Rows total: 41,322\n",
      "Rows with >=1 override: 32,934\n",
      "Row override rate: 0.7970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Primary-label completeness:\n",
      "blank_rfv1_total: 11\n",
      "blank_rfv1_with_nonempty_segments: 0\n",
      "all_uncodable_rows: 917\n",
      "\n",
      "Top uncodable segments (post-fix):\n",
      "  54 | unknown - cc\n",
      "  45 | alter of\n",
      "  41 | mcc\n",
      "  31 | gtube renal\n",
      "  24 | unable to ambulate\n",
      "  21 | iph\n",
      "  18 | clot fistula\n",
      "  16 | mca\n",
      "  16 | ftt\n",
      "  15 | unwitnessed small\n",
      "  15 | icd renal\n",
      "  14 | aortic dissection\n",
      "  11 | atrial fibrillation\n",
      "  10 | _ _ _\n",
      "   9 | intubated heart blood\n",
      "\n",
      "Optional plots skipped (cfg.run_optional_plots=False).\n"
     ]
    }
   ],
   "source": [
    "# Purpose: run QA smoke checks and summarize regression metrics after classification.\n",
    "# Why this matters: immediate diagnostics make it easier to detect unintended behavioral drift.\n",
    "\n",
    "qa_summary = validate_pipeline(df_out, cache=cache, resources=resources, seg_col=\"cc_cleaned\")\n",
    "\n",
    "override_summary = summarize_overrides(df_out, column=\"segment_preds\")\n",
    "\n",
    "print(\"\\nRegression summary\")\n",
    "print(\"------------------\")\n",
    "print(f\"Input rows: {len(df):,}\")\n",
    "print(f\"Output rows: {len(df_out):,}\")\n",
    "print(f\"Row count preserved: {len(df) == len(df_out)}\")\n",
    "\n",
    "print(\"\\nTop RFV1_name counts:\")\n",
    "print(df_out[\"RFV1_name\"].value_counts(dropna=True).head(15).to_string())\n",
    "\n",
    "print(\"\\nOverride usage:\")\n",
    "print(f\"Segments total: {override_summary['segments_total']:,}\")\n",
    "print(f\"Segments overridden: {override_summary['segments_overridden']:,}\")\n",
    "print(f\"Segment override rate: {override_summary['segment_override_rate']:.4f}\")\n",
    "print(f\"Rows total: {override_summary['rows_total']:,}\")\n",
    "print(f\"Rows with >=1 override: {override_summary['rows_with_override']:,}\")\n",
    "print(f\"Row override rate: {override_summary['row_override_rate']:.4f}\")\n",
    "\n",
    "parsed_segment_preds = df_out[\"segment_preds\"].fillna(\"[]\").map(\n",
    "    lambda value: json.loads(value) if isinstance(value, str) else []\n",
    ")\n",
    "blank_rfv1 = df_out[\"RFV1_name\"].fillna(\"\").astype(str).str.strip().eq(\"\")\n",
    "nonempty_segment_rows = parsed_segment_preds.map(lambda payload: isinstance(payload, list) and len(payload) > 0)\n",
    "all_uncodable_rows = parsed_segment_preds.map(\n",
    "    lambda payload: isinstance(payload, list)\n",
    "    and len(payload) > 0\n",
    "    and all(isinstance(item, dict) and item.get(\"code\") == \"RVC-UNCL\" for item in payload)\n",
    ")\n",
    "\n",
    "blank_rfv1_total = int(blank_rfv1.sum())\n",
    "blank_rfv1_with_nonempty_segments = int((blank_rfv1 & nonempty_segment_rows).sum())\n",
    "all_uncodable_total = int(all_uncodable_rows.sum())\n",
    "\n",
    "print(\"\\nPrimary-label completeness:\")\n",
    "print(f\"blank_rfv1_total: {blank_rfv1_total:,}\")\n",
    "print(f\"blank_rfv1_with_nonempty_segments: {blank_rfv1_with_nonempty_segments:,}\")\n",
    "print(f\"all_uncodable_rows: {all_uncodable_total:,}\")\n",
    "\n",
    "uncodable_segment_counter: Counter = Counter()\n",
    "for payload in parsed_segment_preds[all_uncodable_rows]:\n",
    "    for item in payload:\n",
    "        if isinstance(item, dict):\n",
    "            uncodable_segment_counter[item.get(\"segment\", \"\")] += 1\n",
    "\n",
    "print(\"\\nTop uncodable segments (post-fix):\")\n",
    "for segment, count in uncodable_segment_counter.most_common(15):\n",
    "    print(f\"{count:4d} | {segment}\")\n",
    "\n",
    "if cfg.allow_uncodable_primary and blank_rfv1_with_nonempty_segments != 0:\n",
    "    raise AssertionError(\n",
    "        \"Expected zero blank RFV1_name values for rows with non-empty segment predictions, \"\n",
    "        f\"found {blank_rfv1_with_nonempty_segments}.\"\n",
    "    )\n",
    "\n",
    "if cfg.run_optional_plots:\n",
    "    top_table = top_cc_plot(df, top_n=30, normalize_case=\"upper\")\n",
    "    print(top_table.head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nOptional plots skipped (cfg.run_optional_plots=False).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a79a06",
   "metadata": {},
   "source": [
    "## 10) Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "821329da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T21:16:59.677334Z",
     "iopub.status.busy": "2026-02-08T21:16:59.676916Z",
     "iopub.status.idle": "2026-02-08T21:22:03.780593Z",
     "shell.execute_reply": "2026-02-08T21:22:03.772066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote canonical output: /Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/MIMIC tabular data/MIMICIV all with CC_with_NLP.xlsx\n",
      "Wrote archive output: /Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/MIMIC tabular data/prior runs/2026-02-08 MIMICIV all with CC_with_NLP.xlsx\n",
      "Schema summary: rows=41,322, cols=212, rfv_slots=5\n",
      "Export complete: /Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/Hypercap-CC-NLP/MIMIC tabular data/MIMICIV all with CC_with_NLP.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Purpose: export the NLP-augmented workbook using canonical handoff names and a dated archive copy.\n",
    "# Why this matters: downstream analysis reads a stable canonical path while preserving run-history snapshots.\n",
    "\n",
    "canonical_output_path, archive_output_path = resolve_classifier_output_paths(WORK_DIR)\n",
    "archive_output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_out.to_excel(canonical_output_path, index=False, sheet_name=cfg.output_sheet_name)\n",
    "df_out.to_excel(archive_output_path, index=False, sheet_name=cfg.output_sheet_name)\n",
    "\n",
    "print(f\"Wrote canonical output: {canonical_output_path}\")\n",
    "print(f\"Wrote archive output: {archive_output_path}\")\n",
    "print(\n",
    "    \"Schema summary: \"\n",
    "    f\"rows={len(df_out):,}, cols={df_out.shape[1]:,}, \"\n",
    "    f\"rfv_slots={cfg.max_rfv}\"\n",
    ")\n",
    "print(f\"Export complete: {canonical_output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hypercap-CC-NLP (3.11.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0234c8e1d46c4770885249fd2a9fc4be": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "041c42d9ab554cfcac7b4f3b876e5d15": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0571ac64a1d44f45a8dca3f28f7309a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9168a244af4448358434fdf81c2346ee",
        "IPY_MODEL_75d27b1155424a6b932139a1919da750",
        "IPY_MODEL_6964597e5c8d4a92ad15bb6a529a5fb9"
       ],
       "layout": "IPY_MODEL_0234c8e1d46c4770885249fd2a9fc4be",
       "tabbable": null,
       "tooltip": null
      }
     },
     "6964597e5c8d4a92ad15bb6a529a5fb9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_041c42d9ab554cfcac7b4f3b876e5d15",
       "placeholder": "​",
       "style": "IPY_MODEL_f5478ca5d6d04c39be35679a60e41084",
       "tabbable": null,
       "tooltip": null,
       "value": " 134/134 [00:00&lt;00:00, 2104.40it/s, Materializing param=layers.21.mlp_norm.weight]"
      }
     },
     "75d27b1155424a6b932139a1919da750": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_84a1e406d9aa4943876a15f2cef1bf0c",
       "max": 134.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e1fc4117effc431cbbb12db1add5e1c6",
       "tabbable": null,
       "tooltip": null,
       "value": 134.0
      }
     },
     "809903d7b2414f5cbbcde34310694087": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "84a1e406d9aa4943876a15f2cef1bf0c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9168a244af4448358434fdf81c2346ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_93cd4f833a364fb59b20b6ab3c87290c",
       "placeholder": "​",
       "style": "IPY_MODEL_809903d7b2414f5cbbcde34310694087",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading weights: 100%"
      }
     },
     "93cd4f833a364fb59b20b6ab3c87290c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e1fc4117effc431cbbb12db1add5e1c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f5478ca5d6d04c39be35679a60e41084": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
